"""
Chunk ê¸°ë°˜ RAG ì„œë¹„ìŠ¤

ê¸°ì¡´ íŒŒì¼ ë‹¨ìœ„ RAG ëŒ€ì‹  chunk ë‹¨ìœ„ë¡œ ì €ì¥í•˜ê³  ê²€ìƒ‰í•˜ëŠ” ì‹œìŠ¤í…œ
"""

import os
import json
import requests
from typing import List, Dict, Any, Optional
from dataclasses import asdict

try:
    from .ast_chunker import CodeChunk
except ImportError:
    from ast_chunker import CodeChunk


class ChunkRAGService:
    """Chunk ê¸°ë°˜ RAG ì„œë¹„ìŠ¤"""
    
    def __init__(self, rag_base_url: str = "http://localhost:8003"):
        """
        Args:
            rag_base_url: RAG ì„œë¹„ìŠ¤ ê¸°ë³¸ URL
        """
        self.rag_base_url = rag_base_url
    
    def upload_chunks(self, chunks: List[CodeChunk], collection_name: str = "codemuse_chunks") -> Dict[str, Any]:
        """
        Chunk ë¦¬ìŠ¤íŠ¸ë¥¼ RAGì— ì—…ë¡œë“œ
        
        Args:
            chunks: ì—…ë¡œë“œí•  CodeChunk ë¦¬ìŠ¤íŠ¸
            collection_name: ChromaDB ì»¬ë ‰ì…˜ ì´ë¦„
            
        Returns:
            ì—…ë¡œë“œ ê²°ê³¼ ì •ë³´
        """
        print(f"ğŸš€ Chunk RAG ì—…ë¡œë“œ ì‹œì‘...")
        print(f"ğŸ“¦ ì´ chunk ìˆ˜: {len(chunks)}")
        print(f"ğŸ—„ï¸ ì»¬ë ‰ì…˜: {collection_name}")
        
        # 1. ê¸°ì¡´ ì»¬ë ‰ì…˜ ì‚­ì œ (ìƒˆë¡œìš´ chunk ì‹œìŠ¤í…œì„ ìœ„í•´)
        self._clear_collection(collection_name)
        
        # 2. chunkë³„ë¡œ RAGì— ì—…ë¡œë“œ
        upload_results = []
        failed_uploads = []
        
        for i, chunk in enumerate(chunks):
            try:
                print(f"ğŸ“¤ ì—…ë¡œë“œ ì¤‘... {i+1}/{len(chunks)} - {chunk.chunk_type}:{chunk.name}")
                
                # chunkë¥¼ RAG í˜•ì‹ìœ¼ë¡œ ë³€í™˜
                rag_document = self._chunk_to_rag_document(chunk)
                
                # RAG ì„œë¹„ìŠ¤ì— ì—…ë¡œë“œ
                response = self._upload_single_document(rag_document)
                
                if response.get('success'):
                    upload_results.append({
                        'chunk_id': chunk.chunk_id,
                        'status': 'success'
                    })
                else:
                    failed_uploads.append({
                        'chunk_id': chunk.chunk_id,
                        'error': response.get('error', 'Unknown error')
                    })
                    
            except Exception as e:
                print(f"âŒ Chunk {chunk.chunk_id} ì—…ë¡œë“œ ì‹¤íŒ¨: {e}")
                failed_uploads.append({
                    'chunk_id': chunk.chunk_id,
                    'error': str(e)
                })
        
        # 3. ê²°ê³¼ ìš”ì•½
        result = {
            'collection_name': collection_name,
            'total_chunks': len(chunks),
            'successful_uploads': len(upload_results),
            'failed_uploads': len(failed_uploads),
            'success_rate': len(upload_results) / len(chunks) * 100 if chunks else 0,
            'upload_results': upload_results,
            'failed_uploads': failed_uploads
        }
        
        print(f"âœ… ì—…ë¡œë“œ ì™„ë£Œ!")
        print(f"   ì„±ê³µ: {len(upload_results)}ê°œ")
        print(f"   ì‹¤íŒ¨: {len(failed_uploads)}ê°œ")
        print(f"   ì„±ê³µë¥ : {result['success_rate']:.1f}%")
        
        return result
    
    def _clear_collection(self, collection_name: str) -> bool:
        """ê¸°ì¡´ ì»¬ë ‰ì…˜ ì‚­ì œ"""
        try:
            print(f"ğŸ—‘ï¸ ê¸°ì¡´ ì»¬ë ‰ì…˜ '{collection_name}' ì‚­ì œ ì¤‘...")
            
            # ëª¨ë“  ë¬¸ì„œ ì‚­ì œ API í˜¸ì¶œ
            response = requests.delete(f"{self.rag_base_url}/api/v1/documents")
            
            if response.status_code == 200:
                print(f"   âœ… ì»¬ë ‰ì…˜ ì‚­ì œ ì„±ê³µ")
                return True
            else:
                print(f"   âš ï¸ ì»¬ë ‰ì…˜ ì‚­ì œ ì‘ë‹µ: {response.status_code}")
                return False
                
        except Exception as e:
            print(f"   âŒ ì»¬ë ‰ì…˜ ì‚­ì œ ì˜¤ë¥˜: {e}")
            return False
    
    def _chunk_to_rag_document(self, chunk: CodeChunk) -> Dict[str, Any]:
        """CodeChunkë¥¼ RAG ë¬¸ì„œ í˜•ì‹ìœ¼ë¡œ ë³€í™˜"""
        
        # ë©”íƒ€ë°ì´í„° êµ¬ì„±
        metadata = {
            "chunk_id": chunk.chunk_id,
            "file_path": chunk.file_path,
            "filename": os.path.basename(chunk.file_path),
            "chunk_type": chunk.chunk_type,
            "name": chunk.name,
            "line_range": chunk.line_range,
            "tags": ",".join(chunk.tags),  # ChromaDBëŠ” ë¬¸ìì—´ë§Œ ì§€ì›
            "token_count": chunk.token_count or 0,
            "complexity": chunk.complexity or 0,
            "is_async": chunk.is_async,
            "is_generator": chunk.is_generator
        }
        
        # ì„ íƒì  ë©”íƒ€ë°ì´í„° ì¶”ê°€
        if chunk.parent:
            metadata["parent"] = chunk.parent
        
        if chunk.base_classes:
            metadata["base_classes"] = ",".join(chunk.base_classes)
        
        if chunk.decorators:
            metadata["decorators"] = ",".join(chunk.decorators)
        
        if chunk.dependencies:
            metadata["dependencies"] = ",".join(chunk.dependencies)
        
        # ê¸°ë³¸ í´ë” ìš°ì„ ìˆœìœ„ (ì§ˆë¬¸ë³„ë¡œ ë™ì  ì¡°ì •ë¨)
        metadata["folder_priority"] = 50  # ê¸°ë³¸ê°’
        
        # í–¥ìƒëœ ì½˜í…ì¸  êµ¬ì„±
        content = self._generate_enhanced_content(chunk)
        
        return {
            "content": content,
            "metadata": metadata
        }
    
    def get_dynamic_folder_priority(self, file_path: str, query: str, keywords: List[str]) -> int:
        """ì§ˆë¬¸ ë‚´ìš©ê³¼ í‚¤ì›Œë“œì— ê¸°ë°˜í•œ ë™ì  í´ë” ìš°ì„ ìˆœìœ„ ê³„ì‚°"""
        
        path_lower = file_path.lower()
        query_lower = query.lower()
        keywords_lower = [k.lower() for k in keywords]
        
        # ê¸°ë³¸ ìš°ì„ ìˆœìœ„
        base_priority = 50
        
        # ì§ˆë¬¸ íŒ¨í„´ë³„ í´ë” ë§¤í•‘
        folder_patterns = {
            # ë¶„ì„ ê´€ë ¨
            "analyzers": {
                "keywords": ["ë¶„ì„", "analysis", "detect", "issue", "error", "ì˜¤ë¥˜", "ê²€ì¶œ", "ë¬¸ì œ", "detector"],
                "boost": 50
            },
            # ê²€ì¦ ê´€ë ¨  
            "validator": {
                "keywords": ["ê²€ì¦", "validation", "validate", "í™•ì¸", "ì²´í¬", "check"],
                "boost": 45
            },
            # ì²˜ë¦¬ ê´€ë ¨
            "processor": {
                "keywords": ["ì²˜ë¦¬", "process", "ìƒì„±", "create", "ë³€í™˜", "convert", "íŒŒì‹±", "parse"],
                "boost": 40
            },
            # ì„œë¹„ìŠ¤ ê´€ë ¨
            "service": {
                "keywords": ["ì„œë¹„ìŠ¤", "service", "ê´€ë¦¬", "manage", "orchestrator", "ì¡°ìœ¨", "prompt"],
                "boost": 35
            },
            # í¬ë§¤íŒ… ê´€ë ¨
            "formatter": {
                "keywords": ["í¬ë§·", "format", "í…œí”Œë¦¿", "template", "ì¶œë ¥", "output", "í‘œì‹œ", "text"],
                "boost": 30
            },
            # ìŠ¤íŠ¸ë¦¬ë° ê´€ë ¨
            "streaming": {
                "keywords": ["ìŠ¤íŠ¸ë¦¬ë°", "streaming", "ì‹¤ì‹œê°„", "real-time", "chunk", "ì§„í–‰", "stream"],
                "boost": 30
            },
            # ì„¤ì • ê´€ë ¨
            "config": {
                "keywords": ["ì„¤ì •", "config", "configuration", "ì˜µì…˜", "option"],
                "boost": 25
            },
            # ìœ í‹¸ë¦¬í‹° ê´€ë ¨
            "utils": {
                "keywords": ["ìœ í‹¸", "util", "ë„êµ¬", "helper", "ê³µí†µ", "shared", "ìƒìˆ˜", "constants", "language"],
                "boost": 20
            }
        }
        
        # íŒŒì¼ ê²½ë¡œì—ì„œ í´ë” íƒ€ì… ì¶”ì¶œ
        for folder_type, pattern_info in folder_patterns.items():
            if folder_type in path_lower:
                # í•´ë‹¹ í´ë” íƒ€ì…ì— ëŒ€í•œ í‚¤ì›Œë“œ ë§¤ì¹­ ì ìˆ˜ ê³„ì‚°
                keyword_score = 0
                
                # ì§ˆë¬¸ì—ì„œ ì§ì ‘ ë§¤ì¹­
                for pattern_keyword in pattern_info["keywords"]:
                    if pattern_keyword in query_lower:
                        keyword_score += 3
                
                # ì¶”ì¶œëœ í‚¤ì›Œë“œì—ì„œ ë§¤ì¹­
                for extracted_keyword in keywords_lower:
                    for pattern_keyword in pattern_info["keywords"]:
                        if pattern_keyword in extracted_keyword or extracted_keyword in pattern_keyword:
                            keyword_score += 2
                
                # ìµœì¢… ìš°ì„ ìˆœìœ„ = ê¸°ë³¸ê°’ + í´ë” ë¶€ìŠ¤íŠ¸ + í‚¤ì›Œë“œ ì ìˆ˜
                final_priority = base_priority + pattern_info["boost"] + (keyword_score * 5)
                
                return min(final_priority, 150)  # ìµœëŒ€ 150ìœ¼ë¡œ ì œí•œ
        
        return base_priority
    
    def _generate_enhanced_content(self, chunk: CodeChunk) -> str:
        """ê²€ìƒ‰ í’ˆì§ˆ í–¥ìƒì„ ìœ„í•œ ì½˜í…ì¸  ìƒì„±"""
        
        content_parts = []
        
        # 1. ê¸°ë³¸ ì •ë³´
        content_parts.append(f"# {chunk.chunk_type}: {chunk.name}")
        
        # 2. íŒŒì¼ ì •ë³´
        file_name = os.path.basename(chunk.file_path)
        content_parts.append(f"íŒŒì¼: {file_name}")
        
        # 3. ë¶€ëª¨ ì •ë³´ (ë©”ì„œë“œì¸ ê²½ìš°)
        if chunk.parent:
            content_parts.append(f"ì†Œì†: {chunk.parent}")
        
        # 4. íŠ¹ì„± ì •ë³´
        characteristics = []
        if chunk.is_async:
            characteristics.append("ë¹„ë™ê¸°")
        if chunk.is_generator:
            characteristics.append("ì œë„ˆë ˆì´í„°")
        if chunk.decorators:
            characteristics.extend(chunk.decorators)
        
        if characteristics:
            content_parts.append(f"íŠ¹ì„±: {', '.join(characteristics)}")
        
        # 5. docstring (ìˆëŠ” ê²½ìš°)
        if chunk.docstring:
            content_parts.append(f"ì„¤ëª…: {chunk.docstring}")
        
        # 6. íƒœê·¸ ì •ë³´ (ê²€ìƒ‰ í‚¤ì›Œë“œë¡œ í™œìš©)
        if chunk.tags:
            content_parts.append(f"íƒœê·¸: {', '.join(chunk.tags)}")
        
        # 7. ì‹¤ì œ ì½”ë“œ ë‚´ìš©
        content_parts.append("ì½”ë“œ:")
        content_parts.append(chunk.content)
        
        return "\n\n".join(content_parts)
    
    def _upload_single_document(self, document: Dict[str, Any]) -> Dict[str, Any]:
        """ë‹¨ì¼ ë¬¸ì„œë¥¼ RAGì— ì—…ë¡œë“œ"""
        
        try:
            response = requests.post(
                f"{self.rag_base_url}/api/v1/documents",
                json=document,
                headers={"Content-Type": "application/json"},
                timeout=30
            )
            
            if response.status_code == 200:
                return {"success": True, "response": response.json()}
            else:
                return {
                    "success": False,
                    "error": f"HTTP {response.status_code}: {response.text}"
                }
                
        except requests.exceptions.Timeout:
            return {"success": False, "error": "Request timeout"}
        except Exception as e:
            return {"success": False, "error": str(e)}
    
    def verify_upload(self) -> Dict[str, Any]:
        """ì—…ë¡œë“œëœ chunk í™•ì¸"""
        
        try:
            # RAG ì„œë¹„ìŠ¤ ìƒíƒœ í™•ì¸
            response = requests.get(f"{self.rag_base_url}/health")
            
            if response.status_code != 200:
                return {"success": False, "error": "RAG service not available"}
            
            # ë¬¸ì„œ ìˆ˜ í™•ì¸
            response = requests.get(f"{self.rag_base_url}/api/v1/documents")
            
            if response.status_code == 200:
                data = response.json()
                return {
                    "success": True,
                    "total_documents": data.get("total_documents", 0),
                    "collection_info": data
                }
            else:
                return {
                    "success": False,
                    "error": f"Cannot get document count: {response.status_code}"
                }
                
        except Exception as e:
            return {"success": False, "error": str(e)}
    
    def search_chunks(self, query: str, chunk_types: Optional[List[str]] = None, 
                     file_filter: Optional[List[str]] = None, 
                     top_k: int = 10) -> Dict[str, Any]:
        """
        Chunk ê²€ìƒ‰ (í…ŒìŠ¤íŠ¸ìš©)
        
        Args:
            query: ê²€ìƒ‰ ì¿¼ë¦¬
            chunk_types: ê²€ìƒ‰í•  chunk íƒ€ì… í•„í„° (ì˜ˆ: ["function", "method"])
            file_filter: ê²€ìƒ‰í•  íŒŒì¼ í•„í„° (ì˜ˆ: ["issue_detector.py"])
            top_k: ë°˜í™˜í•  ìµœëŒ€ ê²°ê³¼ ìˆ˜
            
        Returns:
            ê²€ìƒ‰ ê²°ê³¼
        """
        
        try:
            search_request = {
                "query": query,
                "top_k": top_k
            }
            
            # í•„í„° ì¶”ê°€ (êµ¬í˜„ ì˜ˆì •)
            if chunk_types:
                search_request["chunk_types"] = chunk_types
            if file_filter:
                search_request["file_filter"] = file_filter
            
            response = requests.post(
                f"{self.rag_base_url}/api/v1/search",
                json=search_request,
                headers={"Content-Type": "application/json"}
            )
            
            if response.status_code == 200:
                return {"success": True, "results": response.json()}
            else:
                return {
                    "success": False,
                    "error": f"Search failed: {response.status_code}"
                }
                
        except Exception as e:
            return {"success": False, "error": str(e)}


def upload_chunks_from_metadata(metadata_file: str, rag_base_url: str = "http://localhost:8003") -> Dict[str, Any]:
    """
    chunk_metadata.json íŒŒì¼ì—ì„œ chunkë“¤ì„ ì½ì–´ì„œ RAGì— ì—…ë¡œë“œ
    
    Args:
        metadata_file: chunk_metadata.json íŒŒì¼ ê²½ë¡œ
        rag_base_url: RAG ì„œë¹„ìŠ¤ URL
        
    Returns:
        ì—…ë¡œë“œ ê²°ê³¼
    """
    
    print(f"ğŸ“‚ ë©”íƒ€ë°ì´í„° íŒŒì¼ ì½ê¸°: {metadata_file}")
    
    try:
        with open(metadata_file, 'r', encoding='utf-8') as f:
            metadata = json.load(f)
        
        chunks_data = metadata.get('chunks', [])
        print(f"ğŸ“¦ ë¡œë“œëœ chunk ìˆ˜: {len(chunks_data)}")
        
        # JSON ë°ì´í„°ë¥¼ CodeChunk ê°ì²´ë¡œ ë³€í™˜
        chunks = []
        for chunk_data in chunks_data:
            # None ê°’ë“¤ì„ ê¸°ë³¸ê°’ìœ¼ë¡œ ë³€í™˜
            for key, default in [
                ('base_classes', []),
                ('decorators', []),
                ('tags', []),
                ('dependencies', []),
                ('parent', None),
                ('complexity', None),
                ('docstring', None),
                ('is_async', False),
                ('is_generator', False),
                ('token_count', None)
            ]:
                if chunk_data.get(key) is None:
                    chunk_data[key] = default
            
            chunk = CodeChunk(**chunk_data)
            chunks.append(chunk)
        
        # RAG ì„œë¹„ìŠ¤ì— ì—…ë¡œë“œ
        rag_service = ChunkRAGService(rag_base_url)
        result = rag_service.upload_chunks(chunks)
        
        return result
        
    except FileNotFoundError:
        return {"success": False, "error": f"Metadata file not found: {metadata_file}"}
    except json.JSONDecodeError as e:
        return {"success": False, "error": f"Invalid JSON format: {e}"}
    except Exception as e:
        return {"success": False, "error": f"Upload failed: {e}"}


if __name__ == "__main__":
    # í…ŒìŠ¤íŠ¸ìš©
    metadata_file = "/Users/roseline/projects/codemuse-backend/generated_docs_chunk/chunk_metadata.json"
    
    print("ğŸš€ Chunk RAG ì—…ë¡œë“œ í…ŒìŠ¤íŠ¸ ì‹œì‘...")
    result = upload_chunks_from_metadata(metadata_file)
    
    if result.get('success', False):
        print(f"âœ… ì—…ë¡œë“œ ì„±ê³µ!")
        print(f"   ì„±ê³µë¥ : {result['success_rate']:.1f}%")
    else:
        print(f"âŒ ì—…ë¡œë“œ ì‹¤íŒ¨: {result.get('error', 'Unknown error')}")
    
    # ì—…ë¡œë“œ í™•ì¸
    rag_service = ChunkRAGService()
    verify_result = rag_service.verify_upload()
    print(f"ğŸ“Š ì—…ë¡œë“œ í™•ì¸: {verify_result}")
