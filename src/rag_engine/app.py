"""
RAG Engine ì„œë¹„ìŠ¤ ì‹¤í–‰ ìŠ¤í¬ë¦½íŠ¸ (ChromaDB ê¸°ë°˜)
"""

import sys
import os
import uuid
from pathlib import Path
from datetime import datetime
# ë³„ë„ ë‚´ë¶€ ëª¨ë“ˆ ì˜ì¡´ ì—†ìŒ. ìƒìœ„ src ì¶”ê°€ëŠ” ë¶ˆí•„ìš”í•˜ë‚˜, ë£¨íŠ¸ ê¸°ì¤€ ì‹¤í–‰ ì‹œ ì•ˆì „í•˜ê²Œ ìœ ì§€
_BASE_DIR = os.path.dirname(__file__)
_SRC_DIR = os.path.dirname(_BASE_DIR)
if _SRC_DIR not in sys.path:
    sys.path.append(_SRC_DIR)

from fastapi import FastAPI, HTTPException
from fastapi.middleware.cors import CORSMiddleware
from pydantic import BaseModel
from typing import List, Optional
import chromadb
from chromadb.config import Settings
from rank_bm25 import BM25Okapi
import re
import openai

# OpenAI API í‚¤ ì„¤ì • ë° ë¡œê¹…
openai.api_key = os.getenv("OPENAI_API_KEY")
print(f"ğŸ”‘ RAG Engine OpenAI API í‚¤: {openai.api_key[:20]}..." if openai.api_key else "âŒ OpenAI API í‚¤ê°€ ì„¤ì •ë˜ì§€ ì•ŠìŒ")

# ê°„ë‹¨í•œ ë§í¬ ë§¤í•‘ (í”„ë¡ íŠ¸ì—”ë“œ í˜¸í™˜ì„±)
SIMPLE_LINK_MAPPING = {
    # IssueDetector í´ë˜ìŠ¤ ê´€ë ¨ ë§í¬ë“¤
    'class_issuedetector': 'sample_code/rule_analyzer/analyzers/issue_detector.py/<a id="class-issuedetector"></a>ğŸ¯ `issuedetector`',
    'class-issuedetector': 'sample_code/rule_analyzer/analyzers/issue_detector.py/<a id="class-issuedetector"></a>ğŸ¯ `issuedetector`',
    'issuedetector': 'sample_code/rule_analyzer/analyzers/issue_detector.py/<a id="class-issuedetector"></a>ğŸ¯ `issuedetector`',
    'issue_detector': 'sample_code/rule_analyzer/analyzers/issue_detector.py/<a id="class-issuedetector"></a>ğŸ¯ `issuedetector`',
    'IssueDetector': 'sample_code/rule_analyzer/analyzers/issue_detector.py/<a id="class-issuedetector"></a>ğŸ¯ `issuedetector`',
    
    # í•¨ìˆ˜ ê´€ë ¨ ë§í¬ë“¤
    'func_detect_type_mismatch_157': 'sample_code/rule_analyzer/analyzers/issue_detector.py/<a id="func-detect-type-mismatch-157"></a>ğŸ”§ `detect_type_mismatch`',
    'detect_type_mismatch': 'sample_code/rule_analyzer/analyzers/issue_detector.py/<a id="func-detect-type-mismatch-157"></a>ğŸ”§ `detect_type_mismatch`',
    
    # íŒŒì¼ ê´€ë ¨ ë§í¬ë“¤
    'file_issue_detector': 'sample_code/rule_analyzer/analyzers/issue_detector.py/<a id="class-issuedetector"></a>ğŸ¯ `issuedetector`',
    'issue_detector.py': 'sample_code/rule_analyzer/analyzers/issue_detector.py/<a id="class-issuedetector"></a>ğŸ¯ `issuedetector`',
}

app = FastAPI(title="RAG Engine", version="1.0.0")

# CORS ì„¤ì •
app.add_middleware(
    CORSMiddleware,
    allow_origins=["*"],
    allow_credentials=True,
    allow_methods=["*"],
    allow_headers=["*"],
)

class SearchRequest(BaseModel):
    query: str
    limit: int = 5
    # ì„ íƒì  í•„í„°ë“¤
    chunk_type_filter: Optional[str] = None  # ì˜ˆ: "overview|function|class"
    exclude_chunk_types: Optional[List[str]] = None
    file_filter: Optional[List[str]] = None  # íŒŒì¼ëª… ë˜ëŠ” ê²½ë¡œ ì¼ë¶€
    include_metadata: Optional[bool] = True

class SearchResponse(BaseModel):
    results: List[dict]
    query: str
    total_found: int

class DocumentRequest(BaseModel):
    id: Optional[str] = None
    content: str
    metadata: Optional[dict] = None

def korean_tokenize(text: str) -> List[str]:
    """í•œêµ­ì–´ í† í¬ë‚˜ì´ì§• (ê°„ë‹¨í•œ ê³µë°± ê¸°ë°˜)"""
    # ê³µë°±ìœ¼ë¡œ ë¶„ë¦¬í•˜ê³  íŠ¹ìˆ˜ë¬¸ì ì œê±°
    tokens = re.findall(r'[ê°€-í£a-zA-Z0-9]+', text.lower())
    return tokens

def preprocess_korean_text(text: str) -> str:
    """í•œêµ­ì–´ í…ìŠ¤íŠ¸ ì „ì²˜ë¦¬ (ì‚¬ìš©ì ì œì‹œ ë°©ì•ˆ ì ìš©)"""
    # ìœ ë‹ˆì½”ë“œ ì •ê·œí™” (NFC)
    import unicodedata
    text = unicodedata.normalize('NFC', text.strip())
    
    # ìˆ«ì/ë‹¨ìœ„ ì •ê·œí™”
    text = re.sub(r'(\d+)\s*ì›', r'\1ì›', text)  # "10 ì›" -> "10ì›"
    text = re.sub(r'(\d+)\s*ê°œ', r'\1ê°œ', text)  # "5 ê°œ" -> "5ê°œ"
    text = re.sub(r'(\d+)\s*G', r'\1G', text)    # "5 G" -> "5G"
    
    # ë¶ˆí•„ìš”í•œ ê³µë°± ì œê±°
    text = re.sub(r'\s+', ' ', text)
    
    # ë³µí•©ëª…ì‚¬ ë¶„í•´ (ë””ì»´íŒŒìš´ë“œ)
    decompositions = {
        'ì˜¤ë¥˜ìœ í˜•': 'ì˜¤ë¥˜ ìœ í˜•',
        'ë³µì¡ë„ê³„ì‚°': 'ë³µì¡ë„ ê³„ì‚°',
        'ì¡°ê±´ë¶„ì„': 'ì¡°ê±´ ë¶„ì„',
        'í†µì‹ ë¹„ì²­êµ¬ì ˆì‚­': 'í†µì‹ ë¹„ ì²­êµ¬ ì ˆì‚­',
        'íƒ€ì…ë¶ˆì¼ì¹˜': 'íƒ€ì… ë¶ˆì¼ì¹˜',
        'ì¤‘ë³µì¡°ê±´': 'ì¤‘ë³µ ì¡°ê±´',
        'ìê¸°ëª¨ìˆœ': 'ìê¸° ëª¨ìˆœ'
    }
    
    for compound, decomposed in decompositions.items():
        text = text.replace(compound, decomposed)
    
    return text

def expand_query_terms(query: str) -> str:
    """ì§ˆì˜ ë™ì˜ì–´ í™•ì¥ ë° ì •ê·œí™”"""
    base = preprocess_korean_text(query)
    q_lower = base.lower()
    synonyms = {
        'ì˜¤ë¥˜': ['ì—ëŸ¬', 'ì´ìŠˆ', 'error', 'issue'],
        'ìœ í˜•': ['íƒ€ì…', 'ì¢…ë¥˜', 'type', 'category'],
        'ê²€ì¶œ': ['íƒì§€', 'ê°ì§€', 'detect', 'detection', 'ë¶„ì„', 'analysis', 'analyze'],
        'ì¶”ê°€': ['í™•ì¥', 'ë“±ë¡', 'extend', 'register', 'add'],
        'ë³µì¡ë„': ['complexity', 'metrics', 'ì ìˆ˜', 'ì§€í‘œ'],
        'ê°œìš”': ['overview', 'summary', 'ì„¤ëª…'],
        'ì‹œìŠ¤í…œ': ['system', 'í”„ë¡œì íŠ¸', 'solution', 'ì„œë¹„ìŠ¤'],
    }
    expanded_terms = []
    for key, vals in synonyms.items():
        if key in q_lower:
            expanded_terms.extend(vals)
        for v in vals:
            if v in q_lower:
                expanded_terms.append(key)
    # ì¤‘ë³µ ì œê±°
    expanded_terms = list(dict.fromkeys(expanded_terms))
    if expanded_terms:
        return base + " " + " ".join(expanded_terms)
    return base

def classify_question(query: str) -> str:
    q = query.lower()
    if any(k in q for k in ['ìœ í˜•', 'íƒ€ì…', 'ì¢…ë¥˜']):
        return 'types'
    if any(k in q for k in ['ì¶”ê°€', 'í™•ì¥', 'ë“±ë¡', 'extend', 'register', 'add']):
        return 'extend'
    if any(k in q for k in ['ë³µì¡ë„', 'complexity', 'metrics']):
        return 'complexity'
    return 'general'

def extract_enumeration_bonus(text: str) -> float:
    """ëª©ë¡/ì—´ê±° ì‹ í˜¸ì— ëŒ€í•œ ë³´ë„ˆìŠ¤ ì ìˆ˜"""
    try:
        count = len(re.findall(r'(?m)^\s*(?:\d+\.|[-*])\s+\S+', text or ''))
        if count >= 3:
            return min(0.1, 0.02 * count)
        return 0.0
    except Exception:
        return 0.0

def keywords_bonus(meta: dict, label: str) -> float:
    kw = str((meta or {}).get('keywords', ''))
    if not kw:
        return 0.0
    if label == 'types' and any(w in kw for w in ['ìœ í˜•', 'íƒ€ì…', 'ì¢…ë¥˜', 'type']):
        return 0.05
    if label == 'extend' and any(w in kw for w in ['ì¶”ê°€', 'í™•ì¥', 'ë“±ë¡', 'extend', 'register']):
        return 0.05
    if label == 'complexity' and any(w in kw for w in ['ë³µì¡ë„', 'complexity', 'metrics']):
        return 0.05
    return 0.0

def pair_boost(content: str, label: str) -> float:
    try:
        text = (content or '').lower()
        pairs = []
        if label == 'types':
            pairs = [('ì˜¤ë¥˜','ìœ í˜•'), ('ì´ìŠˆ','íƒ€ì…'), ('error','type')]
        elif label == 'extend':
            pairs = [('ì¶”ê°€','ë“±ë¡'), ('í™•ì¥','ë“±ë¡'), ('extend','register')]
        elif label == 'complexity':
            pairs = [('ë³µì¡ë„','ì ìˆ˜'), ('complexity','metrics')]
        boost = 0.0
        for a,b in pairs:
            if a in text and b in text:
                boost += 0.05
        return min(boost, 0.1)
    except Exception:
        return 0.0

class OpenAIEmbeddingFunction:
    """ChromaDB í˜¸í™˜ OpenAI ì„ë² ë”© í•¨ìˆ˜"""
    
    _MODEL_NAME = "text-embedding-3-small"
    _DIMENSION = 1536

    def name(self) -> str:
        """ChromaDBê°€ ì„ë² ë”© í•¨ìˆ˜ í˜¸í™˜ì„±ì„ ë¹„êµí•  ë•Œ ì‚¬ìš©í•˜ëŠ” ì´ë¦„"""
        return f"openai:{self._MODEL_NAME}"

    def __call__(self, input: List[str]) -> List[List[float]]:
        """ChromaDB ì¸í„°í˜ì´ìŠ¤ì— ë§ëŠ” ì„ë² ë”© í•¨ìˆ˜"""
        embeddings = []
        for text in input:
            try:
                # ì „ì²˜ë¦¬ëœ í…ìŠ¤íŠ¸ ì‚¬ìš©
                processed_text = preprocess_korean_text(text)
                
                print(f"ğŸ” ì„ë² ë”© API í˜¸ì¶œ: {processed_text[:50]}... (í‚¤: {openai.api_key[:20]}...)")
                response = openai.embeddings.create(
                    model=self._MODEL_NAME,
                    input=processed_text
                )
                print(f"âœ… ì„ë² ë”© API ì„±ê³µ: {len(response.data[0].embedding)}ì°¨ì›")
                emb = response.data[0].embedding
                # ì•ˆì „ ì¥ì¹˜: ì„ë² ë”© ê¸¸ì´ ë³´ì •
                if len(emb) != self._DIMENSION:
                    if len(emb) > self._DIMENSION:
                        emb = emb[: self._DIMENSION]
                    else:
                        emb = emb + [0.0] * (self._DIMENSION - len(emb))
                embeddings.append(emb)
            except Exception as e:
                print(f"OpenAI ì„ë² ë”© ì˜¤ë¥˜: {e}")
                # í´ë°±: ê²°ì •ì  ë‚œìˆ˜ ê¸°ë°˜ 1536ì°¨ì› ë²¡í„° ìƒì„±
                import hashlib, random
                seed = int(hashlib.sha256(processed_text.encode()).hexdigest(), 16) % (2**32)
                rnd = random.Random(seed)
                emb = [rnd.random() for _ in range(self._DIMENSION)]
                embeddings.append(emb)
        
        return embeddings

def get_openai_embedding(text: str) -> List[float]:
    """ë‹¨ì¼ í…ìŠ¤íŠ¸ìš© ì„ë² ë”© í•¨ìˆ˜ (í•˜ìœ„ í˜¸í™˜ì„±)"""
    embedding_func = OpenAIEmbeddingFunction()
    return embedding_func([text])[0]

# ChromaDB ì„¤ì •
CHROMA_DIR = Path("rag_storage/chroma_db")
CHROMA_DIR.mkdir(parents=True, exist_ok=True)

# ChromaDB í´ë¼ì´ì–¸íŠ¸ ì´ˆê¸°í™”
client = chromadb.PersistentClient(
    path=str(CHROMA_DIR),
    settings=Settings(
        anonymized_telemetry=False,
        allow_reset=True
    )
)

# BM25 ì¸ë±ìŠ¤ë¥¼ ìœ„í•œ ì „ì—­ ë³€ìˆ˜
bm25_index = None
document_texts = []
document_metadata = []

# ì»¬ë ‰ì…˜ ìƒì„± ë˜ëŠ” ê°€ì ¸ì˜¤ê¸° (OpenAI ì„ë² ë”© ì‚¬ìš©)
embedding_function = OpenAIEmbeddingFunction()

# ë¶€íŒ… ì‹œ ì»¬ë ‰ì…˜ ì´ˆê¸°í™” ì—¬ë¶€ë¥¼ ENVë¡œ ì œì–´ (ê¸°ë³¸ false)
reset_on_start = os.getenv("RAG_RESET_ON_START", "false").lower() == "true"

try:
    if reset_on_start:
        try:
            client.delete_collection("codemuse_documents")
            print("ğŸ—‘ï¸ ê¸°ì¡´ ë¬¸ì„œ ì»¬ë ‰ì…˜ ì‚­ì œë¨")
        except Exception:
            pass
        try:
            client.delete_collection("codemuse_docstrings")
            print("ğŸ—‘ï¸ ê¸°ì¡´ ë…ìŠ¤íŠ¸ë§ ì»¬ë ‰ì…˜ ì‚­ì œë¨")
        except Exception:
            pass

    # ë¬¸ì„œ ì»¬ë ‰ì…˜ í™•ë³´
    try:
        collection = client.get_collection("codemuse_documents", embedding_function=embedding_function)
        print("âœ… ë¬¸ì„œ ì»¬ë ‰ì…˜ ì‚¬ìš© ì¤€ë¹„ ì™„ë£Œ (ê¸°ì¡´)")
    except Exception:
        try:
            collection = client.create_collection(
                name="codemuse_documents",
                metadata={"description": "CodeMuse ë¬¸ì„œ ì €ì¥ì†Œ (OpenAI ì„ë² ë”© 1536ì°¨ì›)"},
                embedding_function=embedding_function
            )
            print("âœ… ë©”ì¸ ì»¬ë ‰ì…˜ 'codemuse_documents' ìƒì„±ë¨ (OpenAI text-embedding-3-small, 1536ì°¨ì›)")
        except Exception:
            # ì´ë¯¸ ì¡´ì¬ ì˜¤ë¥˜ ë“± ì–´ë–¤ ì´ìœ ë¡œë“  create ì‹¤íŒ¨ ì‹œ ìµœì¢… getìœ¼ë¡œ ì‹œë„
            collection = client.get_collection("codemuse_documents", embedding_function=embedding_function)
            print("âœ… ë¬¸ì„œ ì»¬ë ‰ì…˜ ì‚¬ìš© ì¤€ë¹„ ì™„ë£Œ (get-or-create í´ë°±)")

    # ë…ìŠ¤íŠ¸ë§ ì»¬ë ‰ì…˜ í™•ë³´
    try:
        docstring_collection = client.get_collection("codemuse_docstrings", embedding_function=embedding_function)
        print("âœ… ë…ìŠ¤íŠ¸ë§ ì»¬ë ‰ì…˜ ì‚¬ìš© ì¤€ë¹„ ì™„ë£Œ (ê¸°ì¡´)")
    except Exception:
        try:
            docstring_collection = client.create_collection(
                name="codemuse_docstrings",
                metadata={"description": "ë…ìŠ¤íŠ¸ë§ ì „ìš© ê²€ìƒ‰ ì¸ë±ìŠ¤ (OpenAI ì„ë² ë”© 1536ì°¨ì›)"},
                embedding_function=embedding_function
            )
            print("âœ… ë…ìŠ¤íŠ¸ë§ ì»¬ë ‰ì…˜ 'codemuse_docstrings' ìƒì„±ë¨ (OpenAI text-embedding-3-small, 1536ì°¨ì›)")
        except Exception:
            docstring_collection = client.get_collection("codemuse_docstrings", embedding_function=embedding_function)
            print("âœ… ë…ìŠ¤íŠ¸ë§ ì»¬ë ‰ì…˜ ì‚¬ìš© ì¤€ë¹„ ì™„ë£Œ (get-or-create í´ë°±)")

except Exception as e:
    print(f"âŒ ì»¬ë ‰ì…˜ ì¤€ë¹„ ì˜¤ë¥˜: {e}")
    raise

def update_bm25_index():
    """BM25 ì¸ë±ìŠ¤ ì—…ë°ì´íŠ¸"""
    global bm25_index, document_texts, document_metadata
    
    # ëª¨ë“  ë¬¸ì„œ ê°€ì ¸ì˜¤ê¸°
    all_docs = collection.get()
    document_texts = []
    document_metadata = []
    
    for i, content in enumerate(all_docs['documents']):
        if content:
            # í•œêµ­ì–´ í† í¬ë‚˜ì´ì§•
            tokens = korean_tokenize(content)
            document_texts.append(tokens)
            document_metadata.append({
                'id': all_docs['ids'][i],
                'metadata': all_docs['metadatas'][i] if all_docs['metadatas'] else {}
            })
    
    if document_texts:
        bm25_index = BM25Okapi(document_texts)
        print(f"âœ… BM25 ì¸ë±ìŠ¤ ì—…ë°ì´íŠ¸ ì™„ë£Œ: {len(document_texts)}ê°œ ë¬¸ì„œ")
    else:
        bm25_index = None
        print("â„¹ï¸  BM25 ì¸ë±ìŠ¤: ë¬¸ì„œê°€ ì—†ìŠµë‹ˆë‹¤")

# ì´ˆê¸° BM25 ì¸ë±ìŠ¤ ìƒì„±
update_bm25_index()

def hybrid_search(query: str, limit: int = 10) -> List[dict]:
    """í•˜ì´ë¸Œë¦¬ë“œ ê²€ìƒ‰ (BM25 + ë²¡í„°)"""
    global bm25_index, document_metadata
    
    if not bm25_index or not document_metadata:
        return []
    
    # 1. BM25 ê²€ìƒ‰
    query_tokens = korean_tokenize(query)
    bm25_scores = bm25_index.get_scores(query_tokens)
    
    # 2. ë²¡í„° ê²€ìƒ‰
    vector_results = collection.query(
        query_texts=[query],
        n_results=min(limit * 2, 50),
        include=['documents', 'metadatas', 'distances']
    )
    
    # 3. ê²°ê³¼ í†µí•© (Reciprocal Rank Fusion)
    doc_scores = {}
    
    # BM25 ì ìˆ˜ ì •ê·œí™” ë° ì €ì¥
    if len(bm25_scores) > 0:
        max_bm25 = max(bm25_scores)
        min_bm25 = min(bm25_scores)
        if max_bm25 > min_bm25:
            bm25_scores = [(score - min_bm25) / (max_bm25 - min_bm25) for score in bm25_scores]
        
        for i, score in enumerate(bm25_scores):
            if i < len(document_metadata):
                doc_id = document_metadata[i]['id']
                doc_scores[doc_id] = {'bm25': score, 'vector': 0, 'metadata': document_metadata[i]['metadata']}
    
    # ë²¡í„° ì ìˆ˜ ì €ì¥
    if vector_results['documents'] and vector_results['documents'][0]:
        for i, doc_id in enumerate(vector_results['ids'][0]):
            distance = vector_results['distances'][0][i]
            vector_score = 1.0 / (1.0 + distance)  # ê±°ë¦¬ë¥¼ ì ìˆ˜ë¡œ ë³€í™˜
            
            if doc_id in doc_scores:
                doc_scores[doc_id]['vector'] = vector_score
            else:
                doc_scores[doc_id] = {
                    'bm25': 0,
                    'vector': vector_score,
                    'metadata': vector_results['metadatas'][0][i] if vector_results['metadatas'] and vector_results['metadatas'][0] else {}
                }
    
    # 4. RRF ì ìˆ˜ ê³„ì‚° (Reciprocal Rank Fusion)
    final_scores = []
    for doc_id, scores in doc_scores.items():
        # RRF ê³µì‹: 1 / (k + rank)
        # ì—¬ê¸°ì„œëŠ” ì ìˆ˜ë¥¼ rankë¡œ ì‚¬ìš© (ë‚®ì€ ì ìˆ˜ = ë†’ì€ ìˆœìœ„)
        k = 60  # RRF ìƒìˆ˜
        
        # ë²¡í„°ì™€ BM25 ì ìˆ˜ë¥¼ rankë¡œ ë³€í™˜ (1 - score)
        vector_rank = 1.0 - scores['vector']
        bm25_rank = 1.0 - scores['bm25']
        
        # RRF ì ìˆ˜ ê³„ì‚°
        rrf_score = (1.0 / (k + vector_rank)) + (1.0 / (k + bm25_rank))
        
        final_scores.append({
            'id': doc_id,
            'score': rrf_score,
            'bm25_score': scores['bm25'],
            'vector_score': scores['vector'],
            'metadata': scores['metadata']
        })
    
    # 5. ì ìˆ˜ìˆœ ì •ë ¬ ë° ìƒìœ„ ê²°ê³¼ ë°˜í™˜
    final_scores.sort(key=lambda x: x['score'], reverse=True)
    
    # ë¬¸ì„œ ë‚´ìš© ê°€ì ¸ì˜¤ê¸°
    results = []
    for doc in final_scores[:limit]:
        doc_content = collection.get(ids=[doc['id']], include=['documents'])
        if doc_content['documents'] and doc_content['documents'][0]:
            results.append({
                'id': doc['id'],
                'content': doc_content['documents'][0],
                'metadata': doc['metadata'],
                'score': doc['score'],
                'bm25_score': doc['bm25_score'],
                'vector_score': doc['vector_score'],
                'similarity_score': doc['vector_score'],
                'folder_priority': _get_folder_priority(doc['metadata'])
            })
    
    return results

@app.get("/")
async def root():
    return {"service": "RAG Engine", "status": "running", "port": 8003, "storage": "ChromaDB"}

@app.get("/health")
async def health():
    try:
        # ì»¬ë ‰ì…˜ ìƒíƒœ í™•ì¸
        count = collection.count()
        return {
            "status": "healthy", 
            "service": "RAG Engine",
            "storage": "ChromaDB",
            "documents_count": count
        }
    except Exception as e:
        return {"status": "unhealthy", "error": str(e)}

def _get_folder_priority(metadata):
    """í´ë”ë³„ ìš°ì„ ìˆœìœ„ ì ìˆ˜ ê³„ì‚°"""
    source = metadata.get('source', '')
    
    # í´ë”ë³„ ìš°ì„ ìˆœìœ„ (ë†’ì„ìˆ˜ë¡ ìš°ì„ )
    priority_map = {
        'analyzers': 100,      # í•µì‹¬ ë¶„ì„ê¸° (ê°€ì¥ ì¤‘ìš”)
        'shared': 80,          # ê³µìœ  ëª¨ë“ˆ
        'streaming': 60,       # ìŠ¤íŠ¸ë¦¬ë° ëª¨ë“ˆ
        'formatters': 40,      # í¬ë§¤í„° ëª¨ë“ˆ
        'templates': 20,       # í…œí”Œë¦¿ (ë‚®ì€ ìš°ì„ ìˆœìœ„)
        'options': 20,         # ì˜µì…˜ (ë‚®ì€ ìš°ì„ ìˆœìœ„)
        'utils': 30,           # ìœ í‹¸ë¦¬í‹°
        'config': 50,          # ì„¤ì •
        'protocols': 40,       # í”„ë¡œí† ì½œ
    }
    
    # íŒŒì¼ ê²½ë¡œì—ì„œ í´ë” ì¶”ì¶œ
    for folder, priority in priority_map.items():
        if f'/{folder}/' in source or f'\\{folder}\\' in source:
            return priority
    
    # ê¸°ë³¸ ìš°ì„ ìˆœìœ„
    return 10

def _calculate_enhanced_score(semantic_score, metadata):
    """ì˜ë¯¸ì  ì ìˆ˜ + í´ë” ìš°ì„ ìˆœìœ„ë¡œ ìµœì¢… ì ìˆ˜ ê³„ì‚°"""
    folder_priority = _get_folder_priority(metadata)
    
    # ì˜ë¯¸ì  ì ìˆ˜ (0-1) + í´ë” ìš°ì„ ìˆœìœ„ (0-100)ì„ ê²°í•©
    # í´ë” ìš°ì„ ìˆœìœ„ë¥¼ 0.1 ìŠ¤ì¼€ì¼ë¡œ ì¡°ì •í•˜ì—¬ ì˜ë¯¸ì  ì ìˆ˜ì— ë”í•¨
    enhanced_score = semantic_score + (folder_priority * 0.01)
    
    return min(enhanced_score, 1.0)  # ìµœëŒ€ 1.0ìœ¼ë¡œ ì œí•œ

@app.post("/api/v1/search", response_model=SearchResponse)
async def search_documents(request: SearchRequest):
    """ë¬¸ì„œ ê²€ìƒ‰ API (í•˜ì´ë¸Œë¦¬ë“œ ê²€ìƒ‰: BM25 + ë²¡í„°)"""
    try:
        print(f"ğŸ” í•˜ì´ë¸Œë¦¬ë“œ ê²€ìƒ‰ ìš”ì²­: '{request.query}' (limit: {request.limit})")
        question_label = classify_question(request.query)
        expanded = expand_query_terms(request.query)

        # í•˜ì´ë¸Œë¦¬ë“œ ê²€ìƒ‰ ìˆ˜í–‰ (ì—¬ìœ ë¶„ìœ¼ë¡œ ë” ë§ì´)
        raw_results = hybrid_search(expanded, max(request.limit * 3, request.limit))

        # ê²°ê³¼ë¥¼ í‘œì¤€ í˜•ì‹ìœ¼ë¡œ ë³€í™˜
        formatted_results = []
        for result in raw_results:
            formatted_results.append({
                "id": result['id'],
                "content": result['content'],
                "metadata": result['metadata'],
                "score": result['score'],
                "similarity_score": result['vector_score'],
                "semantic_score": result['vector_score'],
                "bm25_score": result['bm25_score'],
                "folder_priority": result['folder_priority']
            })

        # 1) í•„í„° ì ìš©
        def apply_filters(items: List[dict]) -> List[dict]:
            filtered = items
            # chunk_type_filter (íŒŒì´í”„ êµ¬ë¶„ì ì§€ì›)
            if request.chunk_type_filter:
                allow_types = [t.strip().lower() for t in str(request.chunk_type_filter).split('|') if t.strip()]
                filtered = [x for x in filtered if str(x.get('metadata', {}).get('chunk_type', '')).lower() in allow_types]
            # exclude_chunk_types
            if request.exclude_chunk_types:
                exclude_set = set([t.strip().lower() for t in request.exclude_chunk_types if isinstance(t, str)])
                filtered = [x for x in filtered if str(x.get('metadata', {}).get('chunk_type', '')).lower() not in exclude_set]
            # file_filter (ë¶€ë¶„ ë¬¸ìì—´ ë§¤ì¹­)
            if request.file_filter:
                def match_file(meta: dict) -> bool:
                    filename = (meta.get('filename') or '')
                    source_file = (meta.get('source_file') or '')
                    source = (meta.get('source') or '')
                    target = f"{filename} {source_file} {source}".lower()
                    for f in request.file_filter:
                        if isinstance(f, str) and f.strip().lower() in target:
                            return True
                    return False
                filtered = [x for x in filtered if match_file(x.get('metadata', {}))]
            return filtered

        filtered_results = apply_filters(formatted_results)

        # ì¬ë­í‚¹: ì—´ê±° ë³´ë„ˆìŠ¤ + í‚¤ì›Œë“œ ë³´ë„ˆìŠ¤ + ì˜ë¯¸ìŒ ë³´ë„ˆìŠ¤(ì¼ë°˜ ê·œì¹™)
        for r in filtered_results:
            enum_boost = extract_enumeration_bonus(r.get('content', ''))
            kw_boost = keywords_bonus(r.get('metadata', {}), question_label)
            pair = pair_boost(r.get('content',''), question_label)
            r['score'] = r.get('score', 0) + enum_boost + kw_boost + pair

            # ì¶”ê°€ ê°€ì¤‘ì¹˜: 'types' ì˜ë„ ì‹œ class/function ìš°ì„ , overview ë¯¸ì„¸ ê°ì 
            ct = str(r.get('metadata', {}).get('chunk_type', '')).lower()
            if question_label == 'types':
                if ct in ('class', 'function'):
                    r['score'] += 0.05
                elif ct == 'overview':
                    r['score'] -= 0.02

        # 2) í´ë” ìš°ì„ ìˆœìœ„ ì •ë ¬ (analyzers ìš°ì„ ), í´ë” ë‚´ ì ìˆ˜ìˆœ
        filtered_results.sort(key=lambda x: (x['folder_priority'], x['score']), reverse=True)

        # 3) ìƒìœ„ limit ì¶”ì¶œ
        final_results = filtered_results[:request.limit]

        print(f"ğŸ“Š í•˜ì´ë¸Œë¦¬ë“œ ê²€ìƒ‰ ê²°ê³¼ ë¶„ì„ (í•„í„° ì ìš© í›„ {len(final_results)}/{len(filtered_results)}):")
        for i, result in enumerate(final_results):
            folder = result['metadata'].get('source', '').split('/')[-2] if '/' in result['metadata'].get('source', '') else 'unknown'
            print(f"  {i+1}. {result['metadata'].get('filename', 'N/A')} (í´ë”: {folder}, ì ìˆ˜: {result['score']:.3f})")

        print(f"âœ… ê²€ìƒ‰ ì™„ë£Œ: {len(final_results)}ê°œ ê²°ê³¼ ë°˜í™˜")

        return SearchResponse(
            results=final_results,
            query=request.query,
            total_found=len(final_results)
        )
        
    except Exception as e:
        print(f"âŒ ê²€ìƒ‰ ì˜¤ë¥˜: {str(e)}")
        raise HTTPException(status_code=500, detail=f"ê²€ìƒ‰ ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {str(e)}")

@app.get("/documents")
async def list_documents():
    """ì €ì¥ëœ ë¬¸ì„œ ëª©ë¡ ì¡°íšŒ"""
    try:
        # ëª¨ë“  ë¬¸ì„œ ê°€ì ¸ì˜¤ê¸°
        results = collection.get()
        
        documents = []
        if results['documents']:
            for i, doc in enumerate(results['documents']):
                documents.append({
                    "id": results['ids'][i],
                    "content": doc,
                    "metadata": results['metadatas'][i] if results['metadatas'] else {}
                })
        
        return {"documents": documents, "count": len(documents)}
    except Exception as e:
        raise HTTPException(status_code=500, detail=f"ë¬¸ì„œ ëª©ë¡ ì¡°íšŒ ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {str(e)}")

@app.get("/api/v1/documents/folders")
async def get_folder_structure():
    """í´ë” êµ¬ì¡° ê¸°ë°˜ ë¬¸ì„œ ëª©ë¡ ì¡°íšŒ (ê°œì„ ëœ êµ¬ì¡°)"""
    try:
        results = collection.get()
        
        # í”„ë¡œì íŠ¸ë³„ í´ë” êµ¬ì¡° ìƒì„±
        project_structure = {}
        
        if results['documents']:
            for i, doc in enumerate(results['documents']):
                doc_id = results['ids'][i]
                metadata = results['metadatas'][i] if results['metadatas'] else {}
                source = metadata.get('source', '')
                
                # ê²½ë¡œì—ì„œ í”„ë¡œì íŠ¸ì™€ í´ë” êµ¬ì¡° ì¶”ì¶œ
                if '/' in source:
                    path_parts = source.split('/')
                    
                    # generated_docs/raas-rule-analyzer/src/raas_rule_analyzer/analyzers/issue_detector.md
                    # -> í”„ë¡œì íŠ¸: raas-rule-analyzer, í´ë”: analyzers, íŒŒì¼: issue_detector.md
                    
                    if 'generated_docs' in path_parts:
                        # generated_docs/í”„ë¡œì íŠ¸ëª…/... í˜•íƒœ
                        if len(path_parts) > 2:
                            # generated_docs/rule_analyzer/analyzers/file.md
                            project_name = path_parts[1]
                            remaining_parts = path_parts[2:]  # generated_docs/í”„ë¡œì íŠ¸ëª… ì œê±°
                        else:
                            # generated_docs/project_summary.md (ë£¨íŠ¸ ë ˆë²¨ íŒŒì¼)
                            project_name = 'sample_code'  # ë¶„ì„ ëŒ€ìƒ í”„ë¡œì íŠ¸ëª…ìœ¼ë¡œ ë¶„ë¥˜
                            remaining_parts = [path_parts[1]]  # íŒŒì¼ëª…ë§Œ
                    else:
                        # ì§ì ‘ì ì¸ ê²½ë¡œ
                        project_name = 'sample_code'
                        remaining_parts = path_parts
                    
                    if remaining_parts:
                        filename = remaining_parts[-1]
                        folder_parts = remaining_parts[:-1]
                        
                        # í´ë” ê²½ë¡œ ìƒì„± (í”„ë¡œì íŠ¸ ë‚´ë¶€ êµ¬ì¡°ë§Œ)
                        if folder_parts:
                            # src/raas_rule_analyzer/analyzers -> analyzers
                            # src/raas_rule_analyzer/formatters/options -> formatters/options
                            if 'src' in folder_parts and len(folder_parts) > 2:
                                # src/raas_rule_analyzer ì œê±°í•˜ê³  ë‚˜ë¨¸ì§€ ê²½ë¡œ ìœ ì§€
                                src_index = folder_parts.index('src')
                                if src_index + 2 < len(folder_parts):
                                    folder_path = '/'.join(folder_parts[src_index + 2:])
                                else:
                                    folder_path = 'ë£¨íŠ¸'
                            else:
                                folder_path = '/'.join(folder_parts)
                        else:
                            folder_path = 'ë£¨íŠ¸'
                    else:
                        folder_path = 'ë£¨íŠ¸'
                        filename = source
                else:
                    project_name = 'ê¸°íƒ€'
                    folder_path = 'ë£¨íŠ¸'
                    filename = source
                
                # í”„ë¡œì íŠ¸ë³„ êµ¬ì¡° ìƒì„±
                if project_name not in project_structure:
                    project_structure[project_name] = {}
                
                if folder_path not in project_structure[project_name]:
                    project_structure[project_name][folder_path] = []
                
                project_structure[project_name][folder_path].append({
                    "id": doc_id,
                    "filename": filename,
                    "source": source,
                    "created_at": metadata.get('created_at', ''),
                    "type": metadata.get('type', 'document'),
                    "category": metadata.get('category', 'general'),
                    "preview": doc[:150] + "..." if len(doc) > 150 else doc
                })
        
        # í´ë”ë³„ë¡œ ì •ë ¬
        for project in project_structure:
            for folder in project_structure[project]:
                project_structure[project][folder].sort(key=lambda x: x['filename'])
        
        return {
            "projects": project_structure,
            "total_documents": len(results['ids']) if results['ids'] else 0,
            "project_count": len(project_structure)
        }
        
    except Exception as e:
        raise HTTPException(status_code=500, detail=f"í´ë” êµ¬ì¡° ì¡°íšŒ ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {str(e)}")

@app.get("/api/v1/documents/{document_id}")
async def get_document_detail(document_id: str):
    """íŠ¹ì • ë¬¸ì„œ ìƒì„¸ ì¡°íšŒ (ê°„ë‹¨í•œ ë§í¬ ë§¤í•‘ ì§€ì›)"""
    try:
        # ê°„ë‹¨í•œ ë§í¬ ë§¤í•‘ì—ì„œ ì‹¤ì œ ë¬¸ì„œ ID ì°¾ê¸°
        real_doc_id = SIMPLE_LINK_MAPPING.get(document_id, document_id)
        print(f"ğŸ” ë¬¸ì„œ ìƒì„¸ ì¡°íšŒ - ë§í¬ ë§¤í•‘: '{document_id}' â†’ '{real_doc_id}'")
        
        # ì‹¤ì œ ë¬¸ì„œ IDë¡œ ì¡°íšŒ
        results = collection.get(ids=[real_doc_id])
        
        if not results['ids']:
            raise HTTPException(status_code=404, detail="ë¬¸ì„œë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤.")
        
        doc_id = results['ids'][0]
        content = results['documents'][0]
        metadata = results['metadatas'][0] if results['metadatas'] else {}
        
        return {
            "id": doc_id,
            "content": content,
            "metadata": metadata,
            "word_count": len(content.split()),
            "char_count": len(content),
            "match_type": "link_mapping" if document_id != real_doc_id else "direct_id"
        }
        
    except HTTPException:
        raise
    except Exception as e:
        raise HTTPException(status_code=500, detail=f"ë¬¸ì„œ ìƒì„¸ ì¡°íšŒ ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {str(e)}")

@app.get("/api/v1/documents/{document_id}/content")
async def get_document_content(document_id: str):
    """íŠ¹ì • ë¬¸ì„œ ë‚´ìš©ë§Œ ì¡°íšŒ (ê°„ë‹¨í•œ ë§í¬ ë§¤í•‘ ì§€ì›)"""
    try:
        # ê°„ë‹¨í•œ ë§í¬ ë§¤í•‘ì—ì„œ ì‹¤ì œ ë¬¸ì„œ ID ì°¾ê¸°
        real_doc_id = SIMPLE_LINK_MAPPING.get(document_id, document_id)
        print(f"ğŸ” ë§í¬ ë§¤í•‘: '{document_id}' â†’ '{real_doc_id}'")
        
        # ì‹¤ì œ ë¬¸ì„œ IDë¡œ ì¡°íšŒ
        results = collection.get(ids=[real_doc_id])
        
        if results['ids']:
            doc_id = results['ids'][0]
            content = results['documents'][0]
            metadata = results['metadatas'][0] if results['metadatas'] else {}
            
            return {
                "id": doc_id,
                "content": content,
                "metadata": metadata
            }
        else:
            raise HTTPException(status_code=404, detail="ë¬¸ì„œë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤.")
            
    except HTTPException:
        raise
    except Exception as e:
        raise HTTPException(status_code=500, detail=f"ë¬¸ì„œ ë‚´ìš© ì¡°íšŒ ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {str(e)}")

@app.get("/api/v1/documents/{path:path}")
async def get_document_by_path(path: str):
    """ê²½ë¡œ ê¸°ë°˜ ë¬¸ì„œ ì¡°íšŒ (ê°„ë‹¨í•œ ë§í¬ ë§¤í•‘ ì§€ì›)"""
    try:
        # URL ë””ì½”ë”© ë° ê²½ë¡œ ì •ê·œí™”
        import urllib.parse
        decoded_path = urllib.parse.unquote(path)
        print(f"ğŸ” ê²½ë¡œ ê¸°ë°˜ ë¬¸ì„œ ì¡°íšŒ: {decoded_path}")
        
        # ê°„ë‹¨í•œ ë§í¬ ë§¤í•‘ì—ì„œ ì‹¤ì œ ë¬¸ì„œ ID ì°¾ê¸°
        real_doc_id = SIMPLE_LINK_MAPPING.get(decoded_path, decoded_path)
        print(f"ğŸ” ë§í¬ ë§¤í•‘: '{decoded_path}' â†’ '{real_doc_id}'")
        
        # ì‹¤ì œ ë¬¸ì„œ IDë¡œ ì¡°íšŒ
        results = collection.get(ids=[real_doc_id])
        
        if results['ids']:
            doc_id = results['ids'][0]
            content = results['documents'][0]
            metadata = results['metadatas'][0] if results['metadatas'] else {}
            
            return {
                "id": doc_id,
                "content": content,
                "metadata": metadata,
                "word_count": len(content.split()),
                "char_count": len(content),
                "match_type": "link_mapping",
                "all_matches": 1
            }
        
        # ë¬¸ì„œë¥¼ ì°¾ì„ ìˆ˜ ì—†ëŠ” ê²½ìš°
        raise HTTPException(status_code=404, detail="ë¬¸ì„œë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤.")
        
    except HTTPException:
        raise
    except Exception as e:
        raise HTTPException(status_code=500, detail=f"ë¬¸ì„œ ì¡°íšŒ ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {str(e)}")
        for i, doc_id in enumerate(all_docs['ids']):
            metadata = all_docs['metadatas'][i] if all_docs['metadatas'] else {}
            source = metadata.get('source', '')
            filename = metadata.get('filename', '')
            chunk_type = metadata.get('chunk_type', '')
            
            print(f"  ğŸ“„ ë¬¸ì„œ {i}: ID='{doc_id}', filename='{filename}', chunk_type='{chunk_type}', source='{source}'")
            
            # ê²½ë¡œ ë§¤ì¹­ ì‹œë„
            # 1. ì •í™•í•œ ID ë§¤ì¹­ (ìµœìš°ì„ )
            if doc_id == decoded_path:
                print(f"    âœ… ì •í™•í•œ ID ë§¤ì¹­!")
                matched_docs.append({
                    'id': doc_id,
                    'content': all_docs['documents'][i],
                    'metadata': metadata,
                    'match_type': 'exact_id'
                })
            # 2. ì „ì²´ ê²½ë¡œ ë§¤ì¹­
            elif decoded_path in source or source in decoded_path:
                print(f"    âœ… source_path ë§¤ì¹­!")
                matched_docs.append({
                    'id': doc_id,
                    'content': all_docs['documents'][i],
                    'metadata': metadata,
                    'match_type': 'source_path'
                })
            # 3. íŒŒì¼ëª… + ì²­í¬ íƒ€ì… ë§¤ì¹­
            elif filename and chunk_type:
                # sample_code/rule_analyzer/exceptions.py/file overview í˜•íƒœë¡œ ë§¤ì¹­
                # .md íŒŒì¼ëª…ì„ .pyë¡œ ë³€í™˜í•˜ì—¬ ë§¤ì¹­
                py_filename = filename.replace('.md', '.py')
                expected_pattern = f"{py_filename}/{chunk_type}"
                if expected_pattern.lower() in decoded_path.lower():
                    print(f"    âœ… filename_chunk ë§¤ì¹­! íŒ¨í„´: '{expected_pattern}'")
                    matched_docs.append({
                        'id': doc_id,
                        'content': all_docs['documents'][i],
                        'metadata': metadata,
                        'match_type': 'filename_chunk'
                    })
            # 4. íŒŒì¼ëª…ë§Œ ë§¤ì¹­ (ì²­í¬ íƒ€ì…ì´ 'overview'ì¸ ê²½ìš°)
            elif filename and 'overview' in decoded_path.lower():
                if filename.lower() in decoded_path.lower():
                    print(f"    âœ… filename_overview ë§¤ì¹­!")
                    matched_docs.append({
                        'id': doc_id,
                        'content': all_docs['documents'][i],
                        'metadata': metadata,
                        'match_type': 'filename_overview'
                    })
            # 5. ìƒˆë¡œìš´ ë§í¬ í˜•ì‹ ë§¤ì¹­ (class-issuedetector, class_IssueDetector)
            # class-issuedetector í˜•ì‹ ë§¤ì¹­ (í”„ë¡ íŠ¸ì—”ë“œ í˜¸í™˜)
            elif decoded_path.startswith('class-'):
                class_name = decoded_path.replace('class-', '').lower()
                # í´ë˜ìŠ¤ëª…ì´ contentì— í¬í•¨ë˜ì–´ ìˆëŠ”ì§€ í™•ì¸ (ì¡°ê±´ ì™„í™”)
                content_lower = all_docs['documents'][i].lower()
                if class_name in content_lower:
                    print(f"    âœ… class_dash_link ë§¤ì¹­! í´ë˜ìŠ¤: '{class_name}', chunk_type: '{chunk_type}'")
                    matched_docs.append({
                        'id': doc_id,
                        'content': all_docs['documents'][i],
                        'metadata': metadata,
                        'match_type': 'class_dash_link'
                    })
            # class_IssueDetector í˜•ì‹ ë§¤ì¹­
            elif decoded_path.startswith('class_'):
                class_name = decoded_path.replace('class_', '').lower()
                # í´ë˜ìŠ¤ëª…ì´ contentì— í¬í•¨ë˜ì–´ ìˆëŠ”ì§€ í™•ì¸
                content_lower = all_docs['documents'][i].lower()
                if class_name in content_lower and chunk_type == 'class':
                    print(f"    âœ… class_underscore_link ë§¤ì¹­! í´ë˜ìŠ¤: '{class_name}'")
                    matched_docs.append({
                        'id': doc_id,
                        'content': all_docs['documents'][i],
                        'metadata': metadata,
                        'match_type': 'class_underscore_link'
                    })
            # func_detect_type_mismatch_157 í˜•ì‹ ë§¤ì¹­
            elif decoded_path.startswith('func_'):
                func_parts = decoded_path.replace('func_', '').split('_')
                if len(func_parts) >= 2:
                    func_name = '_'.join(func_parts[:-1])  # ë§ˆì§€ë§‰ ë¶€ë¶„(ì¤„ë²ˆí˜¸) ì œì™¸
                    line_number = func_parts[-1]  # ë§ˆì§€ë§‰ ë¶€ë¶„ì´ ì¤„ë²ˆí˜¸
                    content_lower = all_docs['documents'][i].lower()
                    if func_name in content_lower and chunk_type == 'function':
                        print(f"    âœ… func_link ë§¤ì¹­! í•¨ìˆ˜: '{func_name}', ì¤„: '{line_number}'")
                        matched_docs.append({
                            'id': doc_id,
                            'content': all_docs['documents'][i],
                            'metadata': metadata,
                            'match_type': 'func_link'
                        })
            # file_issue_detector.py í˜•ì‹ ë§¤ì¹­
            elif decoded_path.startswith('file_'):
                file_name = decoded_path.replace('file_', '')
                if filename and file_name.lower() in filename.lower():
                    print(f"    âœ… file_link ë§¤ì¹­! íŒŒì¼: '{file_name}'")
                    matched_docs.append({
                        'id': doc_id,
                        'content': all_docs['documents'][i],
                        'metadata': metadata,
                        'match_type': 'file_link'
                    })
        
        if not matched_docs:
            # ë” ìœ ì—°í•œ ë§¤ì¹­ ì‹œë„
            path_parts = decoded_path.lower().split('/')
            for i, doc_id in enumerate(all_docs['ids']):
                metadata = all_docs['metadatas'][i] if all_docs['metadatas'] else {}
                source = metadata.get('source', '').lower()
                filename = metadata.get('filename', '').lower()
                chunk_type = metadata.get('chunk_type', '').lower()
                content_lower = all_docs['documents'][i].lower()
                
                # class-issuedetector í˜•ì‹ íŠ¹ë³„ ì²˜ë¦¬
                if decoded_path == 'class-issuedetector':
                    if 'issuedetector' in content_lower and chunk_type == 'class':
                        print(f"    âœ… class-issuedetector íŠ¹ë³„ ë§¤ì¹­!")
                        matched_docs.append({
                            'id': doc_id,
                            'content': all_docs['documents'][i],
                            'metadata': metadata,
                            'match_type': 'class_special_match'
                        })
                        break  # ì²« ë²ˆì§¸ ë§¤ì¹­ë§Œ ì‚¬ìš©
                
                # ê²½ë¡œì˜ ë§ˆì§€ë§‰ ë¶€ë¶„ì´ íŒŒì¼ëª…ê³¼ ë§¤ì¹­ë˜ëŠ”ì§€ í™•ì¸
                elif path_parts and filename:
                    last_part = path_parts[-1]
                    if (filename.replace('.py', '').replace('.md', '') in last_part or 
                        last_part in filename.replace('.py', '').replace('.md', '')):
                        matched_docs.append({
                            'id': doc_id,
                            'content': all_docs['documents'][i],
                            'metadata': metadata,
                            'match_type': 'flexible_match'
                        })
        
        if not matched_docs:
            raise HTTPException(status_code=404, detail=f"ê²½ë¡œ '{decoded_path}'ì— í•´ë‹¹í•˜ëŠ” ë¬¸ì„œë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤.")
        
        # ê°€ì¥ ì í•©í•œ ë§¤ì¹­ ê²°ê³¼ ì„ íƒ (ìš°ì„ ìˆœìœ„: exact_id > source_path > filename_chunk > filename_overview > flexible_match)
        priority_order = ['exact_id', 'source_path', 'filename_chunk', 'filename_overview', 'flexible_match']
        best_match = None
        for priority in priority_order:
            for doc in matched_docs:
                if doc['match_type'] == priority:
                    best_match = doc
                    break
            if best_match:
                break
        
        # ê¸°ë³¸ì ìœ¼ë¡œ ì²« ë²ˆì§¸ ë§¤ì¹­ ê²°ê³¼ ì‚¬ìš©
        if not best_match:
            best_match = matched_docs[0]
        
        content = best_match['content']
        metadata = best_match['metadata']
        
        print(f"âœ… ë¬¸ì„œ ì°¾ìŒ: {metadata.get('filename', 'N/A')} (íƒ€ì…: {metadata.get('chunk_type', 'N/A')})")
        
        return {
            "id": best_match['id'],
            "content": content,
            "metadata": metadata,
            "word_count": len(content.split()),
            "char_count": len(content),
            "match_type": best_match['match_type'],
            "all_matches": len(matched_docs)
        }
        
    except HTTPException:
        raise
    except Exception as e:
        print(f"âŒ ê²½ë¡œ ê¸°ë°˜ ë¬¸ì„œ ì¡°íšŒ ì˜¤ë¥˜: {str(e)}")
        raise HTTPException(status_code=500, detail=f"ë¬¸ì„œ ì¡°íšŒ ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {str(e)}")

@app.get("/api/v1/code/search")
async def search_source_code(query: str, limit: int = 10):
    """ì†ŒìŠ¤ì½”ë“œì—ì„œ íŠ¹ì • ë¡œì§ì˜ ìœ„ì¹˜ë¥¼ ì°¾ëŠ” API"""
    try:
        import os
        import re
        from pathlib import Path
        
        print(f"ğŸ” ì†ŒìŠ¤ì½”ë“œ ê²€ìƒ‰: '{query}'")
        
        # ê²€ìƒ‰í•  ë””ë ‰í† ë¦¬ ì„¤ì • (sample_codeë§Œ ê²€ìƒ‰)
        search_dirs = [
            "sample_code"
        ]
        
        matches = []
        
        for search_dir in search_dirs:
            if not os.path.exists(search_dir):
                continue
                
            # Python íŒŒì¼ ê²€ìƒ‰
            for root, dirs, files in os.walk(search_dir):
                for file in files:
                    if file.endswith('.py'):
                        file_path = os.path.join(root, file)
                        try:
                            with open(file_path, 'r', encoding='utf-8') as f:
                                lines = f.readlines()
                                
                            # íŒŒì¼ ë‚´ìš©ì—ì„œ ê²€ìƒ‰
                            for line_num, line in enumerate(lines, 1):
                                if re.search(query.lower(), line.lower(), re.IGNORECASE):
                                    # í•¨ìˆ˜ë‚˜ í´ë˜ìŠ¤ ì •ì˜ ì°¾ê¸°
                                    context_start = max(0, line_num - 5)
                                    context_end = min(len(lines), line_num + 5)
                                    context = ''.join(lines[context_start:context_end])
                                    
                                    matches.append({
                                        "file": file_path,
                                        "line": line_num,
                                        "content": line.strip(),
                                        "context": context,
                                        "relative_path": os.path.relpath(file_path)
                                    })
                                    
                                    if len(matches) >= limit:
                                        break
                                        
                        except Exception as e:
                            print(f"âŒ íŒŒì¼ ì½ê¸° ì˜¤ë¥˜ {file_path}: {e}")
                            continue
                            
                if len(matches) >= limit:
                    break
                    
            if len(matches) >= limit:
                break
        
        print(f"âœ… {len(matches)}ê°œ ë§¤ì¹­ ê²°ê³¼ ë°œê²¬")
        
        return {
            "query": query,
            "matches": matches[:limit],
            "total_found": len(matches)
        }
        
    except Exception as e:
        print(f"âŒ ì†ŒìŠ¤ì½”ë“œ ê²€ìƒ‰ ì˜¤ë¥˜: {str(e)}")
        raise HTTPException(status_code=500, detail=f"ì†ŒìŠ¤ì½”ë“œ ê²€ìƒ‰ ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {str(e)}")

@app.get("/api/v1/files/list")
async def list_md_files():
    """MD íŒŒì¼ ë¦¬ìŠ¤íŠ¸ ì¡°íšŒ (ì‚¬ëŒìš© ë¸Œë¼ìš°ì§•) - ê°„ë‹¨í•œ êµ¬ì¡°"""
    try:
        generated_docs_path = "generated_docs"
        if not os.path.exists(generated_docs_path):
            return {
                "files": [],
                "total_files": 0,
                "folders": []
            }
        
        files = []
        folders = set()
        
        for root, dirs, filenames in os.walk(generated_docs_path):
            for filename in filenames:
                if filename.endswith('.md'):
                    file_path = os.path.join(root, filename)
                    relative_path = os.path.relpath(file_path, generated_docs_path)
                    
                    # íŒŒì¼ ì •ë³´
                    stat = os.stat(file_path)
                    file_size = stat.st_size
                    
                    # í´ë” ê²½ë¡œ ì¶”ì¶œ
                    path_parts = relative_path.split(os.sep)
                    folder_path = "/".join(path_parts[:-1]) if len(path_parts) > 1 else "ë£¨íŠ¸"
                    folders.add(folder_path)
                    
                    file_info = {
                        "id": filename.replace('.md', ''),
                        "name": filename,
                        "path": relative_path,
                        "folder": folder_path,
                        "size": file_size,
                        "size_readable": f"{file_size / 1024:.1f} KB" if file_size > 1024 else f"{file_size} bytes",
                        "created_at": stat.st_ctime,
                        "modified_at": stat.st_mtime
                    }
                    
                    files.append(file_info)
        
        return {
            "files": files,
            "total_files": len(files),
            "folders": sorted(list(folders))
        }
        
    except Exception as e:
        raise HTTPException(status_code=500, detail=f"MD íŒŒì¼ ëª©ë¡ ì¡°íšŒ ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {str(e)}")

@app.get("/api/v1/files/{file_id}")
async def get_md_file_content(file_id: str):
    """MD íŒŒì¼ ë‚´ìš© ì¡°íšŒ (ì‚¬ëŒìš© ë¸Œë¼ìš°ì§•)"""
    try:
        generated_docs_path = "generated_docs"
        
        # íŒŒì¼ ì°¾ê¸°
        md_file_path = None
        for root, dirs, filenames in os.walk(generated_docs_path):
            for filename in filenames:
                if filename.endswith('.md') and filename.replace('.md', '') == file_id:
                    md_file_path = os.path.join(root, filename)
                    break
            if md_file_path:
                break
        
        if not md_file_path or not os.path.exists(md_file_path):
            raise HTTPException(status_code=404, detail="MD íŒŒì¼ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤.")
        
        # íŒŒì¼ ë‚´ìš© ì½ê¸°
        with open(md_file_path, 'r', encoding='utf-8') as f:
            content = f.read()
        
        # íŒŒì¼ ì •ë³´
        stat = os.stat(md_file_path)
        relative_path = os.path.relpath(md_file_path, generated_docs_path)
        
        return {
            "id": file_id,
            "filename": os.path.basename(md_file_path),
            "path": relative_path,
            "content": content,
            "size": stat.st_size,
            "created_at": stat.st_ctime,
            "modified_at": stat.st_mtime,
            "word_count": len(content.split()),
            "char_count": len(content),
            "line_count": len(content.splitlines())
        }
        
    except HTTPException:
        raise
    except Exception as e:
        raise HTTPException(status_code=500, detail=f"MD íŒŒì¼ ë‚´ìš© ì¡°íšŒ ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {str(e)}")

@app.post("/api/v1/documents")
async def add_document(request: DocumentRequest):
    """ìƒˆ ë¬¸ì„œ ì¶”ê°€"""
    try:
        # ë¬¸ì„œ ID ìƒì„±
        doc_id = request.id or str(uuid.uuid4())
        
        # ë©”íƒ€ë°ì´í„° ì¤€ë¹„
        metadata = request.metadata or {}
        
        # tagsê°€ ë¦¬ìŠ¤íŠ¸ì¸ ê²½ìš° ë¬¸ìì—´ë¡œ ë³€í™˜ (ChromaDB í˜¸í™˜ì„±)
        if "tags" in metadata and isinstance(metadata["tags"], list):
            metadata["tags"] = ", ".join(metadata["tags"])
            
        metadata.update({
            "created_at": datetime.now().isoformat(),
            "source": metadata.get("source", "unknown")
        })
        
        # ChromaDBì— ë¬¸ì„œ upsert (ì•ˆì •ì  IDë¡œ ìµœì‹  ìƒíƒœ ìœ ì§€)
        try:
            collection.upsert(
                documents=[request.content],
                metadatas=[metadata],
                ids=[doc_id]
            )
        except Exception:
            # êµ¬ë²„ì „ í˜¸í™˜: ì¡´ì¬ ì‹œ update, ì—†ìœ¼ë©´ add
            try:
                existing = collection.get(ids=[doc_id])
                if existing and existing.get('ids'):
                    collection.update(ids=[doc_id], metadatas=[metadata], documents=[request.content])
                else:
                    collection.add(ids=[doc_id], metadatas=[metadata], documents=[request.content])
            except Exception as inner:
                raise HTTPException(status_code=500, detail=f"ë¬¸ì„œ upsert ì‹¤íŒ¨: {inner}")
        
        # ë…ìŠ¤íŠ¸ë§ì´ ìˆëŠ” ê²½ìš° ë…ìŠ¤íŠ¸ë§ ì»¬ë ‰ì…˜ì—ë„ upsert
        docstring = metadata.get("docstring")
        if docstring and docstring.strip():
            docstring_id = f"docstring_{doc_id}"
            docstring_metadata = {
                "parent_id": doc_id,
                "chunk_type": metadata.get("chunk_type", "unknown"),
                "name": metadata.get("name", "unknown"),
                "filename": metadata.get("filename", "unknown"),
                "project": metadata.get("project", "unknown"),
                "source": metadata.get("source", "unknown"),
                "created_at": datetime.now().isoformat()
            }
            
            try:
                docstring_collection.upsert(
                    documents=[docstring],
                    metadatas=[docstring_metadata],
                    ids=[docstring_id]
                )
            except Exception:
                try:
                    existing = docstring_collection.get(ids=[docstring_id])
                    if existing and existing.get('ids'):
                        docstring_collection.update(ids=[docstring_id], metadatas=[docstring_metadata], documents=[docstring])
                    else:
                        docstring_collection.add(ids=[docstring_id], metadatas=[docstring_metadata], documents=[docstring])
                except Exception as inner:
                    raise HTTPException(status_code=500, detail=f"ë…ìŠ¤íŠ¸ë§ upsert ì‹¤íŒ¨: {inner}")
        
        # BM25 ì¸ë±ìŠ¤ ì—…ë°ì´íŠ¸
        update_bm25_index()
        
        return {
            "status": "success",
            "message": f"ë¬¸ì„œ '{doc_id}' ì €ì¥ ì™„ë£Œ",
            "document_id": doc_id,
            "total_documents": collection.count()
        }
        
    except Exception as e:
        print(f"âŒ ë¬¸ì„œ ì¶”ê°€ ì˜¤ë¥˜: {str(e)}")
        raise HTTPException(status_code=500, detail=f"ë¬¸ì„œ ì¶”ê°€ ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {str(e)}")

@app.post("/api/v1/documents/upload")
async def upload_documents(request: DocumentRequest):
    """ë¬¸ì„œ ì—…ë¡œë“œ (í”„ë¡ íŠ¸ì—”ë“œ í˜¸í™˜ìš©)"""
    try:
        result = await add_document(request)
        # í”„ë¡ íŠ¸ì—”ë“œ í˜¸í™˜ ì‘ë‹µ í˜•ì‹ìœ¼ë¡œ ë³€í™˜
        return {
            "status": "success",
            "message": result.get("message", "ë¬¸ì„œê°€ ì„±ê³µì ìœ¼ë¡œ ì—…ë¡œë“œë˜ì—ˆìŠµë‹ˆë‹¤"),
            "document_id": result.get("document_id"),
            "total_documents": result.get("total_documents", 0)
        }
    except Exception as e:
        print(f"âŒ ë¬¸ì„œ ì—…ë¡œë“œ ì˜¤ë¥˜: {str(e)}")
        raise HTTPException(status_code=500, detail=f"ë¬¸ì„œ ì—…ë¡œë“œ ì¤‘ ì˜¤ë¥˜: {str(e)}")

@app.delete("/api/v1/documents/{doc_id}")
async def delete_document(doc_id: str):
    """ë¬¸ì„œ ì‚­ì œ"""
    try:
        # ë¬¸ì„œ ì¡´ì¬ í™•ì¸
        existing = collection.get(ids=[doc_id])
        if not existing['ids']:
            raise HTTPException(status_code=404, detail=f"ë¬¸ì„œ '{doc_id}'ë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤")
        
        # ë¬¸ì„œ ì‚­ì œ
        collection.delete(ids=[doc_id])
        
        return {
            "status": "success",
            "message": f"ë¬¸ì„œ '{doc_id}' ì‚­ì œ ì™„ë£Œ",
            "total_documents": collection.count()
        }
            
    except HTTPException:
        raise
    except Exception as e:
        raise HTTPException(status_code=500, detail=f"ë¬¸ì„œ ì‚­ì œ ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {str(e)}")

@app.delete("/api/v1/documents")
async def clear_all_documents():
    """ëª¨ë“  ë¬¸ì„œ ì‚­ì œ"""
    try:
        # ì‚­ì œ ì „ ë¬¸ì„œ ìˆ˜ í™•ì¸
        initial_count = collection.count()
        print(f"ğŸ—‘ï¸ ì‚­ì œ ì „ ë¬¸ì„œ ìˆ˜: {initial_count}")
        
        # ëª¨ë“  ë¬¸ì„œ ID ê°€ì ¸ì˜¤ê¸°
        all_docs = collection.get()
        deleted_count = 0
        if all_docs['ids']:
            print(f"ğŸ—‘ï¸ ì‚­ì œí•  ë¬¸ì„œ ID ìˆ˜: {len(all_docs['ids'])}")
            collection.delete(ids=all_docs['ids'])
            deleted_count = len(all_docs['ids'])
        
        # ë…ìŠ¤íŠ¸ë§ ì»¬ë ‰ì…˜ë„ ëª¨ë‘ ì‚­ì œ
        all_docstrings = docstring_collection.get()
        docstring_count = 0
        if all_docstrings['ids']:
            print(f"ğŸ—‘ï¸ ì‚­ì œí•  ë…ìŠ¤íŠ¸ë§ ID ìˆ˜: {len(all_docstrings['ids'])}")
            docstring_collection.delete(ids=all_docstrings['ids'])
            docstring_count = len(all_docstrings['ids'])
        
        # ë…ìŠ¤íŠ¸ë§ ì»¬ë ‰ì…˜ë„ ì¬ìƒì„± (ì™„ì „ ì´ˆê¸°í™”)
        try:
            client.delete_collection("codemuse_docstrings")
            docstring_collection = client.create_collection(
                name="codemuse_docstrings",
                metadata={"description": "CodeMuse ë…ìŠ¤íŠ¸ë§ ì €ì¥ì†Œ (OpenAI ì„ë² ë”© 1536ì°¨ì›)"},
                embedding_function=embedding_function
            )
            print("âœ… ë…ìŠ¤íŠ¸ë§ ì»¬ë ‰ì…˜ ì¬ìƒì„± ì™„ë£Œ")
            # ì „ì—­ ë³€ìˆ˜ ì—…ë°ì´íŠ¸
            globals()['docstring_collection'] = docstring_collection
        except Exception as docstring_error:
            print(f"âš ï¸ ë…ìŠ¤íŠ¸ë§ ì»¬ë ‰ì…˜ ì¬ìƒì„± ì‹¤íŒ¨: {docstring_error}")
        
        # BM25 ì¸ë±ìŠ¤ ì™„ì „ ì´ˆê¸°í™”
        global bm25_index, document_texts, document_metadata
        bm25_index = None
        document_texts = []
        document_metadata = []
        print("âœ… BM25 ì¸ë±ìŠ¤ ì™„ì „ ì´ˆê¸°í™” ì™„ë£Œ")
        
        # ì‚­ì œ í›„ ì‹¤ì œ ë¬¸ì„œ ìˆ˜ í™•ì¸
        final_count = collection.count()
        print(f"ğŸ—‘ï¸ ì‚­ì œ í›„ ë¬¸ì„œ ìˆ˜: {final_count}")
        
        if final_count > 0:
            print(f"âš ï¸ ì¼ë¶€ ë¬¸ì„œê°€ ë‚¨ì•„ìˆìŒ: {final_count}ê°œ")
            # ë‚¨ì€ ë¬¸ì„œê°€ ìˆë‹¤ë©´ ê°•ì œë¡œ ë‹¤ì‹œ ì‚­ì œ ì‹œë„
            remaining_docs = collection.get()
            if remaining_docs['ids']:
                print(f"ğŸ”„ ë‚¨ì€ ë¬¸ì„œ ì¬ì‚­ì œ ì‹œë„: {len(remaining_docs['ids'])}ê°œ")
                collection.delete(ids=remaining_docs['ids'])
                final_count = collection.count()
                print(f"ğŸ—‘ï¸ ì¬ì‚­ì œ í›„ ë¬¸ì„œ ìˆ˜: {final_count}")
                
                # ì—¬ì „íˆ ë¬¸ì„œê°€ ë‚¨ì•„ìˆë‹¤ë©´ ì»¬ë ‰ì…˜ ì¬ìƒì„±
                if final_count > 0:
                    print(f"ğŸ”„ ì»¬ë ‰ì…˜ ì¬ìƒì„±ìœ¼ë¡œ ê°•ì œ ì´ˆê¸°í™”")
                    try:
                        # ê¸°ì¡´ ì»¬ë ‰ì…˜ ì‚­ì œ
                        client.delete_collection("codemuse_documents")
                        # ìƒˆ ì»¬ë ‰ì…˜ ìƒì„±
                        collection = client.create_collection(
                            name="codemuse_documents",
                            metadata={"description": "CodeMuse ë¬¸ì„œ ì €ì¥ì†Œ (OpenAI ì„ë² ë”© 1536ì°¨ì›)"},
                            embedding_function=embedding_function
                        )
                        print("âœ… ì»¬ë ‰ì…˜ ì¬ìƒì„± ì™„ë£Œ")
                        final_count = 0
                    except Exception as recreate_error:
                        print(f"âŒ ì»¬ë ‰ì…˜ ì¬ìƒì„± ì‹¤íŒ¨: {recreate_error}")
                        # ì „ì—­ ë³€ìˆ˜ ì—…ë°ì´íŠ¸
                        globals()['collection'] = collection
        
        return {
            "status": "success",
            "message": f"ëª¨ë“  ë¬¸ì„œ ì‚­ì œ ì™„ë£Œ (ì‚­ì œëœ ë¬¸ì„œ: {deleted_count}ê°œ, ë…ìŠ¤íŠ¸ë§: {docstring_count}ê°œ)",
            "total_documents": final_count,
            "deleted_documents": deleted_count,
            "deleted_docstrings": docstring_count
        }
    except Exception as e:
        print(f"âŒ ë¬¸ì„œ ì „ì²´ ì‚­ì œ ì˜¤ë¥˜: {str(e)}")
        raise HTTPException(status_code=500, detail=f"ë¬¸ì„œ ì „ì²´ ì‚­ì œ ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {str(e)}")

@app.delete("/api/v1/documents/force-reset")
async def force_reset_collections():
    """ê°•ì œë¡œ ëª¨ë“  ì»¬ë ‰ì…˜ ì´ˆê¸°í™” (ì™„ì „ ì‚­ì œ)"""
    try:
        print("ğŸ”„ ê°•ì œ ì»¬ë ‰ì…˜ ì´ˆê¸°í™” ì‹œì‘")
        
        # ëª¨ë“  ì»¬ë ‰ì…˜ ì‚­ì œ
        try:
            client.delete_collection("codemuse_documents")
            print("âœ… ë¬¸ì„œ ì»¬ë ‰ì…˜ ì‚­ì œë¨")
        except Exception as e:
            print(f"âš ï¸ ë¬¸ì„œ ì»¬ë ‰ì…˜ ì‚­ì œ ì‹¤íŒ¨: {e}")
        
        try:
            client.delete_collection("codemuse_docstrings")
            print("âœ… ë…ìŠ¤íŠ¸ë§ ì»¬ë ‰ì…˜ ì‚­ì œë¨")
        except Exception as e:
            print(f"âš ï¸ ë…ìŠ¤íŠ¸ë§ ì»¬ë ‰ì…˜ ì‚­ì œ ì‹¤íŒ¨: {e}")
        
        # ìƒˆ ì»¬ë ‰ì…˜ ìƒì„±
        global collection, docstring_collection, bm25_index, document_texts, document_metadata
        
        collection = client.create_collection(
            name="codemuse_documents",
            metadata={"description": "CodeMuse ë¬¸ì„œ ì €ì¥ì†Œ (OpenAI ì„ë² ë”© 1536ì°¨ì›)"},
            embedding_function=embedding_function
        )
        print("âœ… ìƒˆ ë¬¸ì„œ ì»¬ë ‰ì…˜ ìƒì„±ë¨")
        
        docstring_collection = client.create_collection(
            name="codemuse_docstrings",
            metadata={"description": "CodeMuse ë…ìŠ¤íŠ¸ë§ ì €ì¥ì†Œ (OpenAI ì„ë² ë”© 1536ì°¨ì›)"},
            embedding_function=embedding_function
        )
        print("âœ… ìƒˆ ë…ìŠ¤íŠ¸ë§ ì»¬ë ‰ì…˜ ìƒì„±ë¨")
        
        # BM25 ì¸ë±ìŠ¤ ì´ˆê¸°í™”
        bm25_index = None
        document_texts = []
        document_metadata = []
        print("âœ… BM25 ì¸ë±ìŠ¤ ì´ˆê¸°í™”ë¨")
        
        return {
            "status": "success",
            "message": "ëª¨ë“  ì»¬ë ‰ì…˜ì´ ê°•ì œ ì´ˆê¸°í™”ë˜ì—ˆìŠµë‹ˆë‹¤",
            "total_documents": 0
        }
        
    except Exception as e:
        print(f"âŒ ê°•ì œ ì´ˆê¸°í™” ì˜¤ë¥˜: {str(e)}")
        raise HTTPException(status_code=500, detail=f"ê°•ì œ ì´ˆê¸°í™” ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {str(e)}")

@app.post("/api/v1/search/docstrings")
async def search_docstrings(request: SearchRequest):
    """ë…ìŠ¤íŠ¸ë§ ì „ìš© ê²€ìƒ‰ (1ì°¨ ê²€ìƒ‰)"""
    try:
        # ë…ìŠ¤íŠ¸ë§ ì»¬ë ‰ì…˜ì—ì„œ ê²€ìƒ‰
        results = docstring_collection.query(
            query_texts=[request.query],
            n_results=min(request.limit, 20),
            include=['documents', 'metadatas', 'distances']
        )
        
        # ê²°ê³¼ í¬ë§¤íŒ…
        formatted_results = []
        if results['documents'] and results['documents'][0]:
            for i, doc in enumerate(results['documents'][0]):
                metadata = results['metadatas'][0][i]
                distance = results['distances'][0][i]
                
                formatted_results.append({
                    "id": results['ids'][0][i],
                    "content": doc,
                    "metadata": metadata,
                    "score": 1.0 / (1.0 + distance),  # ê±°ë¦¬ë¥¼ ì ìˆ˜ë¡œ ë³€í™˜
                    "parent_id": metadata.get("parent_id")
                })
        
        return {
            "query": request.query,
            "results": formatted_results,
            "total_results": len(formatted_results),
            "search_type": "docstring_only"
        }
        
    except Exception as e:
        print(f"âŒ ë…ìŠ¤íŠ¸ë§ ê²€ìƒ‰ ì˜¤ë¥˜: {str(e)}")
        raise HTTPException(status_code=500, detail=f"ë…ìŠ¤íŠ¸ë§ ê²€ìƒ‰ ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {str(e)}")

@app.post("/api/v1/search/two-stage")
async def two_stage_search(request: SearchRequest):
    """2ë‹¨ê³„ ê²€ìƒ‰: 1ì°¨ ë…ìŠ¤íŠ¸ë§ â†’ 2ì°¨ ìƒì„¸ ë¬¸ì„œ"""
    try:
        # 1ë‹¨ê³„: ë…ìŠ¤íŠ¸ë§ ê²€ìƒ‰ìœ¼ë¡œ í›„ë³´ ì¢íˆê¸°
        docstring_results = docstring_collection.query(
            query_texts=[request.query],
            n_results=min(request.limit * 2, 20),  # ë” ë§ì€ í›„ë³´ í™•ë³´
            include=['documents', 'metadatas', 'distances']
        )
        
        # í›„ë³´ ë¬¸ì„œ ID ì¶”ì¶œ
        candidate_ids = []
        if docstring_results['metadatas'] and docstring_results['metadatas'][0]:
            for metadata in docstring_results['metadatas'][0]:
                parent_id = metadata.get("parent_id")
                if parent_id:
                    candidate_ids.append(parent_id)
        
        # 2ë‹¨ê³„: í›„ë³´ ë¬¸ì„œì—ì„œ ìƒì„¸ ê²€ìƒ‰ (ìµœëŒ€ limitê°œ)
        if candidate_ids:
            # í›„ë³´ ë¬¸ì„œë“¤ ì§ì ‘ ì¡°íšŒ
            detailed_docs = collection.get(ids=candidate_ids[:request.limit])
            formatted_results = []
            
            if detailed_docs['documents']:
                for i, doc in enumerate(detailed_docs['documents']):
                    metadata = detailed_docs['metadatas'][i]
                    doc_id = detailed_docs['ids'][i]
                    
                    # ê¸°ë³¸ ì ìˆ˜ (ë…ìŠ¤íŠ¸ë§ ê²€ìƒ‰ ê¸°ë°˜)
                    score = 0.8  # ë…ìŠ¤íŠ¸ë§ ë§¤ì¹­ì„ í†µí•´ ì°¾ì€ ë¬¸ì„œëŠ” ë†’ì€ ê¸°ë³¸ ì ìˆ˜
                    
                    formatted_results.append({
                        "id": doc_id,
                        "content": doc,
                        "metadata": metadata,
                        "score": score,
                        "enhanced_score": _calculate_enhanced_score(score, metadata)
                    })
        else:
            # í›„ë³´ê°€ ì—†ìœ¼ë©´ ì¼ë°˜ ê²€ìƒ‰
            detailed_results = collection.query(
                query_texts=[request.query],
                n_results=request.limit,
                include=['documents', 'metadatas', 'distances']
            )
            
            formatted_results = []
            if detailed_results['documents'] and detailed_results['documents'][0]:
                for i, doc in enumerate(detailed_results['documents'][0]):
                    metadata = detailed_results['metadatas'][0][i]
                    distance = detailed_results['distances'][0][i]
                    
                    formatted_results.append({
                        "id": detailed_results['ids'][0][i],
                        "content": doc,
                        "metadata": metadata,
                        "score": 1.0 / (1.0 + distance),
                        "enhanced_score": _calculate_enhanced_score(1.0 / (1.0 + distance), metadata)
                    })
        
        # ì ìˆ˜ìˆœ ì •ë ¬
        formatted_results.sort(key=lambda x: x['enhanced_score'], reverse=True)
        
        return {
            "query": request.query,
            "results": formatted_results,
            "total_results": len(formatted_results),
            "search_type": "two_stage",
            "candidate_count": len(candidate_ids)
        }
        
    except Exception as e:
        print(f"âŒ 2ë‹¨ê³„ ê²€ìƒ‰ ì˜¤ë¥˜: {str(e)}")
        raise HTTPException(status_code=500, detail=f"2ë‹¨ê³„ ê²€ìƒ‰ ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {str(e)}")

if __name__ == "__main__":
    import uvicorn
    print("ğŸš€ ChromaDB ê¸°ë°˜ RAG Engine ì‹œì‘ ì¤‘...")
    print(f"ğŸ“ ì €ì¥ì†Œ ìœ„ì¹˜: {CHROMA_DIR}")
    uvicorn.run(app, host="0.0.0.0", port=8003)