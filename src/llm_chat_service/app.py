"""
LLM Chat ì„œë¹„ìŠ¤ ì‹¤í–‰ ìŠ¤í¬ë¦½íŠ¸ (ë¬¸ì„œ ìŠ¤í™ì— ë§ê²Œ ìˆ˜ì •)
"""

import sys
import os
# ë£¨íŠ¸ src ê²½ë¡œë¥¼ pathì— ì¶”ê°€í•˜ì—¬ íƒ€ ëª¨ë“ˆ ì ˆëŒ€ import ì§€ì›
_BASE_DIR = os.path.dirname(__file__)
_SRC_DIR = os.path.dirname(_BASE_DIR)
if _SRC_DIR not in sys.path:
    sys.path.append(_SRC_DIR)

from fastapi import FastAPI, HTTPException
from fastapi.responses import StreamingResponse
from fastapi.middleware.cors import CORSMiddleware
from pydantic import BaseModel
from typing import Optional, List, Dict, Any
import openai
import time
import uuid
import asyncio
from datetime import datetime

app = FastAPI(title="LLM Chat Service", version="1.0.0")

# CORS ì„¤ì •
app.add_middleware(
    CORSMiddleware,
    allow_origins=["*"],
    allow_credentials=True,
    allow_methods=["*"],
    allow_headers=["*"],
)

# OpenAI í´ë¼ì´ì–¸íŠ¸ ì„¤ì •
openai.api_key = os.getenv("OPENAI_API_KEY")
print(f"ğŸ”‘ OpenAI API Key ì„¤ì •ë¨: {openai.api_key[:20]}..." if openai.api_key else "âŒ OpenAI API Keyê°€ ì„¤ì •ë˜ì§€ ì•ŠìŒ")

# í™˜ê²½ë³€ìˆ˜ í™•ì¸
print(f"ğŸ” í™˜ê²½ë³€ìˆ˜ OPENAI_API_KEY: {os.getenv('OPENAI_API_KEY', 'NOT_SET')[:20]}...")

# ê°•ì œë¡œ ì˜¬ë°”ë¥¸ API í‚¤ ì„¤ì •
correct_api_key = "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"
openai.api_key = correct_api_key
print(f"ğŸ”§ ê°•ì œ API í‚¤ ì„¤ì •: {openai.api_key[:20]}...")

# API í˜¸ì¶œ í†µê³„
api_call_stats = {
    "total_calls": 0,
    "successful_calls": 0,
    "failed_calls": 0,
    "last_call_time": None,
    "calls_per_minute": 0,
    "last_minute_calls": []
}

# ë©”ëª¨ë¦¬ ë‚´ ì„¸ì…˜ ì €ì¥ì†Œ (ì‹¤ì œë¡œëŠ” ë°ì´í„°ë² ì´ìŠ¤ ì‚¬ìš©)
sessions = {}
messages = {}

async def call_openai_with_retry(messages: List[Dict], model: str, max_retries: int = 3, base_delay: float = 1.0):
    """
    OpenAI API í˜¸ì¶œì„ retry ë¡œì§ê³¼ í•¨ê»˜ ìˆ˜í–‰í•©ë‹ˆë‹¤.
    
    Args:
        messages: ì±„íŒ… ë©”ì‹œì§€ ë¦¬ìŠ¤íŠ¸
        model: ì‚¬ìš©í•  ëª¨ë¸ëª…
        max_retries: ìµœëŒ€ ì¬ì‹œë„ íšŸìˆ˜
        base_delay: ê¸°ë³¸ ëŒ€ê¸° ì‹œê°„ (ì´ˆ)
    
    Returns:
        OpenAI API ì‘ë‹µ
        
    Raises:
        HTTPException: ìµœëŒ€ ì¬ì‹œë„ í›„ì—ë„ ì‹¤íŒ¨í•œ ê²½ìš°
    """
    for attempt in range(max_retries + 1):
        try:
            print(f"ğŸ”„ [{attempt + 1}/{max_retries + 1}] OpenAI API í˜¸ì¶œ ì‹œë„")
            response = openai.chat.completions.create(
                model=model,
                messages=messages
            )
            print(f"âœ… OpenAI API í˜¸ì¶œ ì„±ê³µ (ì‹œë„ {attempt + 1})")
            return response
            
        except Exception as e:
            error_str = str(e)
            print(f"âŒ OpenAI API í˜¸ì¶œ ì‹¤íŒ¨ (ì‹œë„ {attempt + 1}): {error_str}")
            
            # 429 ì˜¤ë¥˜ì¸ ê²½ìš°ì—ë§Œ ì¬ì‹œë„
            if "429" in error_str and attempt < max_retries:
                delay = base_delay * (2 ** attempt)  # exponential backoff
                print(f"â³ {delay}ì´ˆ ëŒ€ê¸° í›„ ì¬ì‹œë„...")
                await asyncio.sleep(delay)
                continue
            else:
                # 429ê°€ ì•„ë‹ˆê±°ë‚˜ ìµœëŒ€ ì¬ì‹œë„ì— ë„ë‹¬í•œ ê²½ìš°
                raise HTTPException(status_code=502, detail=f"LLM í˜¸ì¶œ ì‹¤íŒ¨: {error_str}")
    
    # ì´ ì§€ì ì— ë„ë‹¬í•˜ë©´ ì•ˆ ë˜ì§€ë§Œ, ì•ˆì „ì„ ìœ„í•´
    raise HTTPException(status_code=502, detail="ìµœëŒ€ ì¬ì‹œë„ íšŸìˆ˜ ì´ˆê³¼")

def format_response_with_paragraphs(response: str) -> str:
    """
    AI ì‘ë‹µì— ë¬¸ë‹¨ êµ¬ë¶„ì„ ì¶”ê°€í•˜ì—¬ ê°€ë…ì„±ì„ í–¥ìƒì‹œí‚µë‹ˆë‹¤.
    
    Args:
        response: ì›ë³¸ ì‘ë‹µ í…ìŠ¤íŠ¸
        
    Returns:
        ë¬¸ë‹¨ êµ¬ë¶„ì´ ì¶”ê°€ëœ ì‘ë‹µ í…ìŠ¤íŠ¸
    """
    if not response:
        return response
        
    import re
    
    # ë¬¸ë‹¨ êµ¬ë¶„ íŒ¨í„´ë“¤
    paragraph_patterns = [
        # ì œëª©/í—¤ë” íŒ¨í„´
        (r'(\*\*[^*]+\*\*:)', r'\n\n\1\n'),  # **ì œëª©:** í˜•íƒœ
        (r'(### [^#\n]+)', r'\n\n\1\n'),     # ### ì œëª© í˜•íƒœ
        (r'(## [^#\n]+)', r'\n\n\1\n'),      # ## ì œëª© í˜•íƒœ
        (r'(# [^#\n]+)', r'\n\n\1\n'),       # # ì œëª© í˜•íƒœ
        
        # ë²ˆí˜¸ ëª©ë¡ íŒ¨í„´
        (r'(\n\d+\.\s)', r'\n\n\1'),         # 1. 2. 3. í˜•íƒœ
        (r'(\n-\s)', r'\n\n\1'),             # - ëª©ë¡ í˜•íƒœ
        (r'(\n\*\s)', r'\n\n\1'),            # * ëª©ë¡ í˜•íƒœ
        
        # íŠ¹ë³„í•œ í‚¤ì›Œë“œ íŒ¨í„´
        (r'(\ní•µì‹¬ ìš”ì§€)', r'\n\n\1'),
        (r'(\nê·¼ê±° ì„¹ì…˜)', r'\n\n\1'),
        (r'(\nì¡°ì¹˜/í™•ì¥)', r'\n\n\1'),
        (r'(\nì¶”ê°€ ì„¤ëª…)', r'\n\n\1'),
        (r'(\nê²°ë¡ )', r'\n\n\1'),
        (r'(\nìš”ì•½)', r'\n\n\1'),
        
        # ë¬¸ì¥ ë íŒ¨í„´ (ë§ˆì¹¨í‘œ í›„)
        (r'(\.\s)([A-Zê°€-í£])', r'\1\n\n\2'),  # ë§ˆì¹¨í‘œ í›„ ëŒ€ë¬¸ì/í•œê¸€
    ]
    
    formatted_response = response
    
    # íŒ¨í„´ ì ìš©
    for pattern, replacement in paragraph_patterns:
        formatted_response = re.sub(pattern, replacement, formatted_response)
    
    # ì—°ì†ëœ ì¤„ë°”ê¿ˆ ì •ë¦¬ (3ê°œ ì´ìƒì„ 2ê°œë¡œ)
    formatted_response = re.sub(r'\n{3,}', '\n\n', formatted_response)
    
    # ì‹œì‘ê³¼ ëì˜ ë¶ˆí•„ìš”í•œ ì¤„ë°”ê¿ˆ ì œê±°
    formatted_response = formatted_response.strip()
    
    return formatted_response

class CreateSessionRequest(BaseModel):
    title: str = "ìƒˆ ì±„íŒ…"

class SessionResponse(BaseModel):
    id: str
    title: str
    created_at: str
    messages: List[Dict[str, Any]]

class MessageRequest(BaseModel):
    message: str
    model: str = "gpt-3.5-turbo"
    context: Optional[Dict[str, Any]] = None

class MessageResponse(BaseModel):
    message: str
    session_id: str
    model: str
    usage: Dict[str, int]
    response_time: float

@app.get("/")
async def root():
    return {"service": "LLM Chat Service", "status": "running", "port": 8004}

@app.get("/health")
async def health():
    return {"status": "healthy", "service": "LLM Chat Service"}

@app.get("/api/v1/stats")
async def get_api_stats():
    """API í˜¸ì¶œ í†µê³„ í™•ì¸"""
    return {
        "api_call_stats": api_call_stats,
        "timestamp": datetime.now().isoformat()
    }

@app.get("/api/v1/debug")
async def debug_info():
    """ë””ë²„ê·¸ ì •ë³´ í™•ì¸"""
    return {
        "openai_api_key": openai.api_key[:20] + "..." if openai.api_key else "NOT_SET",
        "env_openai_key": os.getenv("OPENAI_API_KEY", "NOT_SET")[:20] + "..." if os.getenv("OPENAI_API_KEY") else "NOT_SET",
        "key_length": len(openai.api_key) if openai.api_key else 0,
        "env_key_length": len(os.getenv("OPENAI_API_KEY", "")),
        "keys_match": openai.api_key == os.getenv("OPENAI_API_KEY"),
        "timestamp": datetime.now().isoformat()
    }

@app.post("/api/v1/chat/sessions", response_model=SessionResponse)
async def create_session(request: CreateSessionRequest):
    """ì±„íŒ… ì„¸ì…˜ ìƒì„±"""
    try:
        session_id = str(uuid.uuid4())
        now = datetime.now().isoformat()
        
        session = {
            "id": session_id,
            "title": request.title,
            "created_at": now,
            "messages": []
        }
        
        sessions[session_id] = session
        messages[session_id] = []
        
        return SessionResponse(**session)
        
    except Exception as e:
        raise HTTPException(status_code=500, detail=f"ì„¸ì…˜ ìƒì„± ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {str(e)}")

@app.get("/api/v1/chat/sessions", response_model=List[SessionResponse])
async def list_sessions():
    """ì„¸ì…˜ ëª©ë¡ ì¡°íšŒ"""
    try:
        return [SessionResponse(**session) for session in sessions.values()]
    except Exception as e:
        raise HTTPException(status_code=500, detail=f"ì„¸ì…˜ ëª©ë¡ ì¡°íšŒ ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {str(e)}")

@app.get("/api/v1/chat/sessions/{session_id}/messages", response_model=List[Dict[str, Any]])
async def get_messages(session_id: str):
    """ì„¸ì…˜ì˜ ë©”ì‹œì§€ ëª©ë¡ ì¡°íšŒ"""
    try:
        if session_id not in messages:
            raise HTTPException(status_code=404, detail="ì„¸ì…˜ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤")
        
        return messages[session_id]
    except HTTPException:
        raise
    except Exception as e:
        raise HTTPException(status_code=500, detail=f"ë©”ì‹œì§€ ì¡°íšŒ ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {str(e)}")

@app.post("/api/v1/chat/sessions/{session_id}/messages", response_model=MessageResponse)
async def send_message(session_id: str, request: MessageRequest):
    """ë©”ì‹œì§€ ì „ì†¡"""
    try:
        if session_id not in sessions:
            raise HTTPException(status_code=404, detail="ì„¸ì…˜ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤")
        
        start_time = time.time()
        
        # ì»¨í…ìŠ¤íŠ¸ê°€ ìˆìœ¼ë©´ í”„ë¡¬í”„íŠ¸ì— ì¶”ê°€
        system_message = """
ë‹¹ì‹ ì€ "CodeMuse" â€“ ë ˆê±°ì‹œ ì½”ë“œë² ì´ìŠ¤ ë¶„ì„ ì–´ì‹œìŠ¤í„´íŠ¸ì…ë‹ˆë‹¤.

ëª©í‘œ: ì œê³µëœ ì»¨í…ìŠ¤íŠ¸ë¥¼ ê·¼ê±°ë¡œ, ì§ˆë¬¸ ì˜ë„ì— ë§ê²Œ ì‹¤ìš©ì ì´ê³  ë§¥ë½ ìˆëŠ” ë‹µë³€ì„ ìƒì„±í•©ë‹ˆë‹¤.

ğŸ“Œ ë‹µë³€ ì§€ì¹¨:
1. **ğŸš¨ ìˆ«ì ë³´ì¡´ í•„ìˆ˜**: ëª¨ë“  ìˆ«ì(7ê°€ì§€, 80ë²ˆ ì¤„, 157ë²ˆ ì¤„ ë“±)ë¥¼ ì ˆëŒ€ ìƒëµí•˜ì§€ ë§ˆì„¸ìš”.
2. **ğŸš¨ êµ¬ì²´ì  í‘œí˜„**: "ëª‡ë²ˆì§¸ì¤„" ëŒ€ì‹  "80ë²ˆ ì¤„", "157ë²ˆ ì¤„" ë“± ì •í™•í•œ ìˆ«ì ì‚¬ìš©
3. **ğŸš¨ ëª©ë¡ ë²ˆí˜¸**: "7ê°€ì§€", "5ê°œ", "3ë‹¨ê³„" ë“± ëª¨ë“  ìˆ«ìë¥¼ ë°˜ë“œì‹œ í¬í•¨
4. **ğŸš¨ ê²½ê³ **: ìˆ«ìë¥¼ ìƒëµí•˜ë©´ ì•ˆ ë©ë‹ˆë‹¤! ë°˜ë“œì‹œ ëª¨ë“  ìˆ«ìë¥¼ í¬í•¨í•˜ì„¸ìš”!
5. ë°˜ë“œì‹œ ì œê³µëœ ì •ë³´ë¥¼ ìš°ì„  í™œìš©í•©ë‹ˆë‹¤. (ìˆ«ì/ëª©ë¡/í•¨ìˆ˜ëª…ì€ ì ˆëŒ€ ìƒëµí•˜ì§€ ì•ŠìŠµë‹ˆë‹¤)  
6. **ì†ŒìŠ¤ì½”ë“œ ì •ë³´ê°€ í¬í•¨ëœ ê²½ìš°**, ë°˜ë“œì‹œ ë‹¤ìŒì„ í¬í•¨í•©ë‹ˆë‹¤:
   - ğŸ“ **íŒŒì¼ ê²½ë¡œ**: ì •í™•í•œ íŒŒì¼ëª…ê³¼ ê²½ë¡œ
   - ğŸ“ **ì¤„ ë²ˆí˜¸**: í•´ë‹¹ ë¡œì§ì´ ìœ„ì¹˜í•œ ì •í™•í•œ ì¤„ ë²ˆí˜¸ (ì˜ˆ: 80ë²ˆ ì¤„, 157ë²ˆ ì¤„)
   - ğŸ”§ **êµ¬ì²´ì  ìœ„ì¹˜**: í•¨ìˆ˜ëª…, í´ë˜ìŠ¤ëª…, ë©”ì„œë“œëª…
7. **ì¤‘ìš”**: sample_code ë””ë ‰í† ë¦¬ì˜ íŒŒì¼ì´ í¬í•¨ëœ ê²½ìš°, ë°˜ë“œì‹œ í•´ë‹¹ íŒŒì¼ì˜ ì •ë³´ë¥¼ ìš°ì„ ì ìœ¼ë¡œ ì‚¬ìš©í•˜ì„¸ìš”.
8. **ì¤„ ë²ˆí˜¸ í•„ìˆ˜**: ğŸ“ **ì¤„ ë²ˆí˜¸** ì •ë³´ê°€ ì œê³µëœ ê²½ìš°, ë°˜ë“œì‹œ "80ë²ˆ ì¤„", "157ë²ˆ ì¤„" ë“±ìœ¼ë¡œ ëª…ì‹œí•˜ì„¸ìš”.
9. **ê²½ê³ **: ì¤„ ë²ˆí˜¸ë¥¼ ìƒëµí•˜ë©´ ì•ˆ ë©ë‹ˆë‹¤. ë°˜ë“œì‹œ êµ¬ì²´ì ì¸ ì¤„ ë²ˆí˜¸ë¥¼ í¬í•¨í•´ì•¼ í•©ë‹ˆë‹¤.
10. **ë§í¬ í˜•ì‹**: í´ë˜ìŠ¤ëª…ì´ë‚˜ íŒŒì¼ëª…ì„ ì–¸ê¸‰í•  ë•ŒëŠ” ë‹¤ìŒ í˜•ì‹ì„ ì‚¬ìš©í•˜ì„¸ìš”:
   - í´ë˜ìŠ¤ëª…: [IssueDetector](class_a1b2c3_IssueDetector)
   - íŒŒì¼ëª…: [issue_detector.py](file_a1b2c3)
   - í•¨ìˆ˜ëª…: [detect_issues()](func_a1b2c3_detect_issues_80)
11. ë‹µë³€ì€ **3ë‹¨ êµ¬ì„±**ìœ¼ë¡œ ì‘ì„±í•©ë‹ˆë‹¤:  
    - **í•µì‹¬ ìš”ì§€**: (2~4ì¤„ ìš”ì•½)  
    - **ê·¼ê±° ì„¹ì…˜ ìš”ì•½**: (ê´€ë ¨ í•¨ìˆ˜/í´ë˜ìŠ¤/ëª¨ë“ˆ, í•„ìš” ì‹œ ì½”ë“œ ì‹œê·¸ë‹ˆì²˜ í¬í•¨)  
    - **ì¡°ì¹˜/í™•ì¥ ë°©ë²•**: (íŒŒì¼ â†’ í´ë˜ìŠ¤ â†’ í•¨ìˆ˜ íë¦„ ë‹¨ìœ„ë¡œ ë‹¨ê³„ì  ì„¤ëª…)  
12. **ë¬¸ë‹¨ êµ¬ë¶„**: ê° ì„¹ì…˜ì€ ë°˜ë“œì‹œ ë¹ˆ ì¤„ë¡œ êµ¬ë¶„í•˜ê³ , ì„¹ì…˜ ì œëª©ì€ **êµµì€ ê¸€ì”¨**ë¡œ í‘œì‹œí•˜ì„¸ìš”.
13. ì •ë³´ê°€ ë¶€ì¡±í•œ ê²½ìš°, "ê·¼ê±° ë¶€ì¡±"ì„ ëª…ì‹œí•˜ê³  í•„ìš”í•œ ì¶”ê°€ ì •ë³´ë¥¼ ìš”ì²­í•©ë‹ˆë‹¤.  
14. ì¶”ì¸¡ì€ í•˜ì§€ ì•Šê³ , ë¶ˆí™•ì‹¤í•˜ë©´ ëª…í™•íˆ í‘œì‹œí•©ë‹ˆë‹¤.  
15. ë‹µë³€ì€ ì¹œê·¼í•˜ë©´ì„œë„ ì „ë¬¸ì ìœ¼ë¡œ, ë ˆê±°ì‹œ ì½”ë“œ ë¶„ì„/ë¦¬íŒ©í† ë§ ë§¥ë½ì„ ìœ ì§€í•©ë‹ˆë‹¤.

---

ğŸ“Œ ìµœì¢… ë‹µë³€:"""

        # RAG ì»¨í…ìŠ¤íŠ¸ ì²˜ë¦¬ (ì—¬ëŸ¬ í˜•íƒœ ì§€ì›)
        context_text = ""
        if request.context:
            # 0) ìµœìš°ì„ : final_context ë¬¸ìì—´
            if isinstance(request.context.get("final_context"), str) and request.context.get("final_context").strip():
                context_text = request.context.get("final_context").strip()
            elif "rag_documents" in request.context:
                # ì›Œí¬í”Œë¡œìš°ì—ì„œ ì „ë‹¬í•˜ëŠ” í˜•íƒœ
                rag_docs = request.context["rag_documents"]
                if rag_docs:
                    context_text = "\n\n".join(rag_docs)
            elif "search_results" in request.context:
                # ê¸°ì¡´ í˜•íƒœ
                search_results = request.context["search_results"]
                context_text = "\n".join([doc.get("content", "") for doc in search_results])
        
        # ì¼ë°˜ ì›Œí¬í”Œë¡œìš°ì™€ ë™ì¼í•œ í”„ë¡¬í”„íŠ¸ êµ¬ì¡° ì‚¬ìš©
        if context_text:
            # ì™„ì „í•œ í”„ë¡¬í”„íŠ¸ êµ¬ì¡° (ì¼ë°˜ ì›Œí¬í”Œë¡œìš°ì™€ ë™ì¼)
            prompt = f"""ë‹¹ì‹ ì€ "CodeMuse" â€“ ë ˆê±°ì‹œ ì½”ë“œë² ì´ìŠ¤ ë¶„ì„ ì–´ì‹œìŠ¤í„´íŠ¸ì…ë‹ˆë‹¤.  
ë‹¤ìŒ ì½”ë“œ ì •ë³´ë¥¼ ë°”íƒ•ìœ¼ë¡œ ì‚¬ìš©ì ì§ˆë¬¸ì— ë‹µë³€í•˜ì„¸ìš”.  

ğŸ“Œ ì°¸ê³  ì •ë³´:
{context_text}

ğŸ“Œ ì§ˆë¬¸:
{request.message}

---

ğŸ“Œ ë‹µë³€ ì§€ì¹¨:
1. **ğŸš¨ ìˆ«ì ë³´ì¡´ í•„ìˆ˜**: ëª¨ë“  ìˆ«ì(7ê°€ì§€, 80ë²ˆ ì¤„, 157ë²ˆ ì¤„ ë“±)ë¥¼ ì ˆëŒ€ ìƒëµí•˜ì§€ ë§ˆì„¸ìš”.
2. **ğŸš¨ êµ¬ì²´ì  í‘œí˜„**: "ëª‡ë²ˆì§¸ì¤„" ëŒ€ì‹  "80ë²ˆ ì¤„", "157ë²ˆ ì¤„" ë“± ì •í™•í•œ ìˆ«ì ì‚¬ìš©
3. **ğŸš¨ ëª©ë¡ ë²ˆí˜¸**: "7ê°€ì§€", "5ê°œ", "3ë‹¨ê³„" ë“± ëª¨ë“  ìˆ«ìë¥¼ ë°˜ë“œì‹œ í¬í•¨
4. **ğŸš¨ ê²½ê³ **: ìˆ«ìë¥¼ ìƒëµí•˜ë©´ ì•ˆ ë©ë‹ˆë‹¤! ë°˜ë“œì‹œ ëª¨ë“  ìˆ«ìë¥¼ í¬í•¨í•˜ì„¸ìš”!
5. ë°˜ë“œì‹œ ì œê³µëœ ì •ë³´ë¥¼ ìš°ì„  í™œìš©í•©ë‹ˆë‹¤. (ìˆ«ì/ëª©ë¡/í•¨ìˆ˜ëª…ì€ ì ˆëŒ€ ìƒëµí•˜ì§€ ì•ŠìŠµë‹ˆë‹¤)  
6. **ì†ŒìŠ¤ì½”ë“œ ì •ë³´ê°€ í¬í•¨ëœ ê²½ìš°**, ë°˜ë“œì‹œ ë‹¤ìŒì„ í¬í•¨í•©ë‹ˆë‹¤:
   - ğŸ“ **íŒŒì¼ ê²½ë¡œ**: ì •í™•í•œ íŒŒì¼ëª…ê³¼ ê²½ë¡œ
   - ğŸ“ **ì¤„ ë²ˆí˜¸**: í•´ë‹¹ ë¡œì§ì´ ìœ„ì¹˜í•œ ì •í™•í•œ ì¤„ ë²ˆí˜¸ (ì˜ˆ: 80ë²ˆ ì¤„, 157ë²ˆ ì¤„)
   - ğŸ”§ **êµ¬ì²´ì  ìœ„ì¹˜**: í•¨ìˆ˜ëª…, í´ë˜ìŠ¤ëª…, ë©”ì„œë“œëª…
7. **ì¤‘ìš”**: sample_code ë””ë ‰í† ë¦¬ì˜ íŒŒì¼ì´ í¬í•¨ëœ ê²½ìš°, ë°˜ë“œì‹œ í•´ë‹¹ íŒŒì¼ì˜ ì •ë³´ë¥¼ ìš°ì„ ì ìœ¼ë¡œ ì‚¬ìš©í•˜ì„¸ìš”.
8. **ì¤„ ë²ˆí˜¸ í•„ìˆ˜**: ğŸ“ **ì¤„ ë²ˆí˜¸** ì •ë³´ê°€ ì œê³µëœ ê²½ìš°, ë°˜ë“œì‹œ "80ë²ˆ ì¤„", "157ë²ˆ ì¤„" ë“±ìœ¼ë¡œ ëª…ì‹œí•˜ì„¸ìš”.
9. **ê²½ê³ **: ì¤„ ë²ˆí˜¸ë¥¼ ìƒëµí•˜ë©´ ì•ˆ ë©ë‹ˆë‹¤. ë°˜ë“œì‹œ êµ¬ì²´ì ì¸ ì¤„ ë²ˆí˜¸ë¥¼ í¬í•¨í•´ì•¼ í•©ë‹ˆë‹¤.
10. **ë§í¬ í˜•ì‹**: í´ë˜ìŠ¤ëª…ì´ë‚˜ íŒŒì¼ëª…ì„ ì–¸ê¸‰í•  ë•ŒëŠ” ë‹¤ìŒ í˜•ì‹ì„ ì‚¬ìš©í•˜ì„¸ìš”:
   - í´ë˜ìŠ¤ëª…: [IssueDetector](class_a1b2c3_IssueDetector)
   - íŒŒì¼ëª…: [issue_detector.py](file_a1b2c3)
   - í•¨ìˆ˜ëª…: [detect_issues()](func_a1b2c3_detect_issues_80)
11. ë‹µë³€ì€ **3ë‹¨ êµ¬ì„±**ìœ¼ë¡œ ì‘ì„±í•©ë‹ˆë‹¤:  
    - **í•µì‹¬ ìš”ì§€**: (2~4ì¤„ ìš”ì•½)  
    - **ê·¼ê±° ì„¹ì…˜ ìš”ì•½**: (ê´€ë ¨ í•¨ìˆ˜/í´ë˜ìŠ¤/ëª¨ë“ˆ, í•„ìš” ì‹œ ì½”ë“œ ì‹œê·¸ë‹ˆì²˜ í¬í•¨)  
    - **ì¡°ì¹˜/í™•ì¥ ë°©ë²•**: (íŒŒì¼ â†’ í´ë˜ìŠ¤ â†’ í•¨ìˆ˜ íë¦„ ë‹¨ìœ„ë¡œ ë‹¨ê³„ì  ì„¤ëª…)  
12. **ë¬¸ë‹¨ êµ¬ë¶„**: ê° ì„¹ì…˜ì€ ë°˜ë“œì‹œ ë¹ˆ ì¤„ë¡œ êµ¬ë¶„í•˜ê³ , ì„¹ì…˜ ì œëª©ì€ **êµµì€ ê¸€ì”¨**ë¡œ í‘œì‹œí•˜ì„¸ìš”.
13. ì •ë³´ê°€ ë¶€ì¡±í•œ ê²½ìš°, "ê·¼ê±° ë¶€ì¡±"ì„ ëª…ì‹œí•˜ê³  í•„ìš”í•œ ì¶”ê°€ ì •ë³´ë¥¼ ìš”ì²­í•©ë‹ˆë‹¤.  
14. ì¶”ì¸¡ì€ í•˜ì§€ ì•Šê³ , ë¶ˆí™•ì‹¤í•˜ë©´ ëª…í™•íˆ í‘œì‹œí•©ë‹ˆë‹¤.  
15. ë‹µë³€ì€ ì¹œê·¼í•˜ë©´ì„œë„ ì „ë¬¸ì ìœ¼ë¡œ, ë ˆê±°ì‹œ ì½”ë“œ ë¶„ì„/ë¦¬íŒ©í† ë§ ë§¥ë½ì„ ìœ ì§€í•©ë‹ˆë‹¤.

---

ğŸ“Œ ìµœì¢… ë‹µë³€:"""
            
            # ì‹œìŠ¤í…œ ë©”ì‹œì§€ëŠ” ê°„ë‹¨í•˜ê²Œ ì„¤ì •
            enhanced_system_message = "ë‹¹ì‹ ì€ CodeMuse - ë ˆê±°ì‹œ ì½”ë“œë² ì´ìŠ¤ ë¶„ì„ ì–´ì‹œìŠ¤í„´íŠ¸ì…ë‹ˆë‹¤."
            user_message = prompt
        else:
            # ì»¨í…ìŠ¤íŠ¸ê°€ ì—†ëŠ” ê²½ìš° ê¸°ì¡´ ë°©ì‹ ì‚¬ìš©
            enhanced_system_message = system_message
            user_message = request.message

        # ëŒ€í™” íˆìŠ¤í† ë¦¬ êµ¬ì„±
        chat_messages = [{"role": "system", "content": enhanced_system_message}]
        
        # ê¸°ì¡´ ë©”ì‹œì§€ë“¤ ì¶”ê°€ (ì»¨í…ìŠ¤íŠ¸ê°€ ìˆëŠ” ê²½ìš° íˆìŠ¤í† ë¦¬ ì œí•œ)
        if context_text:
            # ì»¨í…ìŠ¤íŠ¸ê°€ ìˆìœ¼ë©´ íˆìŠ¤í† ë¦¬ë¥¼ ìµœì†Œí™”í•˜ì—¬ í”„ë¡¬í”„íŠ¸ì— ì§‘ì¤‘
            for msg in messages[session_id][-3:]:  # ìµœê·¼ 3ê°œ ë©”ì‹œì§€ë§Œ
                chat_messages.append({"role": msg["role"], "content": msg["content"]})
        else:
            # ì»¨í…ìŠ¤íŠ¸ê°€ ì—†ìœ¼ë©´ ê¸°ì¡´ ë°©ì‹ ìœ ì§€
            for msg in messages[session_id][-10:]:  # ìµœê·¼ 10ê°œ ë©”ì‹œì§€
                chat_messages.append({"role": msg["role"], "content": msg["content"]})
        
        # ìƒˆ ì‚¬ìš©ì ë©”ì‹œì§€ ì¶”ê°€
        chat_messages.append({"role": "user", "content": user_message})
        
        # API í˜¸ì¶œ í†µê³„ ì—…ë°ì´íŠ¸
        current_time = time.time()
        request_id = str(uuid.uuid4())[:8]
        
        # ë¶„ë‹¹ í˜¸ì¶œ ìˆ˜ ê³„ì‚°
        api_call_stats["last_minute_calls"] = [
            call_time for call_time in api_call_stats["last_minute_calls"] 
            if current_time - call_time < 60
        ]
        api_call_stats["last_minute_calls"].append(current_time)
        api_call_stats["calls_per_minute"] = len(api_call_stats["last_minute_calls"])
        api_call_stats["total_calls"] += 1
        api_call_stats["last_call_time"] = current_time
        
        # OpenAI API í˜¸ì¶œ
        print(f"ğŸ” [{request_id}] OpenAI API í˜¸ì¶œ ì‹œì‘: model={request.model}, messages={len(chat_messages)}ê°œ")
        print(f"ğŸ”‘ [{request_id}] ì‚¬ìš© ì¤‘ì¸ API Key: {openai.api_key[:20]}..." if openai.api_key else f"âŒ [{request_id}] API Key ì—†ìŒ")
        print(f"ğŸ“Š [{request_id}] í†µê³„ - ì´ í˜¸ì¶œ: {api_call_stats['total_calls']}, ë¶„ë‹¹: {api_call_stats['calls_per_minute']}")
        
        # API í˜¸ì¶œ ì „ì— API í‚¤ë¥¼ ë‹¤ì‹œ ì„¤ì •
        correct_api_key = "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"
        openai.api_key = correct_api_key
        print(f"ğŸ”§ [{request_id}] API í˜¸ì¶œ ì „ í‚¤ ì¬ì„¤ì •: {openai.api_key[:20]}...")
        
        try:
            response = await call_openai_with_retry(chat_messages, request.model)
            api_call_stats["successful_calls"] += 1
            print(f"âœ… [{request_id}] OpenAI API í˜¸ì¶œ ì„±ê³µ")
        except HTTPException as e:
            api_call_stats["failed_calls"] += 1
            print(f"âŒ [{request_id}] OpenAI API í˜¸ì¶œ ìµœì¢… ì‹¤íŒ¨: {e.detail}")
            print(f"ğŸ“Š [{request_id}] ì‹¤íŒ¨ í†µê³„ - ì„±ê³µ: {api_call_stats['successful_calls']}, ì‹¤íŒ¨: {api_call_stats['failed_calls']}")
            raise e
        
        response_time = time.time() - start_time
        
        # ì‘ë‹µ ë©”ì‹œì§€ ì €ì¥
        user_msg = {
            "role": "user",
            "content": request.message,
            "timestamp": datetime.now().isoformat()
        }
        ai_msg = {
            "role": "assistant", 
            "content": response.choices[0].message.content,
            "timestamp": datetime.now().isoformat()
        }
        
        messages[session_id].extend([user_msg, ai_msg])
        
        # ì‚¬ìš©ëŸ‰ ì •ë³´ (ì‹¤ì œë¡œëŠ” OpenAI ì‘ë‹µì—ì„œ ê°€ì ¸ì˜´)
        usage = {
            "prompt_tokens": 150,  # ì‹¤ì œë¡œëŠ” response.usage.prompt_tokens
            "completion_tokens": 200,  # ì‹¤ì œë¡œëŠ” response.usage.completion_tokens
            "total_tokens": 350  # ì‹¤ì œë¡œëŠ” response.usage.total_tokens
        }
        
        # ì‘ë‹µ ë‚´ìš© í™•ì¸
        if not response.choices or not response.choices[0].message.content:
            print(f"âŒ OpenAI ì‘ë‹µì´ ë¹„ì–´ìˆìŒ: {response}")
            raise HTTPException(status_code=500, detail="OpenAIì—ì„œ ë¹ˆ ì‘ë‹µì„ ë°›ì•˜ìŠµë‹ˆë‹¤")
        
        response_content = response.choices[0].message.content
        # ì‘ë‹µì— ë¬¸ë‹¨ êµ¬ë¶„ ì¶”ê°€
        response_content = format_response_with_paragraphs(response_content)
        print(f"âœ… ì‘ë‹µ ìƒì„± ì™„ë£Œ: {len(response_content)}ì")
        
        return MessageResponse(
            message=response_content,
            session_id=session_id,
            model=request.model,
            usage=usage,
            response_time=response_time
        )
        
    except HTTPException:
        raise
    except Exception as e:
        raise HTTPException(status_code=500, detail=f"ë©”ì‹œì§€ ì „ì†¡ ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {str(e)}")
    
## í´ë°± í•¨ìˆ˜ ì‚­ì œë¨ (ë„ë©”ì¸ í•˜ë“œì½”ë”© ì œê±°)

# SSE ìŠ¤íŠ¸ë¦¬ë° ì—”ë“œí¬ì¸íŠ¸ (ì‹¤ì‹œê°„ í† í° ìŠ¤íŠ¸ë¦¼)
@app.post("/api/v1/chat/stream")
async def stream_message(request: MessageRequest):
    """ì‹¤ì‹œê°„ SSEë¡œ í† í°ì„ ìŠ¤íŠ¸ë¦¬ë° ë°˜í™˜"""
    try:
        start_time = time.time()

        # ì¼ë°˜ ì›Œí¬í”Œë¡œìš°ì™€ ë™ì¼í•œ í”„ë¡¬í”„íŠ¸ êµ¬ì¡° ì‚¬ìš©
        context_text = ""
        if request.context:
            if isinstance(request.context.get("final_context"), str) and request.context.get("final_context").strip():
                context_text = request.context.get("final_context").strip()

        if context_text:
            # ì™„ì „í•œ í”„ë¡¬í”„íŠ¸ êµ¬ì¡° (ì¼ë°˜ ì›Œí¬í”Œë¡œìš°ì™€ ë™ì¼)
            prompt = f"""ë‹¹ì‹ ì€ "CodeMuse" â€“ ë ˆê±°ì‹œ ì½”ë“œë² ì´ìŠ¤ ë¶„ì„ ì–´ì‹œìŠ¤í„´íŠ¸ì…ë‹ˆë‹¤.  
ë‹¤ìŒ ì½”ë“œ ì •ë³´ë¥¼ ë°”íƒ•ìœ¼ë¡œ ì‚¬ìš©ì ì§ˆë¬¸ì— ë‹µë³€í•˜ì„¸ìš”.  

ğŸ“Œ ì°¸ê³  ì •ë³´:
{context_text}

ğŸ“Œ ì§ˆë¬¸:
{request.message}

---

ğŸ“Œ ë‹µë³€ ì§€ì¹¨:
1. **ğŸš¨ ìˆ«ì ë³´ì¡´ í•„ìˆ˜**: ëª¨ë“  ìˆ«ì(7ê°€ì§€, 80ë²ˆ ì¤„, 157ë²ˆ ì¤„ ë“±)ë¥¼ ì ˆëŒ€ ìƒëµí•˜ì§€ ë§ˆì„¸ìš”.
2. **ğŸš¨ êµ¬ì²´ì  í‘œí˜„**: "ëª‡ë²ˆì§¸ì¤„" ëŒ€ì‹  "80ë²ˆ ì¤„", "157ë²ˆ ì¤„" ë“± ì •í™•í•œ ìˆ«ì ì‚¬ìš©
3. **ğŸš¨ ëª©ë¡ ë²ˆí˜¸**: "7ê°€ì§€", "5ê°œ", "3ë‹¨ê³„" ë“± ëª¨ë“  ìˆ«ìë¥¼ ë°˜ë“œì‹œ í¬í•¨
4. **ğŸš¨ ê²½ê³ **: ìˆ«ìë¥¼ ìƒëµí•˜ë©´ ì•ˆ ë©ë‹ˆë‹¤! ë°˜ë“œì‹œ ëª¨ë“  ìˆ«ìë¥¼ í¬í•¨í•˜ì„¸ìš”!
5. ë°˜ë“œì‹œ ì œê³µëœ ì •ë³´ë¥¼ ìš°ì„  í™œìš©í•©ë‹ˆë‹¤. (ìˆ«ì/ëª©ë¡/í•¨ìˆ˜ëª…ì€ ì ˆëŒ€ ìƒëµí•˜ì§€ ì•ŠìŠµë‹ˆë‹¤)  
6. **ì†ŒìŠ¤ì½”ë“œ ì •ë³´ê°€ í¬í•¨ëœ ê²½ìš°**, ë°˜ë“œì‹œ ë‹¤ìŒì„ í¬í•¨í•©ë‹ˆë‹¤:
   - ğŸ“ **íŒŒì¼ ê²½ë¡œ**: ì •í™•í•œ íŒŒì¼ëª…ê³¼ ê²½ë¡œ
   - ğŸ“ **ì¤„ ë²ˆí˜¸**: í•´ë‹¹ ë¡œì§ì´ ìœ„ì¹˜í•œ ì •í™•í•œ ì¤„ ë²ˆí˜¸ (ì˜ˆ: 80ë²ˆ ì¤„, 157ë²ˆ ì¤„)
   - ğŸ”§ **êµ¬ì²´ì  ìœ„ì¹˜**: í•¨ìˆ˜ëª…, í´ë˜ìŠ¤ëª…, ë©”ì„œë“œëª…
7. **ì¤‘ìš”**: sample_code ë””ë ‰í† ë¦¬ì˜ íŒŒì¼ì´ í¬í•¨ëœ ê²½ìš°, ë°˜ë“œì‹œ í•´ë‹¹ íŒŒì¼ì˜ ì •ë³´ë¥¼ ìš°ì„ ì ìœ¼ë¡œ ì‚¬ìš©í•˜ì„¸ìš”.
8. **ì¤„ ë²ˆí˜¸ í•„ìˆ˜**: ğŸ“ **ì¤„ ë²ˆí˜¸** ì •ë³´ê°€ ì œê³µëœ ê²½ìš°, ë°˜ë“œì‹œ "80ë²ˆ ì¤„", "157ë²ˆ ì¤„" ë“±ìœ¼ë¡œ ëª…ì‹œí•˜ì„¸ìš”.
9. **ê²½ê³ **: ì¤„ ë²ˆí˜¸ë¥¼ ìƒëµí•˜ë©´ ì•ˆ ë©ë‹ˆë‹¤. ë°˜ë“œì‹œ êµ¬ì²´ì ì¸ ì¤„ ë²ˆí˜¸ë¥¼ í¬í•¨í•´ì•¼ í•©ë‹ˆë‹¤.
10. **ë§í¬ í˜•ì‹**: í´ë˜ìŠ¤ëª…ì´ë‚˜ íŒŒì¼ëª…ì„ ì–¸ê¸‰í•  ë•ŒëŠ” ë‹¤ìŒ í˜•ì‹ì„ ì‚¬ìš©í•˜ì„¸ìš”:
   - í´ë˜ìŠ¤ëª…: [IssueDetector](class_a1b2c3_IssueDetector)
   - íŒŒì¼ëª…: [issue_detector.py](file_a1b2c3)
   - í•¨ìˆ˜ëª…: [detect_issues()](func_a1b2c3_detect_issues_80)
11. ë‹µë³€ì€ **3ë‹¨ êµ¬ì„±**ìœ¼ë¡œ ì‘ì„±í•©ë‹ˆë‹¤:  
    - **í•µì‹¬ ìš”ì§€**: (2~4ì¤„ ìš”ì•½)  
    - **ê·¼ê±° ì„¹ì…˜ ìš”ì•½**: (ê´€ë ¨ í•¨ìˆ˜/í´ë˜ìŠ¤/ëª¨ë“ˆ, í•„ìš” ì‹œ ì½”ë“œ ì‹œê·¸ë‹ˆì²˜ í¬í•¨)  
    - **ì¡°ì¹˜/í™•ì¥ ë°©ë²•**: (íŒŒì¼ â†’ í´ë˜ìŠ¤ â†’ í•¨ìˆ˜ íë¦„ ë‹¨ìœ„ë¡œ ë‹¨ê³„ì  ì„¤ëª…)  
12. **ë¬¸ë‹¨ êµ¬ë¶„**: ê° ì„¹ì…˜ì€ ë°˜ë“œì‹œ ë¹ˆ ì¤„ë¡œ êµ¬ë¶„í•˜ê³ , ì„¹ì…˜ ì œëª©ì€ **êµµì€ ê¸€ì”¨**ë¡œ í‘œì‹œí•˜ì„¸ìš”.
13. ì •ë³´ê°€ ë¶€ì¡±í•œ ê²½ìš°, "ê·¼ê±° ë¶€ì¡±"ì„ ëª…ì‹œí•˜ê³  í•„ìš”í•œ ì¶”ê°€ ì •ë³´ë¥¼ ìš”ì²­í•©ë‹ˆë‹¤.  
14. ì¶”ì¸¡ì€ í•˜ì§€ ì•Šê³ , ë¶ˆí™•ì‹¤í•˜ë©´ ëª…í™•íˆ í‘œì‹œí•©ë‹ˆë‹¤.  
15. ë‹µë³€ì€ ì¹œê·¼í•˜ë©´ì„œë„ ì „ë¬¸ì ìœ¼ë¡œ, ë ˆê±°ì‹œ ì½”ë“œ ë¶„ì„/ë¦¬íŒ©í† ë§ ë§¥ë½ì„ ìœ ì§€í•©ë‹ˆë‹¤.

---

ğŸ“Œ ìµœì¢… ë‹µë³€:"""
            
            # ì‹œìŠ¤í…œ ë©”ì‹œì§€ëŠ” ê°„ë‹¨í•˜ê²Œ ì„¤ì •
            system_message = "ë‹¹ì‹ ì€ CodeMuse - ë ˆê±°ì‹œ ì½”ë“œë² ì´ìŠ¤ ë¶„ì„ ì–´ì‹œìŠ¤í„´íŠ¸ì…ë‹ˆë‹¤."
            user_message = prompt
        else:
            # ì»¨í…ìŠ¤íŠ¸ê°€ ì—†ëŠ” ê²½ìš° ê¸°ì¡´ ë°©ì‹ ì‚¬ìš©
            system_message = """
ë‹¹ì‹ ì€ "CodeMuse" â€“ ë ˆê±°ì‹œ ì½”ë“œë² ì´ìŠ¤ ë¶„ì„ ì–´ì‹œìŠ¤í„´íŠ¸ì…ë‹ˆë‹¤.

ëª©í‘œ: ì œê³µëœ Markdown(MD) ì„¹ì…˜ ì»¨í…ìŠ¤íŠ¸ë¥¼ ê·¼ê±°ë¡œ, ì§ˆë¬¸ ì˜ë„ì— ë§ê²Œ ì‹¤ìš©ì ì´ê³  ë§¥ë½ ìˆëŠ” ë‹µë³€ì„ ìƒì„±í•©ë‹ˆë‹¤.
"""
            user_message = request.message

        chat_messages = [{"role": "system", "content": system_message}, {"role": "user", "content": user_message}]

        model_name = request.model or "gpt-3.5-turbo"

        def format_sse(event: str, data: str) -> bytes:
            return f"event: {event}\ndata: {data}\n\n".encode("utf-8")

        async def token_generator():
            yield format_sse("status", "started")
            yield format_sse("model", model_name)

            # API í˜¸ì¶œ í†µê³„ ì—…ë°ì´íŠ¸
            current_time = time.time()
            request_id = str(uuid.uuid4())[:8]
            
            # ë¶„ë‹¹ í˜¸ì¶œ ìˆ˜ ê³„ì‚°
            api_call_stats["last_minute_calls"] = [
                call_time for call_time in api_call_stats["last_minute_calls"] 
                if current_time - call_time < 60
            ]
            api_call_stats["last_minute_calls"].append(current_time)
            api_call_stats["calls_per_minute"] = len(api_call_stats["last_minute_calls"])
            api_call_stats["total_calls"] += 1
            api_call_stats["last_call_time"] = current_time
            
            print(f"ğŸ” [{request_id}] STREAM OpenAI API í˜¸ì¶œ ì‹œì‘: model={model_name}, messages={len(chat_messages)}ê°œ")
            print(f"ğŸ”‘ [{request_id}] ì‚¬ìš© ì¤‘ì¸ API Key: {openai.api_key[:20]}..." if openai.api_key else f"âŒ [{request_id}] API Key ì—†ìŒ")
            print(f"ğŸ“Š [{request_id}] í†µê³„ - ì´ í˜¸ì¶œ: {api_call_stats['total_calls']}, ë¶„ë‹¹: {api_call_stats['calls_per_minute']}")

            # ìŠ¤íŠ¸ë¦¼ APIìš© retry ë¡œì§
            max_retries = 3
            for attempt in range(max_retries + 1):
                try:
                    stream = openai.chat.completions.create(
                        model=model_name,
                        messages=chat_messages,
                        stream=True,
                    )
                    api_call_stats["successful_calls"] += 1
                    print(f"âœ… [{request_id}] STREAM OpenAI API í˜¸ì¶œ ì„±ê³µ (ì‹œë„ {attempt + 1})")
                    break  # ì„±ê³µí•˜ë©´ ë£¨í”„ íƒˆì¶œ
                    
                except Exception as openai_error:
                    error_str = str(openai_error)
                    print(f"âŒ [{request_id}] STREAM OpenAI API í˜¸ì¶œ ì‹¤íŒ¨ (ì‹œë„ {attempt + 1}): {error_str}")
                    
                    # 429 ì˜¤ë¥˜ì¸ ê²½ìš°ì—ë§Œ ì¬ì‹œë„
                    if "429" in error_str and attempt < max_retries:
                        delay = 1.0 * (2 ** attempt)  # exponential backoff
                        print(f"â³ [{request_id}] {delay}ì´ˆ ëŒ€ê¸° í›„ ì¬ì‹œë„...")
                        await asyncio.sleep(delay)
                        continue
                    else:
                        # 429ê°€ ì•„ë‹ˆê±°ë‚˜ ìµœëŒ€ ì¬ì‹œë„ì— ë„ë‹¬í•œ ê²½ìš°
                        api_call_stats["failed_calls"] += 1
                        print(f"ğŸ“Š [{request_id}] ì‹¤íŒ¨ í†µê³„ - ì„±ê³µ: {api_call_stats['successful_calls']}, ì‹¤íŒ¨: {api_call_stats['failed_calls']}")
                        yield format_sse("error", f"LLM call failed: {error_str}")
                        yield format_sse("done", "error")
                        return
            
            # ìµœëŒ€ ì¬ì‹œë„ í›„ì—ë„ ì‹¤íŒ¨í•œ ê²½ìš° (ì´ë¡ ì ìœ¼ë¡œ ë„ë‹¬í•˜ì§€ ì•Šì•„ì•¼ í•¨)
            if attempt == max_retries:
                yield format_sse("error", "ìµœëŒ€ ì¬ì‹œë„ íšŸìˆ˜ ì´ˆê³¼")
                yield format_sse("done", "error")
                return

            full_text = []
            try:
                for chunk in stream:
                    token = None
                    try:
                        delta = chunk.choices[0].delta  # type: ignore[attr-defined]
                        token = getattr(delta, "content", None)
                    except Exception:
                        if chunk.choices and getattr(chunk.choices[0], "delta", None):
                            token = chunk.choices[0].delta.get("content") if isinstance(chunk.choices[0].delta, dict) else None
                    if token:
                        full_text.append(token)
                        yield format_sse("token", token)
            except Exception as stream_error:
                yield format_sse("error", f"stream error: {str(stream_error)}")
            finally:
                duration = time.time() - start_time
                yield format_sse("metrics", str({"response_time": round(duration, 3)}))
                yield format_sse("done", "ok")

        headers = {
            "Cache-Control": "no-cache",
            "Content-Type": "text/event-stream",
            "Connection": "keep-alive",
        }
        return StreamingResponse(token_generator(), headers=headers, media_type="text/event-stream")
    except Exception as e:
        raise HTTPException(status_code=500, detail=f"ìŠ¤íŠ¸ë¦¬ë° ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {str(e)}")

if __name__ == "__main__":
    import uvicorn
    uvicorn.run(app, host="0.0.0.0", port=8004)
