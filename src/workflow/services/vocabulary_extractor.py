"""
MD ë¬¸ì„œì—ì„œ ìš©ì–´ì§‘ì„ ì¶”ì¶œí•˜ëŠ” ëª¨ë“ˆ
"""

import os
import json
import re
from typing import Dict, List, Set
from pathlib import Path


class VocabularyExtractor:
    """MD ë¬¸ì„œì—ì„œ ìš©ì–´ì§‘ì„ ì¶”ì¶œí•˜ëŠ” í´ë˜ìŠ¤"""
    
    def __init__(self, generated_docs_path: str = None):
        if generated_docs_path is None:
            # ê¸°ë³¸ ê²½ë¡œ ì„¤ì •
            current_dir = os.path.dirname(os.path.dirname(os.path.dirname(__file__)))
            self.generated_docs_path = os.path.join(current_dir, "..", "generated_docs")
        else:
            self.generated_docs_path = generated_docs_path
        
        # ìš©ì–´ì§‘ íŒŒì¼ ê²½ë¡œ
        self.vocabulary_file = os.path.join(self.generated_docs_path, "vocabulary.json")
    
    def load_vocabulary(self) -> Dict[str, List[str]]:
        """ìƒì„±ëœ ìš©ì–´ì§‘ íŒŒì¼ì„ ë¡œë“œ"""
        
        try:
            # ë¨¼ì € ìƒì„±ëœ ìš©ì–´ì§‘ íŒŒì¼ì´ ìˆëŠ”ì§€ í™•ì¸
            if os.path.exists(self.vocabulary_file):
                with open(self.vocabulary_file, 'r', encoding='utf-8') as f:
                    vocabulary = json.load(f)
                print(f"ğŸ“š ìš©ì–´ì§‘ íŒŒì¼ ë¡œë“œ ì™„ë£Œ: {sum(len(v) for v in vocabulary.values())}ê°œ ìš©ì–´")
                return vocabulary
            
            # ìš©ì–´ì§‘ íŒŒì¼ì´ ì—†ìœ¼ë©´ MD íŒŒì¼ì—ì„œ ì¶”ì¶œ (í´ë°±)
            print("âš ï¸ ìš©ì–´ì§‘ íŒŒì¼ì´ ì—†ìŠµë‹ˆë‹¤. MD íŒŒì¼ì—ì„œ ì¶”ì¶œí•©ë‹ˆë‹¤...")
            vocabulary = {
                "class_names": [],
                "method_names": [],
                "function_names": [],
                "domain_concepts": [],
                "korean_terms": [],
                "technical_terms": []
            }
            
            # MD íŒŒì¼ë“¤ ìŠ¤ìº”
            for root, dirs, files in os.walk(self.generated_docs_path):
                for file in files:
                    if file.endswith('.md'):
                        file_path = os.path.join(root, file)
                        self._extract_from_md_file(file_path, vocabulary)
            
            # ì¤‘ë³µ ì œê±° ë° ì •ë ¬
            for key in vocabulary:
                vocabulary[key] = sorted(list(set(vocabulary[key])))
            
            print(f"ğŸ“š MDì—ì„œ ìš©ì–´ì§‘ ì¶”ì¶œ ì™„ë£Œ: {sum(len(v) for v in vocabulary.values())}ê°œ ìš©ì–´")
            return vocabulary
            
        except Exception as e:
            print(f"âŒ ìš©ì–´ì§‘ ë¡œë“œ ì‹¤íŒ¨: {e}")
            return {
                "class_names": [],
                "method_names": [],
                "function_names": [],
                "domain_concepts": [],
                "korean_terms": [],
                "technical_terms": []
            }
    
    def _extract_from_md_file(self, file_path: str, vocabulary: Dict[str, List[str]]):
        """MD íŒŒì¼ì—ì„œ ìš©ì–´ ì¶”ì¶œ"""
        
        try:
            with open(file_path, 'r', encoding='utf-8') as f:
                content = f.read()
            
            # í´ë˜ìŠ¤ëª… ì¶”ì¶œ (### í´ë˜ìŠ¤ëª… íŒ¨í„´)
            class_pattern = r'###\s+`([A-Z][a-zA-Z0-9_]*)`'
            classes = re.findall(class_pattern, content)
            vocabulary["class_names"].extend(classes)
            
            # ë©”ì„œë“œëª… ì¶”ì¶œ (#### ë©”ì„œë“œëª… íŒ¨í„´)
            method_pattern = r'####\s+`([a-zA-Z_][a-zA-Z0-9_]*)`'
            methods = re.findall(method_pattern, content)
            vocabulary["method_names"].extend(methods)
            
            # í•¨ìˆ˜ëª… ì¶”ì¶œ (### í•¨ìˆ˜ëª… íŒ¨í„´)
            function_pattern = r'###\s+`([a-zA-Z_][a-zA-Z0-9_]*)`'
            functions = re.findall(function_pattern, content)
            vocabulary["function_names"].extend(functions)
            
            # ê¸°ìˆ  ìš©ì–´ ì¶”ì¶œ (ì½”ë“œ ë¸”ë¡ ë‚´)
            code_pattern = r'```python\n(.*?)\n```'
            code_blocks = re.findall(code_pattern, content, re.DOTALL)
            for code in code_blocks:
                # í´ë˜ìŠ¤/í•¨ìˆ˜ ì •ì˜ ì¶”ì¶œ
                class_defs = re.findall(r'class\s+([A-Z][a-zA-Z0-9_]*)', code)
                method_defs = re.findall(r'def\s+([a-zA-Z_][a-zA-Z0-9_]*)', code)
                vocabulary["class_names"].extend(class_defs)
                vocabulary["method_names"].extend(method_defs)
            
            # ë„ë©”ì¸ ê°œë… ì¶”ì¶œ (í•œê¸€ + ì˜ì–´ í˜¼í•©)
            domain_pattern = r'[ê°€-í£]+(?:[A-Za-z]+|[ê°€-í£]+)*'
            domains = re.findall(domain_pattern, content)
            vocabulary["domain_concepts"].extend(domains)
            
            # í•œêµ­ì–´ ê¸°ìˆ  ìš©ì–´ ì¶”ì¶œ
            korean_tech_terms = [
                'ê²€ì¶œ', 'ë¶„ì„', 'ì²˜ë¦¬', 'ìƒì„±', 'ë³€í™˜', 'ê²€ì¦', 'ì˜¤ë¥˜', 'ì´ìŠˆ',
                'ì¡°ê±´', 'ê·œì¹™', 'ë¡œì§', 'íƒ€ì…', 'ë¶ˆì¼ì¹˜', 'ì¤‘ë³µ', 'ë³µì¡ì„±',
                'íŒŒì‹±', 'ë Œë”ë§', 'í¬ë§·íŒ…', 'ìŠ¤íŠ¸ë¦¬ë°', 'ì„ë² ë”©', 'ë²¡í„°',
                'ê²€ìƒ‰', 'ì¸ë±ì‹±', 'í† í°í™”', 'ì „ì²˜ë¦¬', 'í›„ì²˜ë¦¬'
            ]
            
            for term in korean_tech_terms:
                if term in content:
                    vocabulary["korean_terms"].append(term)
            
            # ì˜ì–´ ê¸°ìˆ  ìš©ì–´ ì¶”ì¶œ
            tech_terms = [
                'detection', 'analysis', 'processing', 'generation', 'transformation',
                'validation', 'error', 'issue', 'condition', 'rule', 'logic', 'type',
                'mismatch', 'duplicate', 'complexity', 'parsing', 'rendering', 'formatting',
                'streaming', 'embedding', 'vector', 'search', 'indexing', 'tokenization',
                'preprocessing', 'postprocessing', 'chunking', 'rag', 'llm', 'ai'
            ]
            
            for term in tech_terms:
                if term.lower() in content.lower():
                    vocabulary["technical_terms"].append(term)
            
        except Exception as e:
            print(f"âŒ MD íŒŒì¼ ì²˜ë¦¬ ì‹¤íŒ¨ {file_path}: {e}")
    
    def save_vocabulary(self, vocabulary: Dict[str, List[str]], output_path: str = None):
        """ìš©ì–´ì§‘ì„ íŒŒì¼ë¡œ ì €ì¥"""
        
        if output_path is None:
            output_path = os.path.join(self.generated_docs_path, "vocabulary.json")
        
        try:
            with open(output_path, 'w', encoding='utf-8') as f:
                json.dump(vocabulary, f, ensure_ascii=False, indent=2)
            
            print(f"ğŸ’¾ ìš©ì–´ì§‘ ì €ì¥ ì™„ë£Œ: {output_path}")
            
        except Exception as e:
            print(f"âŒ ìš©ì–´ì§‘ ì €ì¥ ì‹¤íŒ¨: {e}")


# ì „ì—­ ì¸ìŠ¤í„´ìŠ¤
vocabulary_extractor = VocabularyExtractor()
