from fastapi import FastAPI, HTTPException, Request
from fastapi.responses import StreamingResponse
from fastapi.middleware.cors import CORSMiddleware
from pydantic import BaseModel
import json
import os
import sys
from typing import Dict, Any, Optional
from pathlib import Path

# ìƒìœ„ ë””ë ‰í„°ë¦¬ë¥¼ pathì— ì¶”ê°€í•˜ì—¬ shared ëª¨ë“ˆ import ê°€ëŠ¥í•˜ê²Œ í•¨
_CURRENT_DIR = Path(__file__).parent
_SRC_DIR = _CURRENT_DIR.parent
if str(_SRC_DIR) not in sys.path:
    sys.path.insert(0, str(_SRC_DIR))

# ë¡œê¹… ì„¤ì • - logger ëª¨ë“ˆì„ ì§ì ‘ import
import logging
from pathlib import Path as _Path

# ë¡œê·¸ íŒŒì¼ ê²½ë¡œ ì„¤ì • (í”„ë¡œì íŠ¸ ë£¨íŠ¸ ê¸°ì¤€)
_PROJECT_ROOT = _SRC_DIR.parent
_LOG_FILE = _PROJECT_ROOT / "logs" / "workflow_detailed.log"
_LOG_FILE.parent.mkdir(parents=True, exist_ok=True)

# ë¡œê±° ì„¤ì •
logging.basicConfig(
    level=logging.INFO,
    format="%(asctime)s - %(name)s - %(levelname)s - %(message)s",
    handlers=[
        logging.StreamHandler(sys.stdout),
        logging.FileHandler(str(_LOG_FILE))
    ]
)
logger = logging.getLogger("workflow")

# í™˜ê²½ ë³€ìˆ˜ì—ì„œ ì„œë¹„ìŠ¤ URL ê°€ì ¸ì˜¤ê¸°
LLM_SERVICE_URL = os.getenv("LLM_SERVICE_URL", "http://localhost:8004")
RAG_SERVICE_URL = os.getenv("RAG_SERVICE_URL", "http://localhost:8003")

# ìµœì í™”ëœ Chunk ì›Œí¬í”Œë¡œìš° ì„œë¹„ìŠ¤ import
from .services.chunk_workflow_service import ChunkWorkflowService, ChunkWorkflowRequest

app = FastAPI(title="Workflow Orchestrator", version="1.0.0")

logger.info("ğŸš€ Workflow Orchestrator ì‹œì‘")

# CORS ì„¤ì •
app.add_middleware(
    CORSMiddleware,
    allow_origins=["*"],
    allow_credentials=True,
    allow_methods=["*"],
    allow_headers=["*"],
)

class WorkflowRequest(BaseModel):
    query: str
    model: str = "gpt-3.5-turbo"
    use_rag: bool = True
    session_id: Optional[str] = None

@app.get("/")
async def root():
    return {"message": "Workflow Orchestrator is running"}

@app.get("/health")
async def health_check():
    return {"status": "healthy", "service": "workflow_orchestrator"}

@app.options("/workflow/stream")
async def options_workflow_stream():
    """CORS preflight ìš”ì²­ ì²˜ë¦¬"""
    return {"message": "OK"}

@app.post("/workflow/stream")
async def process_workflow_stream(request: WorkflowRequest):
    """SSEë¡œ ì§„í–‰ìƒí™© + ìµœì¢…ì‘ë‹µ í† í° ìŠ¤íŠ¸ë¦¬ë°"""
    import json as _json
    try:
        chunk_service = ChunkWorkflowService()

        def format_sse(event: str, data: str) -> bytes:
            return f"event: {event}\ndata: {data}\n\n".encode("utf-8")

        async def generator():
            # ChunkWorkflowRequest ê°ì²´ ìƒì„±
            chunk_request = ChunkWorkflowRequest(
                query=request.query,
                model=request.model,
                use_rag=request.use_rag,
                session_id=request.session_id
            )
            
            # 1ë‹¨ê³„: ì§ˆë¬¸ í‘œì¤€í™”
            yield format_sse("status", "Step 1: Question standardization in progress...")
            yield format_sse("step", "1")
            
            # 2ë‹¨ê³„: í‚¤ì›Œë“œ ì¶”ì¶œ
            yield format_sse("status", "Step 2: Keyword extraction in progress...")
            yield format_sse("step", "2")
            
            # 3ë‹¨ê³„: RAG ê²€ìƒ‰
            yield format_sse("status", "Step 3: RAG search in progress...")
            yield format_sse("step", "3")
            
            # 4ë‹¨ê³„: í’ˆì§ˆ ê²€í† 
            yield format_sse("status", "Step 4: Quality review in progress...")
            yield format_sse("step", "4")
            
            # 5ë‹¨ê³„: ìµœì¢… ì‘ë‹µ ìƒì„±
            yield format_sse("status", "Step 5: Final response generation in progress...")
            yield format_sse("step", "5")
            
            # ì‹¤ì œ ì›Œí¬í”Œë¡œìš° ì‹¤í–‰
            result = await chunk_service.process_chunk_workflow(chunk_request)
            
            # í‘œì¤€í™”ëœ ì§ˆë¬¸ ì „ì†¡
            standardized_query = result.get("standardized_query", request.query)
            yield format_sse("standardized_question", standardized_query)
            
            # ê²°ê³¼ë¥¼ ìŠ¤íŠ¸ë¦¬ë°ìœ¼ë¡œ ì „ì†¡
            yield format_sse("status", "Response generation completed")
            yield format_sse("started", "")
            yield format_sse("model", result.get("model", request.model))
            
            # ìµœì¢… ì‘ë‹µ ìŠ¤íŠ¸ë¦¬ë°
            response_text = result.get("response", "")
            for token in response_text:
                yield format_sse("token", token)
            
            # ë©”íŠ¸ë¦­ìŠ¤ ì „ì†¡
            metrics = {
                "processing_time": result.get("processing_time", 0),
                "search_results_count": result.get("search_results_count", 0),
                "quality_score": result.get("quality_score", 0),
                "keywords": result.get("keywords", [])
            }
            yield format_sse("metrics", _json.dumps(metrics))
            yield format_sse("done", "ok")

        headers = {
            "Cache-Control": "no-cache",
            "Content-Type": "text/event-stream",
            "Connection": "keep-alive",
            "Access-Control-Allow-Origin": "*",
            "Access-Control-Allow-Methods": "GET, POST, OPTIONS",
            "Access-Control-Allow-Headers": "Content-Type, Authorization",
        }
        return StreamingResponse(generator(), headers=headers, media_type="text/event-stream")
    
    except Exception as e:
        logger.error(f"âŒ ìŠ¤íŠ¸ë¦¬ë° ì›Œí¬í”Œë¡œìš° ì˜¤ë¥˜: {str(e)}", exc_info=True)
        raise HTTPException(status_code=500, detail=str(e))

if __name__ == "__main__":
    import uvicorn
    uvicorn.run(app, host="0.0.0.0", port=8006)