"""
í…œí”Œë¦¿ ê¸°ë°˜ Chunk ë¬¸ì„œ ìƒì„±ê¸°
"""

import os
import json
import re
import aiohttp
from typing import List, Dict, Any
from dataclasses import asdict

try:
    from .ast_chunker import CodeChunk, chunk_directory
    from .template_renderer import ChunkTemplateRenderer
    from .chunkers import MultiLanguageChunker
except ImportError:
    from ast_chunker import CodeChunk, chunk_directory
    from template_renderer import ChunkTemplateRenderer
    from chunkers import MultiLanguageChunker


class TemplateChunkGenerator:
    """í…œí”Œë¦¿ ê¸°ë°˜ Chunk ë¬¸ì„œ ìƒì„±ê¸°"""
    
    def __init__(self, source_dir: str, output_dir: str, template_dir: str = None):
        self.source_dir = source_dir
        self.output_dir = output_dir
        self.renderer = ChunkTemplateRenderer(template_dir)
        
        # ë‹¤ì¤‘ ì–¸ì–´ Chunker ì´ˆê¸°í™”
        self.multi_chunker = MultiLanguageChunker(project_root=source_dir)
        
        # ì¶œë ¥ ë””ë ‰í† ë¦¬ ìƒì„±
        os.makedirs(output_dir, exist_ok=True)
    
    async def generate_documents(self, upload_to_rag: bool = True) -> Dict[str, Any]:
        """ë¬¸ì„œ ìƒì„± ë©”ì¸ í•¨ìˆ˜"""
        import time
        total_start = time.time()
        
        print(f"ğŸš€ í…œí”Œë¦¿ ê¸°ë°˜ Chunk ë¬¸ì„œ ìƒì„± ì‹œì‘...")
        print(f"   ğŸ“‚ ì†ŒìŠ¤: {self.source_dir}")
        print(f"   ğŸ“‚ ì¶œë ¥: {self.output_dir}")
        
        # 1. Chunk ìƒì„± (ë‹¤ì¤‘ ì–¸ì–´ ì§€ì›)
        step1_start = time.time()
        all_chunks = self.multi_chunker.chunk_directory(self.source_dir)
        step1_time = time.time() - step1_start
        print(f"   ğŸ¯ ì´ {len(all_chunks)}ê°œì˜ chunk ìƒì„± ì™„ë£Œ [â±ï¸ {step1_time:.2f}ì´ˆ]")
        
        # í”„ë¡œì íŠ¸ ë¶„ì„ ê²°ê³¼ ì¶œë ¥
        project_summary = self.multi_chunker.get_project_summary(all_chunks)
        print(f"   ğŸ“Š í”„ë¡œì íŠ¸ ë¶„ì„:")
        print(f"      - íƒ€ì…: {project_summary['project_analysis']['project_type']}")
        print(f"      - í”„ë ˆì„ì›Œí¬: {project_summary['project_analysis']['framework_info']}")
        print(f"      - ì–¸ì–´ ë¶„í¬: {project_summary['language_distribution']}")
        print(f"      - í”„ë ˆì„ì›Œí¬ ë¶„í¬: {project_summary['framework_distribution']}")
        
        # 2. íŒŒì¼ë³„ ê·¸ë£¹í™”
        step2_start = time.time()
        file_chunks = self._group_chunks_by_file(all_chunks)
        step2_time = time.time() - step2_start
        print(f"   ğŸ“‚ íŒŒì¼ë³„ ê·¸ë£¹í™” ì™„ë£Œ: {len(file_chunks)}ê°œ íŒŒì¼ [â±ï¸ {step2_time:.2f}ì´ˆ]")
        
        # 3. êµ¬ì¡°í™”ëœ ë¬¸ì„œ ìƒì„±
        step3_start = time.time()
        generated_files = self._generate_structured_documents(file_chunks)
        step3_time = time.time() - step3_start
        print(f"   ğŸ“ MD ë¬¸ì„œ ìƒì„± ì™„ë£Œ: {len(generated_files)}ê°œ íŒŒì¼ [â±ï¸ {step3_time:.2f}ì´ˆ]")
        
        # 4. Chunk ë©”íƒ€ë°ì´í„° ì €ì¥
        step4_start = time.time()
        metadata_file = self._save_chunk_metadata(all_chunks)
        step4_time = time.time() - step4_start
        print(f"   ğŸ’¾ ë©”íƒ€ë°ì´í„° ì €ì¥ ì™„ë£Œ [â±ï¸ {step4_time:.2f}ì´ˆ]")
        
        # 5. í”„ë¡œì íŠ¸ ìš”ì•½ ìƒì„±
        step5_start = time.time()
        summary_file = self._generate_project_summary(all_chunks, file_chunks)
        step5_time = time.time() - step5_start
        print(f"   ğŸ“Š í”„ë¡œì íŠ¸ ìš”ì•½ ìƒì„± ì™„ë£Œ [â±ï¸ {step5_time:.2f}ì´ˆ]")
        
        # 6. ìš©ì–´ì§‘ ìƒì„±
        step6_start = time.time()
        vocabulary_file = self._generate_vocabulary(all_chunks)
        step6_time = time.time() - step6_start
        print(f"   ğŸ“š ìš©ì–´ì§‘ ìƒì„± ì™„ë£Œ [â±ï¸ {step6_time:.2f}ì´ˆ]")
        
        # 7. ì—”í„°í”„ë¼ì´ì¦ˆ ì•„í‚¤í…ì²˜ ë¶„ì„ (ì—”í„°í”„ë¼ì´ì¦ˆ ì• í”Œë¦¬ì¼€ì´ì…˜ì¸ ê²½ìš°)
        step7_start = time.time()
        architecture_file = None
        business_logic_file = None
        if self._is_enterprise_project(all_chunks):
            architecture_file = self._generate_enterprise_architecture(all_chunks, file_chunks)
            business_logic_file = self._generate_business_logic_analysis(all_chunks)
        step7_time = time.time() - step7_start
        if architecture_file:
            print(f"   ğŸ—ï¸ ì—”í„°í”„ë¼ì´ì¦ˆ ì•„í‚¤í…ì²˜ ë¶„ì„ ì™„ë£Œ [â±ï¸ {step7_time:.2f}ì´ˆ]")
        
        result = {
            "source_dir": self.source_dir,
            "output_dir": self.output_dir,
            "generated_files": generated_files,
            "total_chunks": len(all_chunks),
            "total_files": len(file_chunks),
            "chunk_metadata_file": metadata_file,
            "project_summary_file": summary_file,
            "vocabulary_file": vocabulary_file,
            "architecture_file": architecture_file,
            "business_logic_file": business_logic_file
        }
        
        step6_time = 0.0
        if upload_to_rag:
            # ğŸ¯ RAGì— MD ë¬¸ì„œë“¤ì„ ì„¹ì…˜ë³„ë¡œ ë¶„í• í•˜ì—¬ ì—…ë¡œë“œ
            step6_start = time.time()
            print(f"\nğŸ”„ RAG ì—…ë¡œë“œ ì‹œì‘...")
            rag_upload_result = await self._upload_md_sections_to_rag(all_chunks, generated_files, summary_file)
            result["rag_upload_result"] = rag_upload_result
            step6_time = time.time() - step6_start
            print(f"   ğŸ“¤ RAG ì—…ë¡œë“œ ì™„ë£Œ [â±ï¸ {step6_time:.2f}ì´ˆ]")
        else:
            print("\nâ­ï¸ RAG ì—…ë¡œë“œ ê±´ë„ˆëœ€ (generate-local ëª¨ë“œ)")
        
        total_time = time.time() - total_start
        print(f"\nâœ… ì „ì²´ ë¬¸ì„œ ìƒì„± ì™„ë£Œ [â±ï¸ ì´ {total_time:.2f}ì´ˆ]")
        print(f"   ğŸ“ˆ ë‹¨ê³„ë³„ ì‹œê°„:")
        print(f"     1. Chunk ìƒì„±: {step1_time:.2f}ì´ˆ ({step1_time/total_time*100:.1f}%)")
        print(f"     2. íŒŒì¼ ê·¸ë£¹í™”: {step2_time:.2f}ì´ˆ ({step2_time/total_time*100:.1f}%)")
        print(f"     3. MD ë¬¸ì„œ ìƒì„±: {step3_time:.2f}ì´ˆ ({step3_time/total_time*100:.1f}%)")
        print(f"     4. ë©”íƒ€ë°ì´í„° ì €ì¥: {step4_time:.2f}ì´ˆ ({step4_time/total_time*100:.1f}%)")
        print(f"     5. í”„ë¡œì íŠ¸ ìš”ì•½: {step5_time:.2f}ì´ˆ ({step5_time/total_time*100:.1f}%)")
        print(f"     6. RAG ì—…ë¡œë“œ: {step6_time:.2f}ì´ˆ ({(step6_time/total_time*100 if total_time>0 else 0):.1f}%)")
        
        return result
    
    def _group_chunks_by_file(self, chunks: List[CodeChunk]) -> Dict[str, List[CodeChunk]]:
        """íŒŒì¼ë³„ë¡œ chunk ê·¸ë£¹í™”"""
        
        file_chunks = {}
        for chunk in chunks:
            if chunk.file_path not in file_chunks:
                file_chunks[chunk.file_path] = []
            file_chunks[chunk.file_path].append(chunk)
        
        return file_chunks
    
    def _generate_structured_documents(self, file_chunks: Dict[str, List[CodeChunk]]) -> List[str]:
        """êµ¬ì¡°í™”ëœ ë¬¸ì„œ ìƒì„±"""
        
        generated_files = []
        
        for file_path, chunks in file_chunks.items():
            # MD íŒŒì¼ ê²½ë¡œ ìƒì„±
            md_file_path = self._get_md_file_path(file_path)
            
            # ë””ë ‰í† ë¦¬ ìƒì„±
            os.makedirs(os.path.dirname(md_file_path), exist_ok=True)
            
            # í…œí”Œë¦¿ìœ¼ë¡œ ë¬¸ì„œ ìƒì„±
            md_content = self.renderer.render_file_document(file_path, chunks, self.source_dir)
            
            # íŒŒì¼ ì €ì¥
            with open(md_file_path, 'w', encoding='utf-8') as f:
                f.write(md_content)
            
            generated_files.append(md_file_path)
            
            # ìƒëŒ€ ê²½ë¡œë¡œ ì¶œë ¥
            rel_path = os.path.relpath(md_file_path, self.output_dir)
            print(f"   âœ… {rel_path}")
        
        return generated_files
    
    def _get_md_file_path(self, source_file_path: str) -> str:
        """ì†ŒìŠ¤ íŒŒì¼ ê²½ë¡œë¥¼ MD íŒŒì¼ ê²½ë¡œë¡œ ë³€í™˜ (ë‹¤ì¤‘ ì–¸ì–´ ì§€ì›)"""
        
        # ì†ŒìŠ¤ ë””ë ‰í† ë¦¬ë¡œë¶€í„°ì˜ ìƒëŒ€ ê²½ë¡œ
        rel_path = os.path.relpath(source_file_path, self.source_dir)
        
        # íŒŒì¼ í™•ì¥ìë¥¼ .mdë¡œ ë³€ê²½
        name, ext = os.path.splitext(rel_path)
        md_rel_path = name + '.md'
        
        # ì „ì²´ ê²½ë¡œ ìƒì„±
        return os.path.join(self.output_dir, md_rel_path)
    
    def _save_chunk_metadata(self, chunks: List[CodeChunk]) -> str:
        """RAGìš© chunk ë©”íƒ€ë°ì´í„° ì €ì¥"""
        
        metadata_file = os.path.join(self.output_dir, "chunk_metadata.json")
        
        # CodeChunkë¥¼ ë”•ì…”ë„ˆë¦¬ë¡œ ë³€í™˜
        chunks_data = []
        for chunk in chunks:
            chunk_dict = asdict(chunk)
            chunks_data.append(chunk_dict)
        
        metadata = {
            "total_chunks": len(chunks),
            "generation_timestamp": "2024-01-01T00:00:00Z",  # ì‹¤ì œë¡œëŠ” í˜„ì¬ ì‹œê°„
            "source_directory": self.source_dir,
            "chunks": chunks_data
        }
        
        with open(metadata_file, 'w', encoding='utf-8') as f:
            json.dump(metadata, f, ensure_ascii=False, indent=2)
        
        return metadata_file
    
    def _generate_project_summary(self, all_chunks: List[CodeChunk], file_chunks: Dict[str, List[CodeChunk]]) -> str:
        """í”„ë¡œì íŠ¸ ìš”ì•½ ìƒì„±"""
        
        # í”„ë¡œì íŠ¸ ë°ì´í„° ì¤€ë¹„
        project_data = self._prepare_project_data(all_chunks, file_chunks)
        
        # í…œí”Œë¦¿ìœ¼ë¡œ ìš”ì•½ ìƒì„±
        summary_content = self.renderer.render_project_summary(project_data)
        
        # íŒŒì¼ ì €ì¥
        summary_file = os.path.join(self.output_dir, "project_summary.md")
        with open(summary_file, 'w', encoding='utf-8') as f:
            f.write(summary_content)
        
        return summary_file
    
    def _prepare_project_data(self, all_chunks: List[CodeChunk], file_chunks: Dict[str, List[CodeChunk]]) -> Dict[str, Any]:
        """í”„ë¡œì íŠ¸ ë°ì´í„° ì¤€ë¹„"""
        
        project_name = os.path.basename(self.source_dir)
        
        # ê¸°ë³¸ í†µê³„
        total_files = len(file_chunks)
        total_chunks = len(all_chunks)
        total_tokens = sum(chunk.token_count or 0 for chunk in all_chunks)
        
        # ë³µì¡ë„ í†µê³„
        complexities = [chunk.complexity for chunk in all_chunks if chunk.complexity]
        avg_complexity = sum(complexities) / len(complexities) if complexities else 0
        max_complexity = max(complexities) if complexities else 0
        
        # Chunk ë¶„í¬
        chunk_distribution = self._analyze_chunk_distribution(all_chunks)
        
        # ë³µì¡ë„ ë¶„í¬
        complexity_distribution = self._analyze_complexity_distribution(all_chunks)
        
        # ë³µì¡í•œ í•¨ìˆ˜ TOP 10
        top_complex_functions = self._get_top_complex_functions(all_chunks, 10)
        
        # ì¸ê¸° íƒœê·¸ TOP 20
        popular_tags = self._get_popular_tags(all_chunks, 20)
        
        # ë””ë ‰í† ë¦¬ êµ¬ì¡°
        directory_tree = self._generate_directory_tree()
        directories = self._analyze_directories(all_chunks)
        
        # ì •ì„± ìš”ì•½/í’ˆì§ˆ/ê°œì„  ì œì•ˆ ì¶”ë¡ 
        project_summary, main_purpose, main_domain, core_features = self._infer_project_overview(all_chunks)
        main_modules = self._infer_main_modules(all_chunks)
        risky_files, duplicate_code_count = self._find_risky_files(all_chunks)
        test_code_percentage = self._estimate_test_ratio()
        refactoring_priorities, recommended_practices, modernization_suggestions = self._recommend_refactoring(risky_files, avg_complexity)
        
        return {
            "project_name": project_name,
            "total_files": total_files,
            "total_chunks": total_chunks,
            "total_tokens": total_tokens,
            "avg_complexity": avg_complexity,
            "max_complexity": max_complexity,
            "chunk_distribution": chunk_distribution,
            "complexity_distribution": complexity_distribution,
            "top_complex_functions": top_complex_functions,
            "popular_tags": popular_tags,
            "directory_tree": directory_tree,
            "directories": directories,
            # ë³´ê°• ì„¹ì…˜ ë°ì´í„°
            "project_summary": project_summary,
            "main_purpose": main_purpose,
            "main_domain": main_domain,
            "core_features": core_features,
            "main_modules": main_modules,
            "risky_files": risky_files,
            "duplicate_code_count": duplicate_code_count,
            "test_code_percentage": test_code_percentage,
            "refactoring_priorities": refactoring_priorities,
            "recommended_practices": recommended_practices,
            "modernization_suggestions": modernization_suggestions
        }

    def _infer_project_overview(self, chunks: List[CodeChunk]) -> tuple:
        """í”„ë¡œì íŠ¸ ê°œìš”/ëª©ì /ë„ë©”ì¸/í•µì‹¬ ê¸°ëŠ¥ì„ ê°„ë‹¨ ê·œì¹™ìœ¼ë¡œ ì¶”ë¡ """
        try:
            file_paths = [c.file_path for c in chunks]
            text = " ".join(file_paths).lower()
            domain = os.path.basename(self.source_dir)
            purpose = "ì½”ë“œ ë¶„ì„/ê²€ì¦" if ("analyzer" in text or "analyz" in text) else "ì¼ë°˜ ì• í”Œë¦¬ì¼€ì´ì…˜"
            features = []
            if any("issue_detector" in p for p in file_paths):
                features.append("ì´ìŠˆ/ì˜¤ë¥˜ ê²€ì¶œ")
            if any("metrics" in p for p in file_paths):
                features.append("ì§€í‘œ/ë©”íŠ¸ë¦­ ìƒì„±")
            if any("condition" in p for p in file_paths):
                features.append("ì¡°ê±´ íŒŒì‹±/ë¶„ì„")
            if not features:
                features = ["í•µì‹¬ ê¸°ëŠ¥ ìë™ ì¶”ì¶œ ëŒ€ê¸°"]
            summary = f"'{domain}' ì½”ë“œë² ì´ìŠ¤ëŠ” {purpose}ì„(ë¥¼) ìˆ˜í–‰í•˜ë©°, {', '.join(features)}ë¥¼(ì„) í¬í•¨í•©ë‹ˆë‹¤."
            return summary, purpose, domain, features
        except Exception:
            return "", "", os.path.basename(self.source_dir), []

    def _infer_main_modules(self, chunks: List[CodeChunk]) -> List[Dict[str, Any]]:
        """ì£¼ìš” ë””ë ‰í† ë¦¬ëª…ì„ ëª¨ë“ˆë¡œ ìš”ì•½"""
        by_dir = {}
        for c in chunks:
            rel = os.path.relpath(c.file_path, self.source_dir)
            d = os.path.dirname(rel).split(os.sep)
            key = d[0] if d and d[0] else "root"
            by_dir.setdefault(key, set()).add(os.path.basename(c.file_path))
        modules = []
        desc_map = {
            "analyzers": "ë¶„ì„/ê²€ì¦ ë¡œì§",
            "formatters": "ì¶œë ¥/ì„œì‹ ì²˜ë¦¬",
            "streaming": "ìŠ¤íŠ¸ë¦¬ë°/ì‹¤ì‹œê°„ ì²˜ë¦¬",
            "shared": "ê³µí†µ ìœ í‹¸/ëª¨ë¸",
            "models": "ë°ì´í„°/ë„ë©”ì¸ ëª¨ë¸",
            "utils": "ë„ìš°ë¯¸ í•¨ìˆ˜ ëª¨ìŒ"
        }
        for name, files in by_dir.items():
            modules.append({
                "name": name,
                "description": desc_map.get(name, "ëª¨ë“ˆ ê¸°ëŠ¥ ìš”ì•½"),
                "key_components": sorted(list(files))[:6]
            })
        # ê°€ì¥ ì˜ë¯¸ìˆëŠ” ìƒìœ„ 6ê°œë§Œ
        return sorted(modules, key=lambda m: len(m["key_components"]), reverse=True)[:6]

    def _find_risky_files(self, chunks: List[CodeChunk]) -> tuple:
        """íŒŒì¼ë³„ ë³µì¡ë„ í•©ìœ¼ë¡œ ìƒìœ„ ìœ„í—˜ íŒŒì¼ ì¶”ì • + ì¤‘ë³µ ì¶”ì • ê°œìˆ˜"""
        score = {}
        fn_hashes = {}
        for c in chunks:
            score[c.file_path] = score.get(c.file_path, 0) + (c.complexity or 0)
            if c.code_hash:
                fn_hashes.setdefault(c.code_hash, 0)
                fn_hashes[c.code_hash] += 1
        risky = [
            os.path.relpath(fp, self.source_dir) for fp, s in sorted(score.items(), key=lambda x: x[1], reverse=True)[:8]
        ]
        duplicates = sum(1 for k,v in fn_hashes.items() if v > 1)
        return risky, duplicates

    def _estimate_test_ratio(self) -> int:
        """í…ŒìŠ¤íŠ¸ ì½”ë“œ ë¹„ìœ¨ ê·¼ì‚¬ì¹˜(ê°„ë‹¨ ê·œì¹™)"""
        try:
            total, tests = 0, 0
            for root, dirs, files in os.walk(self.source_dir):
                for f in files:
                    if f.endswith('.py'):
                        total += 1
                        rel = os.path.relpath(os.path.join(root, f), self.source_dir)
                        if rel.startswith('tests') or f.startswith('test_'):
                            tests += 1
            return int((tests / total * 100)) if total else 0
        except Exception:
            return 0

    def _recommend_refactoring(self, risky_files: List[str], avg_complexity: float) -> tuple:
        """ë¦¬íŒ©í† ë§/í˜„ëŒ€í™” ì œì•ˆ ìƒì„±"""
        priorities = risky_files[:5]
        practices = [
            "í° í•¨ìˆ˜ ë¶„ë¦¬(SRP)",
            "íƒ€ì… íŒíŠ¸/ì •ì  ë¶„ì„ ê°•í™”",
            "ë‹¨ìœ„ í…ŒìŠ¤íŠ¸ ì¶”ê°€",
            "ë³µì¡ë„ ë†’ì€ ë¶„ê¸° ë‹¨ìˆœí™”"
        ]
        modernization = [
            "CI ë„ì… ë° ìë™ í…ŒìŠ¤íŠ¸",
            "ë¡œê¹…/ì˜µì €ë²„ë¹Œë¦¬í‹° ê°•í™”",
            "ì„¤ì •ì˜ í™˜ê²½ë³€ìˆ˜í™”/ë³´ì•ˆ ë¹„ë°€ ë¶„ë¦¬"
        ]
        if avg_complexity > 7:
            practices.append("í•µì‹¬ ê²½ë¡œ ìš°ì„  ë¦¬íŒ©í† ë§")
        return priorities, practices, modernization
    
    def _analyze_chunk_distribution(self, chunks: List[CodeChunk]) -> Dict[str, Dict[str, Any]]:
        """Chunk íƒ€ì…ë³„ ë¶„í¬ ë¶„ì„"""
        
        distribution = {}
        total_chunks = len(chunks)
        
        for chunk in chunks:
            chunk_type = chunk.chunk_type
            if chunk_type not in distribution:
                distribution[chunk_type] = {"count": 0}
            distribution[chunk_type]["count"] += 1
        
        # ë¹„ìœ¨ ê³„ì‚°
        type_names = {
            "overview": "ğŸ“‹ íŒŒì¼ ê°œìš”",
            "class": "ğŸ—ï¸ í´ë˜ìŠ¤",
            "function": "âš™ï¸ í•¨ìˆ˜",
            "async_function": "ğŸ”„ ë¹„ë™ê¸° í•¨ìˆ˜",
            "method": "ğŸ”§ ë©”ì„œë“œ"
        }
        
        for chunk_type, data in distribution.items():
            data["percentage"] = (data["count"] / total_chunks * 100) if total_chunks > 0 else 0
            data["display_name"] = type_names.get(chunk_type, chunk_type)
        
        return distribution
    
    def _analyze_complexity_distribution(self, chunks: List[CodeChunk]) -> Dict[str, int]:
        """ë³µì¡ë„ ë¶„í¬ ë¶„ì„"""
        
        distribution = {"ë‚®ìŒ(1-3)": 0, "ë³´í†µ(4-7)": 0, "ë†’ìŒ(8-15)": 0, "ë§¤ìš°ë†’ìŒ(16+)": 0}
        
        for chunk in chunks:
            if chunk.complexity:
                if chunk.complexity <= 3:
                    distribution["ë‚®ìŒ(1-3)"] += 1
                elif chunk.complexity <= 7:
                    distribution["ë³´í†µ(4-7)"] += 1
                elif chunk.complexity <= 15:
                    distribution["ë†’ìŒ(8-15)"] += 1
                else:
                    distribution["ë§¤ìš°ë†’ìŒ(16+)"] += 1
        
        return {k: v for k, v in distribution.items() if v > 0}
    
    def _get_top_complex_functions(self, chunks: List[CodeChunk], limit: int) -> List[Dict[str, Any]]:
        """ë³µì¡í•œ í•¨ìˆ˜ TOP N"""
        
        functions = []
        for chunk in chunks:
            if chunk.chunk_type in ["function", "async_function", "method"] and chunk.complexity:
                functions.append({
                    "name": chunk.name,
                    "file_name": os.path.basename(chunk.file_path),
                    "complexity": chunk.complexity,
                    "token_count": chunk.token_count or 0
                })
        
        # ë³µì¡ë„ ìˆœìœ¼ë¡œ ì •ë ¬
        functions.sort(key=lambda x: x["complexity"], reverse=True)
        
        return functions[:limit]
    
    def _get_popular_tags(self, chunks: List[CodeChunk], limit: int) -> List[tuple]:
        """ì¸ê¸° íƒœê·¸ TOP N"""
        
        tag_counts = {}
        for chunk in chunks:
            if chunk.tags:
                for tag in chunk.tags:
                    tag_counts[tag] = tag_counts.get(tag, 0) + 1
        
        # ë¹ˆë„ìˆœìœ¼ë¡œ ì •ë ¬
        popular_tags = sorted(tag_counts.items(), key=lambda x: x[1], reverse=True)
        
        return popular_tags[:limit]
    
    def _generate_directory_tree(self) -> str:
        """ë””ë ‰í† ë¦¬ íŠ¸ë¦¬ ìƒì„±"""
        
        # ê°„ë‹¨í•œ íŠ¸ë¦¬ êµ¬ì¡° (ì‹¤ì œë¡œëŠ” ë” ë³µì¡í•œ ë¡œì§ í•„ìš”)
        tree_lines = []
        tree_lines.append(f"{os.path.basename(self.source_dir)}/")
        
        # ë””ë ‰í† ë¦¬ êµ¬ì¡° ìˆ˜ì§‘
        dirs = set()
        for root, dirnames, filenames in os.walk(self.source_dir):
            rel_root = os.path.relpath(root, self.source_dir)
            if rel_root != ".":
                dirs.add(rel_root)
        
        # ì •ë ¬í•˜ì—¬ íŠ¸ë¦¬ ìƒì„±
        for dir_path in sorted(dirs):
            depth = dir_path.count(os.sep)
            indent = "  " * (depth + 1)
            dir_name = os.path.basename(dir_path)
            tree_lines.append(f"{indent}â”œâ”€â”€ {dir_name}/")
        
        return "\n".join(tree_lines)
    
    def _analyze_directories(self, chunks: List[CodeChunk]) -> List[Dict[str, Any]]:
        """ë””ë ‰í† ë¦¬ë³„ ë¶„ì„"""
        
        dir_info = {}
        
        for chunk in chunks:
            rel_path = os.path.relpath(chunk.file_path, self.source_dir)
            rel_dir = os.path.dirname(rel_path)
            
            if rel_dir == "":
                rel_dir = "root"
            
            if rel_dir not in dir_info:
                dir_info[rel_dir] = {
                    "name": rel_dir,
                    "path": rel_dir,
                    "files": set(),
                    "chunks": 0
                }
            
            dir_info[rel_dir]["files"].add(os.path.basename(chunk.file_path))
            dir_info[rel_dir]["chunks"] += 1
        
        # ì„¸íŠ¸ë¥¼ ê°œìˆ˜ë¡œ ë³€í™˜
        directories = []
        for dir_data in dir_info.values():
            dir_data["files"] = len(dir_data["files"])
            directories.append(dir_data)
        
        return directories
    
    def _generate_vocabulary(self, all_chunks: List[Any]) -> str:
        """Chunkì—ì„œ ìš©ì–´ì§‘ ìƒì„±"""
        
        vocabulary = {
            "class_names": [],
            "method_names": [],
            "function_names": [],
            "domain_concepts": [],
            "korean_terms": [],
            "technical_terms": []
        }
        
        # Chunkì—ì„œ ìš©ì–´ ì¶”ì¶œ
        for chunk in all_chunks:
            self._extract_terms_from_chunk(chunk, vocabulary)
        
        # ì¤‘ë³µ ì œê±° ë° ì •ë ¬
        for key in vocabulary:
            vocabulary[key] = sorted(list(set(vocabulary[key])))
        
        # ìš©ì–´ì§‘ íŒŒì¼ ì €ì¥
        vocabulary_file = os.path.join(self.output_dir, "vocabulary.json")
        with open(vocabulary_file, 'w', encoding='utf-8') as f:
            json.dump(vocabulary, f, ensure_ascii=False, indent=2)
        
        print(f"   ğŸ“š ìš©ì–´ì§‘ ì €ì¥: {vocabulary_file}")
        print(f"   ğŸ“Š ìš©ì–´ í†µê³„:")
        for key, terms in vocabulary.items():
            print(f"      {key}: {len(terms)}ê°œ")
        
        return vocabulary_file
    
    def _is_enterprise_project(self, chunks: List[Any]) -> bool:
        """ì—”í„°í”„ë¼ì´ì¦ˆ í”„ë¡œì íŠ¸ì¸ì§€ ê°ì§€"""
        enterprise_count = 0
        for chunk in chunks:
            if hasattr(chunk, 'framework') and chunk.framework in ['spring-boot', 'vue']:
                enterprise_count += 1
            if hasattr(chunk, 'chunk_type') and chunk.chunk_type in ['class', 'component']:
                enterprise_patterns = ['controller', 'service', 'entity', 'dto', 'repository', 'component', 'store']
                if any(pattern in chunk.name.lower() for pattern in enterprise_patterns):
                    enterprise_count += 1
        
        return enterprise_count >= 5
    
    def _generate_enterprise_architecture(self, chunks: List[Any], file_chunks: Dict[str, List[Any]]) -> str:
        """ì—”í„°í”„ë¼ì´ì¦ˆ ì•„í‚¤í…ì²˜ ë¶„ì„ ë¬¸ì„œ ìƒì„±"""
        try:
            from .chunkers.enterprise_chunker import EnterpriseChunker
            enterprise_chunker = EnterpriseChunker(self.source_dir)
            
            # ì•„í‚¤í…ì²˜ ë¶„ì„ ë°ì´í„° ìƒì„±
            architecture_data = enterprise_chunker.analyze_architecture(chunks, file_chunks)
            architecture_data['source_directory'] = self.source_dir
            architecture_data['project_name'] = os.path.basename(self.source_dir)
            
            # í…œí”Œë¦¿ ë Œë”ë§
            content = self.renderer.render_enterprise_architecture(architecture_data)
            
            # íŒŒì¼ ì €ì¥
            architecture_file = os.path.join(self.output_dir, "enterprise_architecture.md")
            with open(architecture_file, 'w', encoding='utf-8') as f:
                f.write(content)
            
            return architecture_file
        except Exception as e:
            print(f"   âš ï¸ ì—”í„°í”„ë¼ì´ì¦ˆ ì•„í‚¤í…ì²˜ ë¶„ì„ ì‹¤íŒ¨: {e}")
            return None
    
    def _generate_business_logic_analysis(self, chunks: List[Any]) -> str:
        """ë¹„ì¦ˆë‹ˆìŠ¤ ë¡œì§ ë¶„ì„ ë¬¸ì„œ ìƒì„±"""
        try:
            from .chunkers.enterprise_chunker import EnterpriseChunker
            enterprise_chunker = EnterpriseChunker(self.source_dir)
            
            # ë¹„ì¦ˆë‹ˆìŠ¤ ë¡œì§ ë¶„ì„ ë°ì´í„° ìƒì„±
            business_data = enterprise_chunker.analyze_business_logic(chunks)
            business_data['source_directory'] = self.source_dir
            business_data['project_name'] = os.path.basename(self.source_dir)
            
            # í…œí”Œë¦¿ ë Œë”ë§
            content = self.renderer.render_business_logic_analysis(business_data)
            
            # íŒŒì¼ ì €ì¥
            business_file = os.path.join(self.output_dir, "business_logic_analysis.md")
            with open(business_file, 'w', encoding='utf-8') as f:
                f.write(content)
            
            return business_file
        except Exception as e:
            print(f"   âš ï¸ ë¹„ì¦ˆë‹ˆìŠ¤ ë¡œì§ ë¶„ì„ ì‹¤íŒ¨: {e}")
            return None
    
    def _extract_terms_from_chunk(self, chunk: Any, vocabulary: Dict[str, List[str]]):
        """Chunkì—ì„œ ìš©ì–´ ì¶”ì¶œ"""
        
        # í´ë˜ìŠ¤ëª… ì¶”ì¶œ
        if chunk.chunk_type == "class" and chunk.name:
            vocabulary["class_names"].append(chunk.name)
        
        # ë©”ì„œë“œëª… ì¶”ì¶œ
        if chunk.chunk_type == "method" and chunk.name:
            vocabulary["method_names"].append(chunk.name)
        
        # í•¨ìˆ˜ëª… ì¶”ì¶œ
        if chunk.chunk_type in ["function", "async_function"] and chunk.name:
            vocabulary["function_names"].append(chunk.name)
        
        # ë…ìŠ¤íŠ¸ë§ì—ì„œ ë„ë©”ì¸ ê°œë… ì¶”ì¶œ
        if chunk.docstring:
            docstring = chunk.docstring.lower()
            
            # í•œêµ­ì–´ ê¸°ìˆ  ìš©ì–´
            korean_terms = [
                'ê²€ì¶œ', 'ë¶„ì„', 'ì²˜ë¦¬', 'ìƒì„±', 'ë³€í™˜', 'ê²€ì¦', 'ì˜¤ë¥˜', 'ì´ìŠˆ',
                'ì¡°ê±´', 'ê·œì¹™', 'ë¡œì§', 'íƒ€ì…', 'ë¶ˆì¼ì¹˜', 'ì¤‘ë³µ', 'ë³µì¡ì„±',
                'íŒŒì‹±', 'ë Œë”ë§', 'í¬ë§·íŒ…', 'ìŠ¤íŠ¸ë¦¬ë°', 'ì„ë² ë”©', 'ë²¡í„°',
                'ê²€ìƒ‰', 'ì¸ë±ì‹±', 'í† í°í™”', 'ì „ì²˜ë¦¬', 'í›„ì²˜ë¦¬'
            ]
            
            for term in korean_terms:
                if term in docstring:
                    vocabulary["korean_terms"].append(term)
            
            # ì˜ì–´ ê¸°ìˆ  ìš©ì–´
            english_terms = [
                'detection', 'analysis', 'processing', 'generation', 'transformation',
                'validation', 'error', 'issue', 'condition', 'rule', 'logic', 'type',
                'mismatch', 'duplicate', 'complexity', 'parsing', 'rendering', 'formatting',
                'streaming', 'embedding', 'vector', 'search', 'indexing', 'tokenization',
                'preprocessing', 'postprocessing', 'chunking', 'rag', 'llm', 'ai'
            ]
            
            for term in english_terms:
                if term in docstring:
                    vocabulary["technical_terms"].append(term)
        
        # íƒœê·¸ì—ì„œ ë„ë©”ì¸ ê°œë… ì¶”ì¶œ
        if hasattr(chunk, 'tags') and chunk.tags:
            for tag in chunk.tags:
                if isinstance(tag, str) and len(tag) > 2:
                    vocabulary["domain_concepts"].append(tag)
        
        # ì˜ì¡´ì„±ì—ì„œ ê¸°ìˆ  ìš©ì–´ ì¶”ì¶œ
        if hasattr(chunk, 'dependencies') and chunk.dependencies:
            for dep in chunk.dependencies:
                if isinstance(dep, str):
                    vocabulary["technical_terms"].append(dep)

    async def _upload_md_sections_to_rag(self, all_chunks: List[CodeChunk], generated_files: List[str], summary_file: str) -> Dict[str, Any]:
        """ìƒì„±ëœ MD íŒŒì¼ë“¤ì„ ì„¹ì…˜ë³„ë¡œ ë¶„í• í•˜ì—¬ RAGì— ì—…ë¡œë“œ"""
        
        rag_base_url = os.getenv("RAG_SERVICE_URL", "http://localhost:8003")
        upload_results = []
        
        try:
            async with aiohttp.ClientSession() as session:
                # 1. ê°œë³„ MD íŒŒì¼ë“¤ì„ ì„¹ì…˜ë³„ë¡œ ë¶„í• í•˜ì—¬ ì—…ë¡œë“œ
                for md_file_path in generated_files:
                    file_upload_result = await self._upload_single_md_file_sections(
                        session, rag_base_url, md_file_path, all_chunks
                    )
                    upload_results.append(file_upload_result)
                
                # 2. í”„ë¡œì íŠ¸ ìš”ì•½ íŒŒì¼ ì„¹ì…˜ë³„ ì—…ë¡œë“œ
                summary_upload_result = await self._upload_project_summary_sections(
                    session, rag_base_url, summary_file
                )
                upload_results.append(summary_upload_result)
            
            # ì´ ì—…ë¡œë“œëœ ì„¹ì…˜ ìˆ˜ ê³„ì‚°
            total_sections = sum(detail.get("sections_uploaded", 0) for detail in upload_results)
            successful_files = sum(1 for detail in upload_results if detail.get("success", False))
            
            print(f"âœ… RAG ì—…ë¡œë“œ ì™„ë£Œ: {successful_files}/{len(upload_results)}ê°œ íŒŒì¼ ì²˜ë¦¬")
            print(f"   ğŸ“Š ì´ ì—…ë¡œë“œëœ ì„¹ì…˜: {total_sections}ê°œ")
            
            return {
                "success": True,
                "uploaded_files": len(upload_results),
                "successful_files": successful_files,
                "total_sections": total_sections,
                "details": upload_results
            }
            
        except Exception as e:
            print(f"âŒ RAG ì—…ë¡œë“œ ì‹¤íŒ¨: {e}")
            return {
                "success": False,
                "error": str(e),
                "details": []
            }
    
    async def _upload_single_md_file_sections(self, session: aiohttp.ClientSession, rag_base_url: str, 
                                            md_file_path: str, all_chunks: List[CodeChunk]) -> Dict[str, Any]:
        """ë‹¨ì¼ MD íŒŒì¼ì„ ì„¹ì…˜ë³„ë¡œ ë¶„í• í•˜ì—¬ RAGì— ì—…ë¡œë“œ"""
        
        try:
            # MD íŒŒì¼ ì½ê¸°
            with open(md_file_path, 'r', encoding='utf-8') as f:
                content = f.read()
            
            # íŒŒì¼ëª…ì—ì„œ Python íŒŒì¼ ê²½ë¡œ ì¶”ì¶œ (í”„ë¡œì íŠ¸ ë£¨íŠ¸ prefix ìœ ì§€)
            relative_path = os.path.relpath(md_file_path, self.output_dir)
            # ì ˆëŒ€ ì†ŒìŠ¤ ê²½ë¡œë¡œ ë³€í™˜í•˜ì—¬ chunk.file_path ì™€ ì •í™•íˆ ë§¤ì¹­
            python_file = os.path.join(self.source_dir, relative_path.replace('.md', '.py'))
            
            # í•´ë‹¹ íŒŒì¼ì˜ chunkë“¤ ì°¾ê¸°
            file_chunks = [chunk for chunk in all_chunks if chunk.file_path == python_file]
            
            # MD íŒŒì¼ì„ ì„¹ì…˜ë³„ë¡œ ë¶„í• 
            sections = self._split_md_into_sections(content, file_chunks, md_file_path)
            
            upload_count = 0
            for section in sections:
                # ì•ˆì •ì  ID ìƒì„±: project+source_file+section_title
                meta = section.get("metadata", {})
                stable_id = f"{meta.get('project','')}/{meta.get('source_file','')}/{meta.get('section_title','')}".lower()
                meta["id_hint"] = stable_id
                upload_success = await self._upload_section_to_rag(session, rag_base_url, {"content": section["content"], "metadata": meta})
                if upload_success:
                    upload_count += 1
            
            print(f"   ğŸ“„ {os.path.basename(md_file_path)}: {upload_count}/{len(sections)} ì„¹ì…˜ ì—…ë¡œë“œ")
            if upload_count != len(sections):
                print(f"   âš ï¸ ì¼ë¶€ ì„¹ì…˜ ì—…ë¡œë“œ ì‹¤íŒ¨: {len(sections) - upload_count}ê°œ ì‹¤íŒ¨")
            
            return {
                "file": md_file_path,
                "sections_total": len(sections),
                "sections_uploaded": upload_count,
                "success": upload_count > 0
            }
            
        except Exception as e:
            print(f"   âŒ {os.path.basename(md_file_path)}: {e}")
            return {
                "file": md_file_path,
                "sections_total": 0,
                "sections_uploaded": 0,
                "success": False,
                "error": str(e)
            }
    
    async def _upload_project_summary_sections(self, session: aiohttp.ClientSession, 
                                             rag_base_url: str, summary_file: str) -> Dict[str, Any]:
        """í”„ë¡œì íŠ¸ ìš”ì•½ì„ ì„¹ì…˜ë³„ë¡œ ë¶„í• í•˜ì—¬ RAGì— ì—…ë¡œë“œ"""
        
        try:
            # í”„ë¡œì íŠ¸ ìš”ì•½ íŒŒì¼ ì½ê¸°
            with open(summary_file, 'r', encoding='utf-8') as f:
                content = f.read()
            
            # í”„ë¡œì íŠ¸ ìš”ì•½ì„ ì„¹ì…˜ë³„ë¡œ ë¶„í• 
            sections = self._split_project_summary_into_sections(content)
            
            upload_count = 0
            for section in sections:
                meta = section.get("metadata", {})
                stable_id = f"{meta.get('project','')}/{meta.get('source_file','')}/{meta.get('section_title','')}".lower()
                meta["id_hint"] = stable_id
                upload_success = await self._upload_section_to_rag(session, rag_base_url, {"content": section["content"], "metadata": meta})
                if upload_success:
                    upload_count += 1
            
            print(f"   ğŸ“Š í”„ë¡œì íŠ¸ ìš”ì•½: {upload_count}/{len(sections)} ì„¹ì…˜ ì—…ë¡œë“œ")
            
            return {
                "file": summary_file,
                "sections_total": len(sections),
                "sections_uploaded": upload_count,
                "success": upload_count > 0
            }
            
        except Exception as e:
            print(f"   âŒ í”„ë¡œì íŠ¸ ìš”ì•½: {e}")
            return {
                "file": summary_file,
                "sections_total": 0,
                "sections_uploaded": 0,
                "success": False,
                "error": str(e)
            }
    
    def _split_md_into_sections(self, content: str, file_chunks: List[CodeChunk], md_file_path: str = None) -> List[Dict[str, Any]]:
        """MD íŒŒì¼ì„ ì˜ë¯¸ìˆëŠ” ì„¹ì…˜ë³„ë¡œ ë¶„í•  (ë‹¤ì–‘í•œ ì œëª© ë ˆë²¨ ì§€ì›)"""
        
        sections = []
        print(f"   ğŸ“„ ì„¹ì…˜ ë¶„í•  ì‹œì‘: {os.path.basename(md_file_path) if md_file_path else 'unknown'}")
        
        # ëª¨ë“  ì œëª© ë ˆë²¨ ì§€ì› (#, ##, ###, ####, #####, ######)
        # H1, H2ëŠ” í° ì„¹ì…˜, H3~H6ëŠ” ì‘ì€ ì„¹ì…˜ìœ¼ë¡œ ë¶„ë¥˜
        section_patterns = [
            (r'^(#{1,2}\s+.*?)(?=^#{1,2}|\Z)', 'major'),  # H1, H2 - ì£¼ìš” ì„¹ì…˜
            (r'^(#{3,6}\s+.*?)(?=^#{1,6}|\Z)', 'minor')   # H3~H6 - ì„¸ë¶€ ì„¹ì…˜
        ]
        
        total_sections_found = 0
        total_sections_processed = 0
        
        for pattern, section_type in section_patterns:
            matches = re.finditer(pattern, content, re.MULTILINE | re.DOTALL)
            
            for match in matches:
                total_sections_found += 1
                section_content = match.group(1).strip()
                
                # ìµœì†Œ ê¸¸ì´ ê¸°ì¤€ ì™„í™” (20ìë¡œ ê°ì†Œ)
                if len(section_content) < 20:
                    print(f"     â­ï¸ ì„¹ì…˜ ê±´ë„ˆëœ€ (ë„ˆë¬´ ì§§ìŒ: {len(section_content)}ì): {section_content[:50]}...")
                    continue
                
                total_sections_processed += 1
            
                # ì„¹ì…˜ ì œëª© ì¶”ì¶œ (ëª¨ë“  ì œëª© ë ˆë²¨ ì§€ì›)
                title_match = re.match(r'^(#{1,6})\s+(.+)', section_content)
                if title_match:
                    title_level = len(title_match.group(1))
                    title = title_match.group(2).strip()
                    print(f"     ğŸ“ {section_type} ì„¹ì…˜ ë°œê²¬ (H{title_level}): {title[:60]}...")
                else:
                    title = "Unknown Section"
                    print(f"     âš ï¸ ì œëª© ì¶”ì¶œ ì‹¤íŒ¨: {section_content[:50]}...")
            
                # ê´€ë ¨ chunk ì°¾ê¸°
                related_chunk = self._find_related_chunk(title, section_content, file_chunks)
                
                # chunk_type ê²°ì • ë¡œì§ ê°œì„  + Sequence ì„¹ì…˜ ì¸ì‹
                lowered_title = title.lower()
                lowered_content = section_content.lower()
                if related_chunk:
                    chunk_type = related_chunk.chunk_type
                    print(f"       ğŸ”— ê´€ë ¨ chunk ë°œê²¬: {chunk_type} - {related_chunk.name}")
                elif 'sequencediagram' in lowered_content or '### ğŸ”— í˜¸ì¶œ ìˆœì„œ' in title or 'sequence' in lowered_title:
                    chunk_type = 'sequence'
                    print(f"       ğŸ”— ì‹œí€€ìŠ¤ ì„¹ì…˜ìœ¼ë¡œ ë¶„ë¥˜")
                elif "function" in lowered_title or "def " in lowered_content:
                    chunk_type = "function"
                    print(f"       ğŸ”§ í•¨ìˆ˜ ì„¹ì…˜ìœ¼ë¡œ ë¶„ë¥˜")
                elif "class" in lowered_title or "class " in lowered_content:
                    chunk_type = "class"
                    print(f"       ğŸ—ï¸ í´ë˜ìŠ¤ ì„¹ì…˜ìœ¼ë¡œ ë¶„ë¥˜")
                else:
                    chunk_type = "overview"
                    print(f"       ğŸ“‹ ê°œìš” ì„¹ì…˜ìœ¼ë¡œ ë¶„ë¥˜")
            
                # íŒŒì¼ ê²½ë¡œì—ì„œ ì •ë³´ ì¶”ì¶œ (ê°œì„ ë¨)
                # project_nameì„ ë¨¼ì € ì´ˆê¸°í™”
                project_name = os.path.basename(self.source_dir)
                
                if file_chunks and file_chunks[0].file_path:
                    file_path = file_chunks[0].file_path
                    filename = os.path.basename(file_path).replace('.py', '.md')
                    # self.source_dir ì´í›„ ê²½ë¡œ ì¶”ì¶œ
                    rel_base = self.source_dir.rstrip(os.sep) + os.sep
                    relative_path = file_path[len(rel_base):] if file_path.startswith(rel_base) else os.path.basename(file_path)
                else:
                    # file_chunksê°€ ì—†ì–´ë„ md íŒŒì¼ ê²½ë¡œì—ì„œ ì¶”ì¶œ ì‹œë„
                    if md_file_path and md_file_path != 'unknown':
                        # generated_docs/<project>/<path>.md â†’ <source_dir>/<path>.py
                        try:
                            md_rel = os.path.relpath(md_file_path, self.output_dir)
                        except Exception:
                            md_rel = os.path.basename(md_file_path)
                        file_path = os.path.join(self.source_dir, md_rel.replace('.md', '.py'))
                        filename = os.path.basename(md_file_path)
                        # metadataìš© ì†ŒìŠ¤ ìƒëŒ€ ê²½ë¡œ
                        rel_base = self.source_dir.rstrip(os.sep) + os.sep
                        relative_path = file_path[len(rel_base):] if file_path.startswith(rel_base) else os.path.basename(file_path)
                    else:
                        file_path = "unknown"
                        filename = "unknown"
                        relative_path = "unknown"
            
                # source ê²½ë¡œ(ë¸Œë¼ìš°ì§•/ê°€ì¤‘ì¹˜ìš©) êµ¬ì„±: generated_docs/<project>/<relative_md_path>
                md_relative_path = None
                if md_file_path and self.output_dir in md_file_path:
                    try:
                        md_relative_path = os.path.relpath(md_file_path, self.output_dir)
                    except Exception:
                        md_relative_path = os.path.basename(md_file_path)

                # ìƒ‰ì¸ ê°•í™”: ë™ì˜ì–´ í‚¤ì›Œë“œ í”„ë¦¬í”½ìŠ¤ ì¶”ê°€
                enable_prefix = os.getenv("ENABLE_CONTENT_KEYWORD_PREFIX", "false").lower() == "true"
                keyword_prefix = (
                    "í‚¤ì›Œë“œ: ì˜¤ë¥˜, ì—ëŸ¬, ì´ìŠˆ, ìœ í˜•, íƒ€ì…, ì¢…ë¥˜, ê²€ì¶œ, íƒì§€, ê°ì§€, ë¶„ì„, ì¶”ê°€, í™•ì¥, ë“±ë¡, detect, detection, analyze, analysis, overview, summary, system, class, function, method, complexity, metrics\n\n"
                )
                content_to_store = (keyword_prefix + section_content) if enable_prefix else section_content

                # ID ìƒì„± ê°œì„  (ë” ê³ ìœ í•œ ID)
                section_id = f"section_{len(sections)}_{hash(title) % 10000:04d}"
                if related_chunk:
                    section_id = f"{related_chunk.chunk_id}_{len(sections)}"

                sections.append({
                    "content": content_to_store,
                    "metadata": {
                        "chunk_type": chunk_type,
                        "filename": filename,
                        "source_file": relative_path,
                        "project": project_name,
                        "section_title": title,
                        "section_level": title_level if 'title_level' in locals() else 3,
                        "section_type": section_type,
                        "chunk_id": related_chunk.chunk_id if related_chunk else section_id,
                        "id_hint": section_id,
                        "folder_priority": self._get_folder_priority(file_path),
                        "tags": related_chunk.tags if related_chunk else [],
                        "keywords": "ì˜¤ë¥˜, ì—ëŸ¬, ì´ìŠˆ, ìœ í˜•, íƒ€ì…, ì¢…ë¥˜, ê²€ì¶œ, íƒì§€, ê°ì§€, ë¶„ì„, ì¶”ê°€, í™•ì¥, ë“±ë¡, detect, detection, analyze, analysis, overview, summary, system, class, function, method, complexity, metrics",
                        "docstring": related_chunk.docstring if related_chunk and related_chunk.docstring else "",
                        "description": related_chunk.docstring if related_chunk and related_chunk.docstring else "",
                        "source": f"generated_docs/{project_name}/{md_relative_path}" if md_relative_path else f"generated_docs/{project_name}/{filename}",
                        "name": related_chunk.name if related_chunk else title
                    }
                })
                
                print(f"       âœ… ì„¹ì…˜ ì¶”ê°€ë¨: {chunk_type} - {title[:40]}... (ID: {section_id})")
        
        print(f"   ğŸ“Š ì„¹ì…˜ ë¶„í•  ì™„ë£Œ: {total_sections_processed}/{total_sections_found}ê°œ ì²˜ë¦¬ë¨")
        
        # ê°œìš” ì„¹ì…˜ë„ ì¶”ê°€ (íŒŒì¼ ìƒë‹¨ ë¶€ë¶„) - ê°œì„ ëœ íŒ¨í„´
        overview_pattern = r'^(.*?)(?=^#{1,6}|\Z)'
        overview_match = re.match(overview_pattern, content, re.MULTILINE | re.DOTALL)
        if overview_match:
            overview_content = overview_match.group(1).strip()
            if len(overview_content) > 50:  # ìµœì†Œ ê¸¸ì´ ì™„í™”
                print(f"     ğŸ“‹ ê°œìš” ì„¹ì…˜ ë°œê²¬: {len(overview_content)}ì")
                # ê°œìš” ì„¹ì…˜ì˜ ë©”íƒ€ë°ì´í„°ë„ í†µì¼
                file_path = file_chunks[0].file_path if file_chunks else "unknown"
                filename = os.path.basename(file_path).replace('.py', '.md') if file_chunks else "unknown"
                relative_path = file_path.split('sample_code/')[-1] if 'sample_code/' in file_path else file_path
                project_name = "sample_code" if file_chunks else "unknown"
                
                # í”„ë¡œì íŠ¸ ë£¨íŠ¸ ê¸°ì¤€ source ê²½ë¡œ ê³„ì‚°
                md_relative_path = None
                if md_file_path and self.output_dir in md_file_path:
                    try:
                        md_relative_path = os.path.relpath(md_file_path, self.output_dir)
                    except Exception:
                        md_relative_path = os.path.basename(md_file_path)

                enable_prefix = os.getenv("ENABLE_CONTENT_KEYWORD_PREFIX", "false").lower() == "true"
                keyword_prefix = (
                    "í‚¤ì›Œë“œ: ê°œìš”, overview, summary, ì‹œìŠ¤í…œ, system, í”„ë¡œì íŠ¸, project, êµ¬ì¡°, êµ¬ì¡°ë„, analyzers, formatters, streaming, shared, utils\n\n"
                )
                overview_content_to_store = (keyword_prefix + overview_content) if enable_prefix else overview_content
                
                # ê°œìš” ì„¹ì…˜ ID ìƒì„±
                overview_id = f"overview_{len(sections)}_{hash(overview_content[:100]) % 10000:04d}"
                
                sections.insert(0, {
                    "content": overview_content_to_store,
                    "metadata": {
                        "chunk_type": "overview",
                        "filename": filename,
                        "source_file": relative_path,
                        "project": project_name,
                        "section_title": "File Overview",
                        "section_level": 0,
                        "section_type": "overview",
                        "chunk_id": overview_id,
                        "id_hint": overview_id,
                        "folder_priority": self._get_folder_priority(file_path),
                        "tags": ["overview", "file_info"],
                        "keywords": "ê°œìš”, overview, summary, ì‹œìŠ¤í…œ, system, í”„ë¡œì íŠ¸, project, êµ¬ì¡°, êµ¬ì¡°ë„, analyzers, formatters, streaming, shared, utils",
                        "source": f"generated_docs/{project_name}/{md_relative_path}" if md_relative_path else f"generated_docs/{project_name}/{filename}",
                        "name": "file_overview"
                    }
                })
                
                print(f"       âœ… ê°œìš” ì„¹ì…˜ ì¶”ê°€ë¨: overview - File Overview (ID: {overview_id})")
        
        return sections
    
    def _split_project_summary_into_sections(self, content: str) -> List[Dict[str, Any]]:
        """í”„ë¡œì íŠ¸ ìš”ì•½ì„ ì„¹ì…˜ë³„ë¡œ ë¶„í• """
        
        sections = []
        
        # ì œëª©ë³„ë¡œ ì„¹ì…˜ ë¶„í•  (## ê¸°ì¤€)
        section_pattern = r'^(##\s+.*?)(?=^##|\Z)'
        matches = re.finditer(section_pattern, content, re.MULTILINE | re.DOTALL)
        
        for i, match in enumerate(matches):
            section_content = match.group(1).strip()
            if len(section_content) < 50:  # ë„ˆë¬´ ì§§ì€ ì„¹ì…˜ ì œì™¸
                continue
            
            # ì„¹ì…˜ ì œëª© ì¶”ì¶œ
            title_match = re.match(r'^##\s+(.+)', section_content)
            title = title_match.group(1) if title_match else f"Section {i+1}"
            
            enable_prefix = os.getenv("ENABLE_CONTENT_KEYWORD_PREFIX", "false").lower() == "true"
            keyword_prefix = (
                "í‚¤ì›Œë“œ: í”„ë¡œì íŠ¸, ìš”ì•½, ê°œìš”, overview, summary, ì‹œìŠ¤í…œ, êµ¬ì¡°, êµ¬ì¡°ë„, ë¶„ì„, analyzers, formatters, streaming, shared, utils\n\n"
            )
            section_content_to_store = (keyword_prefix + section_content) if enable_prefix else section_content
            sections.append({
                "content": section_content_to_store,
                "metadata": {
                    "chunk_type": "overview",
                    "filename": "project_summary.md",
                    "source_file": "project_summary.md",
                    "project": os.path.basename(self.source_dir),
                    "section_title": title,
                    "chunk_id": f"project_summary_section_{i}",
                    "folder_priority": 10,  # í”„ë¡œì íŠ¸ ìš”ì•½ì€ ìµœê³  ìš°ì„ ìˆœìœ„
                    "tags": ["project", "summary", "overview"],
                    "keywords": "í”„ë¡œì íŠ¸, ìš”ì•½, ê°œìš”, overview, summary, ì‹œìŠ¤í…œ, êµ¬ì¡°, ë¶„ì„, analyzers, formatters, streaming, shared, utils",
                    "source": f"generated_docs/{os.path.basename(self.source_dir)}/project_summary.md",
                    "name": title
                }
            })
        
        return sections
    
    def _find_related_chunk(self, title: str, content: str, file_chunks: List[CodeChunk]) -> CodeChunk:
        """ì„¹ì…˜ê³¼ ê´€ë ¨ëœ chunk ì°¾ê¸°"""
        
        # ì œëª©ì—ì„œ í•¨ìˆ˜/í´ë˜ìŠ¤ëª… ì¶”ì¶œ
        function_match = re.search(r'`([^`]+)`', title)
        if function_match:
            function_name = function_match.group(1)
            for chunk in file_chunks:
                if chunk.name == function_name:
                    return chunk
        
        # ë‚´ìš©ì—ì„œ chunk ID ì°¾ê¸°
        chunk_id_match = re.search(r'ğŸ†”\s*\*\*ID\*\*:\s*`([^`]+)`', content)
        if chunk_id_match:
            chunk_id = chunk_id_match.group(1)
            for chunk in file_chunks:
                if chunk.chunk_id == chunk_id:
                    return chunk
        
        return None
    
    def _get_folder_priority(self, file_path: str) -> int:
        """í´ë” ìš°ì„ ìˆœìœ„ ê³„ì‚°"""
        
        priority_map = {
            "analyzers": 9,
            "formatters": 8,
            "streaming": 7,
            "shared": 6,
            "service": 6,
            "models": 5,
            "utils": 4,
            "exceptions": 3
        }
        
        for folder, priority in priority_map.items():
            if folder in file_path:
                return priority
        
        return 1  # ê¸°ë³¸ ìš°ì„ ìˆœìœ„
    
    async def _upload_section_to_rag(self, session: aiohttp.ClientSession, 
                                   rag_base_url: str, section: Dict[str, Any]) -> bool:
        """ê°œë³„ ì„¹ì…˜ì„ RAGì— ì—…ë¡œë“œ"""
        
        try:
            upload_data = {
                "id": section.get("metadata", {}).get("id_hint"),
                "content": section["content"],
                "metadata": section["metadata"]
            }
            
            async with session.post(
                f"{rag_base_url}/api/v1/documents",
                json=upload_data,
                timeout=aiohttp.ClientTimeout(total=10)
            ) as response:
                
                if response.status == 200:
                    return True
                else:
                    error_text = await response.text()
                    print(f"     âŒ RAG ì—…ë¡œë“œ ì‹¤íŒ¨ (HTTP {response.status}): {error_text}")
                    print(f"     ğŸ“ ì‹¤íŒ¨í•œ ì„¹ì…˜ ID: {section.get('metadata', {}).get('id_hint', 'unknown')}")
                    return False
                    
        except Exception as e:
            print(f"     âŒ RAG ì—…ë¡œë“œ ì˜ˆì™¸: {e}")
            return False


async def generate_template_chunk_documents(source_dir: str, output_dir: str, template_dir: str = None) -> Dict[str, Any]:
    """í…œí”Œë¦¿ ê¸°ë°˜ chunk ë¬¸ì„œ ìƒì„± ë©”ì¸ í•¨ìˆ˜"""
    
    generator = TemplateChunkGenerator(source_dir, output_dir, template_dir)
    return await generator.generate_documents(upload_to_rag=True)


async def upload_generated_documents_to_rag(source_dir: str, output_dir: str, template_dir: str = None) -> Dict[str, Any]:
    """ì´ë¯¸ ìƒì„±ëœ MD ë¬¸ì„œë¥¼ RAGì—ë§Œ ì—…ë¡œë“œ (ë‹¨ë… ì‹¤í–‰)"""
    try:
        # ì²­í¬ ì¬êµ¬ì„± (ì„¹ì…˜ ë§¤í•‘ì„ ìœ„í•´ í•„ìš”)
        all_chunks = chunk_directory(source_dir, max_tokens=600)

        # ì—…ë¡œë“œ ëŒ€ìƒ MD íŒŒì¼ ìˆ˜ì§‘
        md_files = []
        for root, dirs, files in os.walk(output_dir):
            for f in files:
                if f.endswith('.md') and f != 'project_summary.md':
                    md_files.append(os.path.join(root, f))

        summary_file = os.path.join(output_dir, 'project_summary.md')
        if not os.path.exists(summary_file):
            summary_file = None

        renderer = TemplateChunkGenerator(source_dir, output_dir, template_dir)

        rag_base_url = os.getenv("RAG_SERVICE_URL", "http://localhost:8003")
        upload_details = []
        import aiohttp
        async with aiohttp.ClientSession() as session:
            # 1) ê°œë³„ íŒŒì¼ ì—…ë¡œë“œ
            for md in md_files:
                file_result = await renderer._upload_single_md_file_sections(session, rag_base_url, md, all_chunks)
                upload_details.append(file_result)
            # 2) í”„ë¡œì íŠ¸ ìš”ì•½ ì—…ë¡œë“œ
            if summary_file:
                summary_result = await renderer._upload_project_summary_sections(session, rag_base_url, summary_file)
                upload_details.append(summary_result)

        return {
            "success": True,
            "uploaded_files": len(upload_details),
            "details": upload_details
        }
    except Exception as e:
        return {"success": False, "error": str(e), "details": []}


if __name__ == "__main__":
    import asyncio
    
    # ê¸°ì¡´ generated_docs í´ë”ì— ì—…ë°ì´íŠ¸
    source_dir = "/Users/roseline/projects/codemuse-backend/sample_code"
    output_dir = "/Users/roseline/projects/codemuse-backend/generated_docs"
    
    result = asyncio.run(generate_template_chunk_documents(source_dir, output_dir))
    print(f"\nğŸ¯ ìµœì¢… ê²°ê³¼: {result}")
